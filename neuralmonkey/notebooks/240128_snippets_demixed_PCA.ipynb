{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" \n",
    "2/2/23 - All methods realted to dPCA here.\n",
    "Thought of consoldiating into functions, but best to leave here, and isntaed I tried to carefully doc below.\n",
    "\n",
    "Script: analy_dpca_plot_script.py. It is \n",
    "\n",
    "Building from\n",
    "_snippets_rsa.ipynb.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load DFallpa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef8faa1035f23ec"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_dpca_script_quick import preprocess_pa_to_frtensor\n",
    "from neuralmonkey.classes.snippets import load_and_concat_mult_snippets\n",
    "from neuralmonkey.classes.session import load_mult_session_helper\n",
    "import os\n",
    "import pandas as pd\n",
    "from neuralmonkey.analyses.state_space_good import snippets_extract_popanals_split_bregion_twind\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:27:35.227601677Z",
     "start_time": "2024-02-14T03:27:35.090804896Z"
    }
   },
   "id": "3750e537f858d4f3"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T20:32:51.453194774Z",
     "start_time": "2024-02-15T20:32:51.423902975Z"
    }
   },
   "id": "804b58348125cdf2"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "  which_level       event bregion       twind  \\\n0   substroke  00_substrk      M1  (0.0, 0.2)   \n1   substroke  00_substrk     PMv  (0.0, 0.2)   \n2   substroke  00_substrk     PMd  (0.0, 0.2)   \n3   substroke  00_substrk   dlPFC  (0.0, 0.2)   \n4   substroke  00_substrk   vlPFC  (0.0, 0.2)   \n5   substroke  00_substrk      FP  (0.0, 0.2)   \n6   substroke  00_substrk     SMA  (0.0, 0.2)   \n7   substroke  00_substrk  preSMA  (0.0, 0.2)   \n\n                                                  pa  \n0  <neuralmonkey.classes.population.PopAnal objec...  \n1  <neuralmonkey.classes.population.PopAnal objec...  \n2  <neuralmonkey.classes.population.PopAnal objec...  \n3  <neuralmonkey.classes.population.PopAnal objec...  \n4  <neuralmonkey.classes.population.PopAnal objec...  \n5  <neuralmonkey.classes.population.PopAnal objec...  \n6  <neuralmonkey.classes.population.PopAnal objec...  \n7  <neuralmonkey.classes.population.PopAnal objec...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>which_level</th>\n      <th>event</th>\n      <th>bregion</th>\n      <th>twind</th>\n      <th>pa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>M1</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>PMv</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>PMd</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>dlPFC</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>vlPFC</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>FP</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>SMA</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>substroke</td>\n      <td>00_substrk</td>\n      <td>preSMA</td>\n      <td>(0.0, 0.2)</td>\n      <td>&lt;neuralmonkey.classes.population.PopAnal objec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neuralmonkey.scripts.analy_dpca_script_substrokes import dpca_compute_pa_to_space\n",
    "\n",
    "# dpca_compute_pa_to_space(pa, effect_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T20:33:40.676392732Z",
     "start_time": "2024-02-15T20:33:40.639887522Z"
    }
   },
   "id": "c0f3566690e7fd9c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 0.2)]\n"
     ]
    }
   ],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "import sys\n",
    "\n",
    "SAVEDIR_ANALYSES = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/dPCA\"\n",
    "\n",
    "############### PARAMS\n",
    "animal = \"Diego\"\n",
    "date = 230616\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "bin_by_time_dur = 0.05\n",
    "bin_by_time_slide = 0.025\n",
    "which_level = \"substroke\"\n",
    "\n",
    "if False:\n",
    "    # METHOD 2 - Merging across time windows into single PA bofre doing dPCA\n",
    "    question = \"SP_shape_loc_TIME\"\n",
    "    slice_agg_slices = [\n",
    "        (\"trial\", \"03_samp\", (-0.3, 0.5)),\n",
    "        (\"trial\", \"04_go_cue\", (-0.45, 0.25)),\n",
    "        (\"trial\", \"06_on_strokeidx_0\", (-0.25, 0.7))\n",
    "    ]\n",
    "    slice_agg_vars_to_split = [\"bregion\"]\n",
    "    slice_agg_concat_dim = \"times\"\n",
    "    \n",
    "    list_time_windows = [sl[2] for sl in slice_agg_slices]\n",
    "    events_keep = list(set([sl[1] for sl in slice_agg_slices]))\n",
    "    print(list_time_windows)\n",
    "else:\n",
    "    # METHOD 1 - Standard, running separately for each PA\n",
    "    question = \"SS_shape\"\n",
    "    slice_agg_slices = None\n",
    "    slice_agg_vars_to_split = None\n",
    "    slice_agg_concat_dim = None\n",
    "    \n",
    "    # list_time_windows = [(-0.3, 0.)]\n",
    "    list_time_windows = [(0., 0.2)]\n",
    "    events_keep = [\"00_substrk\"]\n",
    "    # list_time_windows = [(-0.3, 0.)]\n",
    "    # events_keep = [\"06_on_strokeidx_0\"]\n",
    "    print(list_time_windows)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:27:43.615414933Z",
     "start_time": "2024-02-14T03:27:43.571457629Z"
    }
   },
   "id": "21ee6a1220959299"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230616*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004\n",
      "session:  0\n",
      "Beh Sessions that exist on this date:  {230616: [(1, 'priminvar5b'), (2, 'priminvar5b')]}\n",
      "------------------------------\n",
      "Loading this neural session: 0\n",
      "Loading these beh expts: ['priminvar5b']\n",
      "Loading these beh sessions: [1]\n",
      "Using this beh_trial_map_list: [(1, 0)]\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230616*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004\n",
      "{'filename_components_hyphened': ['Diego', '230616', '110840'], 'basedirs': ['/mnt/Freiwald/ltian/recordings/Diego', '/mnt/Freiwald/ltian/recordings/Diego/230616'], 'basedirs_filenames': ['230616', 'Diego-230616-110840'], 'filename_final_ext': 'Diego-230616-110840', 'filename_final_noext': 'Diego-230616-110840'}\n",
      "FOund this path for spikes:  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840/spikes_tdt_quick-4\n",
      "== PATHS for this expt: \n",
      "raws  --  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840\n",
      "tank  --  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840/Diego-230616-110840\n",
      "spikes  --  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840/spikes_tdt_quick-4\n",
      "final_dir_name  --  Diego-230616-110840\n",
      "time  --  110840\n",
      "pathbase_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840\n",
      "tank_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/data_tank.pkl\n",
      "spikes_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/data_spikes.pkl\n",
      "datall_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/data_datall.pkl\n",
      "events_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/events_photodiode.pkl\n",
      "mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/mapper_st2dat.pkl\n",
      "figs_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/figs\n",
      "metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego\n",
      "cached_dir  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/cached\n",
      "Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego/230616.yaml\n",
      "updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')\n",
      "[_sitesdirty_update] skipping! since did not find:  sites_error_spikes\n",
      "Printing whether spikes gotten (o) or not (-) because of spike peak to trough\n",
      "== Loading TDT tank\n",
      "** Loading tank data from local (previusly cached)\n",
      "== Done\n",
      "== Trying to load events data\n",
      "Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-110840/events_photodiode.pkl\n",
      "== Done\n",
      "** MINIMAL_LOADING, therefore loading previuosly cached data\n",
      "=== CLEANING UP self.Dat (_cleanup_reloading_saved_state) ===== \n",
      "0 _behclass_alignsim_compute\n",
      "200 _behclass_alignsim_compute\n",
      "400 _behclass_alignsim_compute\n",
      "Running D._behclass_tokens_extract_datsegs\n",
      "0 _behclass_tokens_extract_datsegs\n",
      "200 _behclass_tokens_extract_datsegs\n",
      "400 _behclass_tokens_extract_datsegs\n",
      "stored in self.Dat[BehClass]\n",
      "- starting/ending len (grouping params):\n",
      "489\n",
      "489\n",
      "- starting/ending len (getting sequence):\n",
      "489\n",
      "489\n",
      "--- Removing nans\n",
      "start len: 489\n",
      "- num names for each col\n",
      "not removing nans, since columns=[]\n",
      "ADded new column: supervision_online\n",
      "Reassigned train/test, using key: probe\n",
      "and values:\n",
      "Train =  [0]\n",
      "Test =  [1]\n",
      " \n",
      "New distribution of train/test:\n",
      "train    489\n",
      "Name: monkey_train_or_test, dtype: int64\n",
      "Appended column: los_info\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_SEQUENCE_ALPHA]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "Appended self.Dat[superv_VISUALFB_METH]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_new\n",
      "[taskgroup_reassign_by_mapper], reassigned values in column: taskgroup\n",
      "GROUPING epoch\n",
      "GROUPING_LEVELS ['230616']\n",
      "FEATURE_NAMES ['hdoffline', 'num_strokes_beh', 'num_strokes_task', 'circ', 'dist']\n",
      "SCORE_COL_NAMES []\n",
      "appended col to self.Dat:\n",
      "date_epoch\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_concise\n",
      "Append column to self.Dat:  supervision_stage_semantic\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "... Generated these...\n",
      "self.BehTrialMapList [(1, 0)]\n",
      "self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27), 27: (0, 28), 28: (0, 29), 29: (0, 30), 30: (0, 31), 31: (0, 32), 32: (0, 33), 33: (0, 34), 34: (0, 35), 35: (0, 36), 36: (0, 37), 37: (0, 38), 38: (0, 39), 39: (0, 40), 40: (0, 41), 41: (0, 42), 42: (0, 43), 43: (0, 44), 44: (0, 45), 45: (0, 46), 46: (0, 47), 47: (0, 48), 48: (0, 49), 49: (0, 50), 50: (0, 51), 51: (0, 52), 52: (0, 53), 53: (0, 54), 54: (0, 55), 55: (0, 56), 56: (0, 57), 57: (0, 58), 58: (0, 59), 59: (0, 60), 60: (0, 61), 61: (0, 62), 62: (0, 63), 63: (0, 64), 64: (0, 65), 65: (0, 66), 66: (0, 67), 67: (0, 68), 68: (0, 69), 69: (0, 70), 70: (0, 71), 71: (0, 72), 72: (0, 73), 73: (0, 74), 74: (0, 75), 75: (0, 76), 76: (0, 77), 77: (0, 78), 78: (0, 79), 79: (0, 80), 80: (0, 81), 81: (0, 82), 82: (0, 83), 83: (0, 84), 84: (0, 85), 85: (0, 86), 86: (0, 87), 87: (0, 88), 88: (0, 89), 89: (0, 90), 90: (0, 91), 91: (0, 92), 92: (0, 93), 93: (0, 94), 94: (0, 95), 95: (0, 96), 96: (0, 97), 97: (0, 98), 98: (0, 99), 99: (0, 100), 100: (0, 101), 101: (0, 102), 102: (0, 103), 103: (0, 104), 104: (0, 105), 105: (0, 106), 106: (0, 107), 107: (0, 108), 108: (0, 109), 109: (0, 110), 110: (0, 111), 111: (0, 112), 112: (0, 113), 113: (0, 114), 114: (0, 115), 115: (0, 116), 116: (0, 117), 117: (0, 118), 118: (0, 119), 119: (0, 120), 120: (0, 121), 121: (0, 122), 122: (0, 123), 123: (0, 124), 124: (0, 125), 125: (0, 126), 126: (0, 127), 127: (0, 128), 128: (0, 129), 129: (0, 130), 130: (0, 131), 131: (0, 132), 132: (0, 133), 133: (0, 134), 134: (0, 135), 135: (0, 136), 136: (0, 137), 137: (0, 138), 138: (0, 139), 139: (0, 140), 140: (0, 141), 141: (0, 142), 142: (0, 143), 143: (0, 144), 144: (0, 145), 145: (0, 146), 146: (0, 147), 147: (0, 148), 148: (0, 149), 149: (0, 150), 150: (0, 151), 151: (0, 152), 152: (0, 153), 153: (0, 154), 154: (0, 155), 155: (0, 156), 156: (0, 157), 157: (0, 158), 158: (0, 159), 159: (0, 160), 160: (0, 161), 161: (0, 162), 162: (0, 163), 163: (0, 164), 164: (0, 165), 165: (0, 166), 166: (0, 167), 167: (0, 168), 168: (0, 169), 169: (0, 170), 170: (0, 171), 171: (0, 172), 172: (0, 173), 173: (0, 174), 174: (0, 175), 175: (0, 176), 176: (0, 177), 177: (0, 178), 178: (0, 179), 179: (0, 180), 180: (0, 181), 181: (0, 182), 182: (0, 183), 183: (0, 184), 184: (0, 185), 185: (0, 186), 186: (0, 187), 187: (0, 188), 188: (0, 189), 189: (0, 190), 190: (0, 191), 191: (0, 192), 192: (0, 193), 193: (0, 194), 194: (0, 195), 195: (0, 196), 196: (0, 197), 197: (0, 198), 198: (0, 199), 199: (0, 200), 200: (0, 201), 201: (0, 202), 202: (0, 203), 203: (0, 204), 204: (0, 205), 205: (0, 206), 206: (0, 207), 207: (0, 208), 208: (0, 209), 209: (0, 210), 210: (0, 211), 211: (0, 212), 212: (0, 213), 213: (0, 214), 214: (0, 215), 215: (0, 216), 216: (0, 217), 217: (0, 218), 218: (0, 219), 219: (0, 220), 220: (0, 221), 221: (0, 222), 222: (0, 223), 223: (0, 224), 224: (0, 225), 225: (0, 226), 226: (0, 227), 227: (0, 228), 228: (0, 229), 229: (0, 230), 230: (0, 231), 231: (0, 232), 232: (0, 233), 233: (0, 234), 234: (0, 235), 235: (0, 236), 236: (0, 237), 237: (0, 238), 238: (0, 239), 239: (0, 240), 240: (0, 241), 241: (0, 242), 242: (0, 243), 243: (0, 244), 244: (0, 245), 245: (0, 246), 246: (0, 247), 247: (0, 248), 248: (0, 249), 249: (0, 250), 250: (0, 251), 251: (0, 252), 252: (0, 253), 253: (0, 254), 254: (0, 255), 255: (0, 256), 256: (0, 257), 257: (0, 258), 258: (0, 259), 259: (0, 260), 260: (0, 261), 261: (0, 262), 262: (0, 263), 263: (0, 264), 264: (0, 265), 265: (0, 266), 266: (0, 267), 267: (0, 268), 268: (0, 269), 269: (0, 270), 270: (0, 271), 271: (0, 272), 272: (0, 273), 273: (0, 274), 274: (0, 275), 275: (0, 276), 276: (0, 277), 277: (0, 278), 278: (0, 279), 279: (0, 280), 280: (0, 281), 281: (0, 282), 282: (0, 283), 283: (0, 284), 284: (0, 285), 285: (0, 286), 286: (0, 287), 287: (0, 288), 288: (0, 289), 289: (0, 290), 290: (0, 291), 291: (0, 292), 292: (0, 293), 293: (0, 294), 294: (0, 295), 295: (0, 296), 296: (0, 297), 297: (0, 298), 298: (0, 299), 299: (0, 300), 300: (0, 301), 301: (0, 302), 302: (0, 303), 303: (0, 304), 304: (0, 305), 305: (0, 306), 306: (0, 307), 307: (0, 308), 308: (0, 309), 309: (0, 310), 310: (0, 311), 311: (0, 312), 312: (0, 313), 313: (0, 314), 314: (0, 315), 315: (0, 316), 316: (0, 317), 317: (0, 318), 318: (0, 319), 319: (0, 320), 320: (0, 321), 321: (0, 322), 322: (0, 323), 323: (0, 324), 324: (0, 325), 325: (0, 326), 326: (0, 327), 327: (0, 328), 328: (0, 329), 329: (0, 330), 330: (0, 331), 331: (0, 332), 332: (0, 333), 333: (0, 334), 334: (0, 335), 335: (0, 336), 336: (0, 337), 337: (0, 338), 338: (0, 339), 339: (0, 340), 340: (0, 341), 341: (0, 342), 342: (0, 343), 343: (0, 344), 344: (0, 345), 345: (0, 346), 346: (0, 347), 347: (0, 348), 348: (0, 349), 349: (0, 350), 350: (0, 351), 351: (0, 352), 352: (0, 353), 353: (0, 354), 354: (0, 355), 355: (0, 356), 356: (0, 357), 357: (0, 358), 358: (0, 359), 359: (0, 360), 360: (0, 361), 361: (0, 362), 362: (0, 363), 363: (0, 364), 364: (0, 365), 365: (0, 366), 366: (0, 367), 367: (0, 368), 368: (0, 369), 369: (0, 370), 370: (0, 371), 371: (0, 372), 372: (0, 373), 373: (0, 374), 374: (0, 375), 375: (0, 376), 376: (0, 377), 377: (0, 378), 378: (0, 379), 379: (0, 380), 380: (0, 381), 381: (0, 382), 382: (0, 383), 383: (0, 384), 384: (0, 385), 385: (0, 386), 386: (0, 387), 387: (0, 388), 388: (0, 389), 389: (0, 390), 390: (0, 391), 391: (0, 392), 392: (0, 393), 393: (0, 394), 394: (0, 395), 395: (0, 396), 396: (0, 397), 397: (0, 398), 398: (0, 399), 399: (0, 400), 400: (0, 401), 401: (0, 402), 402: (0, 403), 403: (0, 404), 404: (0, 405), 405: (0, 406), 406: (0, 407), 407: (0, 408), 408: (0, 409), 409: (0, 410), 410: (0, 411), 411: (0, 412), 412: (0, 413), 413: (0, 414), 414: (0, 415), 415: (0, 416), 416: (0, 417), 417: (0, 418), 418: (0, 419), 419: (0, 420), 420: (0, 421), 421: (0, 422), 422: (0, 423), 423: (0, 424), 424: (0, 425), 425: (0, 426), 426: (0, 427), 427: (0, 428), 428: (0, 429), 429: (0, 430), 430: (0, 431), 431: (0, 432), 432: (0, 433), 433: (0, 434), 434: (0, 435), 435: (0, 436), 436: (0, 437), 437: (0, 438), 438: (0, 439), 439: (0, 440), 440: (0, 441), 441: (0, 442), 442: (0, 443), 443: (0, 444), 444: (0, 445), 445: (0, 446), 446: (0, 447), 447: (0, 448), 448: (0, 449), 449: (0, 450), 450: (0, 451), 451: (0, 452), 452: (0, 453), 453: (0, 454), 454: (0, 455), 455: (0, 456), 456: (0, 457), 457: (0, 458), 458: (0, 459), 459: (0, 460), 460: (0, 461), 461: (0, 462), 462: (0, 463), 463: (0, 464), 464: (0, 465), 465: (0, 466), 466: (0, 467), 467: (0, 468), 468: (0, 469), 469: (0, 470), 470: (0, 471), 471: (0, 472), 472: (0, 473), 473: (0, 474), 474: (0, 475), 475: (0, 476), 476: (0, 477), 477: (0, 478), 478: (0, 479), 479: (0, 480), 480: (0, 481), 481: (0, 482), 482: (0, 483), 483: (0, 484), 484: (0, 485), 485: (0, 486), 486: (0, 487), 487: (0, 488), 488: (0, 489), 489: (0, 490), 490: (0, 491), 491: (0, 492), 492: (0, 493), 493: (0, 494), 494: (0, 495), 495: (0, 496), 496: (0, 497), 497: (0, 498), 498: (0, 499), 499: (0, 500), 500: (0, 501), 501: (0, 502), 502: (0, 503), 503: (0, 504), 504: (0, 505), 505: (0, 506), 506: (0, 507), 507: (0, 508), 508: (0, 509), 509: (0, 510), 510: (0, 511), 511: (0, 512), 512: (0, 513), 513: (0, 514), 514: (0, 515), 515: (0, 516), 516: (0, 517), 517: (0, 518), 518: (0, 519), 519: (0, 520), 520: (0, 521), 521: (0, 522), 522: (0, 523), 523: (0, 524), 524: (0, 525), 525: (0, 526), 526: (0, 527), 527: (0, 528), 528: (0, 529), 529: (0, 530), 530: (0, 531), 531: (0, 532), 532: (0, 533), 533: (0, 534), 534: (0, 535), 535: (0, 536), 536: (0, 537), 537: (0, 538), 538: (0, 539), 539: (0, 540), 540: (0, 541), 541: (0, 542), 542: (0, 543), 543: (0, 544), 544: (0, 545), 545: (0, 546), 546: (0, 547), 547: (0, 548), 548: (0, 549), 549: (0, 550), 550: (0, 551), 551: (0, 552), 552: (0, 553), 553: (0, 554), 554: (0, 555), 555: (0, 556), 556: (0, 557), 557: (0, 558), 558: (0, 559), 559: (0, 560), 560: (0, 561), 561: (0, 562), 562: (0, 563), 563: (0, 564), 564: (0, 565), 565: (0, 566), 566: (0, 567), 567: (0, 568), 568: (0, 569), 569: (0, 570), 570: (0, 571), 571: (0, 572), 572: (0, 573), 573: (0, 574), 574: (0, 575), 575: (0, 576), 576: (0, 577), 577: (0, 578), 578: (0, 579), 579: (0, 580), 580: (0, 581), 581: (0, 582), 582: (0, 583), 583: (0, 584), 584: (0, 585), 585: (0, 586), 586: (0, 587), 587: (0, 588), 588: (0, 589), 589: (0, 590), 590: (0, 591), 591: (0, 592), 592: (0, 593), 593: (0, 594), 594: (0, 595), 595: (0, 596), 596: (0, 597), 597: (0, 598), 598: (0, 599), 599: (0, 600), 600: (0, 601), 601: (0, 602), 602: (0, 603), 603: (0, 604), 604: (0, 605), 605: (0, 606), 606: (0, 607), 607: (0, 608), 608: (0, 609), 609: (0, 610), 610: (0, 611), 611: (0, 612), 612: (0, 613), 613: (0, 614), 614: (0, 615), 615: (0, 616), 616: (0, 617), 617: (0, 618), 618: (0, 619), 619: (0, 620), 620: (0, 621), 621: (0, 622), 622: (0, 623), 623: (0, 624), 624: (0, 625), 625: (0, 626), 626: (0, 627), 627: (0, 628), 628: (0, 629), 629: (0, 630), 630: (0, 631), 631: (0, 632), 632: (0, 633), 633: (0, 634), 634: (0, 635), 635: (0, 636), 636: (0, 637), 637: (0, 638), 638: (0, 639), 639: (0, 640), 640: (0, 641), 641: (0, 642), 642: (0, 643), 643: (0, 644), 644: (0, 645), 645: (0, 646), 646: (0, 647), 647: (0, 648), 648: (0, 649), 649: (0, 650), 650: (0, 651), 651: (0, 652), 652: (0, 653), 653: (0, 654), 654: (0, 655), 655: (0, 656), 656: (0, 657), 657: (0, 658), 658: (0, 659), 659: (0, 660), 660: (0, 661), 661: (0, 662), 662: (0, 663), 663: (0, 664), 664: (0, 665), 665: (0, 666), 666: (0, 667), 667: (0, 668), 668: (0, 669), 669: (0, 670), 670: (0, 671), 671: (0, 672), 672: (0, 673), 673: (0, 674), 674: (0, 675), 675: (0, 676), 676: (0, 677), 677: (0, 678), 678: (0, 679), 679: (0, 680), 680: (0, 681), 681: (0, 682), 682: (0, 683), 683: (0, 684), 684: (0, 685), 685: (0, 686), 686: (0, 687), 687: (0, 688), 688: (0, 689), 689: (0, 690), 690: (0, 691), 691: (0, 692), 692: (0, 693), 693: (0, 694), 694: (0, 695)}\n",
      "Generated self._MapperTrialcode2TrialToTrial!\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "Extracted successfully for session:  0\n",
      "session:  1\n",
      "Beh Sessions that exist on this date:  {230616: [(1, 'priminvar5b'), (2, 'priminvar5b')]}\n",
      "------------------------------\n",
      "Loading this neural session: 1\n",
      "Loading these beh expts: ['priminvar5b']\n",
      "Loading these beh sessions: [2]\n",
      "Using this beh_trial_map_list: [(1, 0)]\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230616*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-110840\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004\n",
      "{'filename_components_hyphened': ['Diego', '230616', '134004'], 'basedirs': ['/mnt/Freiwald/ltian/recordings/Diego', '/mnt/Freiwald/ltian/recordings/Diego/230616'], 'basedirs_filenames': ['230616', 'Diego-230616-134004'], 'filename_final_ext': 'Diego-230616-134004', 'filename_final_noext': 'Diego-230616-134004'}\n",
      "FOund this path for spikes:  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004/spikes_tdt_quick-4\n",
      "== PATHS for this expt: \n",
      "raws  --  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004\n",
      "tank  --  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004/Diego-230616-134004\n",
      "spikes  --  /mnt/Freiwald/ltian/recordings/Diego/230616/Diego-230616-134004/spikes_tdt_quick-4\n",
      "final_dir_name  --  Diego-230616-134004\n",
      "time  --  134004\n",
      "pathbase_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004\n",
      "tank_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/data_tank.pkl\n",
      "spikes_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/data_spikes.pkl\n",
      "datall_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/data_datall.pkl\n",
      "events_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/events_photodiode.pkl\n",
      "mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/mapper_st2dat.pkl\n",
      "figs_local  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/figs\n",
      "metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego\n",
      "cached_dir  --  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/cached\n",
      "Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego/230616.yaml\n",
      "updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')\n",
      "[_sitesdirty_update] skipping! since did not find:  sites_error_spikes\n",
      "Printing whether spikes gotten (o) or not (-) because of spike peak to trough\n",
      "== Loading TDT tank\n",
      "** Loading tank data from local (previusly cached)\n",
      "== Done\n",
      "== Trying to load events data\n",
      "Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Diego/230616/Diego-230616-134004/events_photodiode.pkl\n",
      "== Done\n",
      "** MINIMAL_LOADING, therefore loading previuosly cached data\n",
      "=== CLEANING UP self.Dat (_cleanup_reloading_saved_state) ===== \n",
      "0 _behclass_alignsim_compute\n",
      "Running D._behclass_tokens_extract_datsegs\n",
      "0 _behclass_tokens_extract_datsegs\n",
      "stored in self.Dat[BehClass]\n",
      "- starting/ending len (grouping params):\n",
      "92\n",
      "92\n",
      "- starting/ending len (getting sequence):\n",
      "92\n",
      "92\n",
      "--- Removing nans\n",
      "start len: 92\n",
      "- num names for each col\n",
      "not removing nans, since columns=[]\n",
      "ADded new column: supervision_online\n",
      "Reassigned train/test, using key: probe\n",
      "and values:\n",
      "Train =  [0]\n",
      "Test =  [1]\n",
      " \n",
      "New distribution of train/test:\n",
      "train    92\n",
      "Name: monkey_train_or_test, dtype: int64\n",
      "Appended column: los_info\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_SEQUENCE_ALPHA]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "Appended self.Dat[superv_VISUALFB_METH]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_new\n",
      "[taskgroup_reassign_by_mapper], reassigned values in column: taskgroup\n",
      "GROUPING epoch\n",
      "GROUPING_LEVELS ['230616']\n",
      "FEATURE_NAMES ['hdoffline', 'num_strokes_beh', 'num_strokes_task', 'circ', 'dist']\n",
      "SCORE_COL_NAMES []\n",
      "appended col to self.Dat:\n",
      "date_epoch\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_concise\n",
      "Append column to self.Dat:  supervision_stage_semantic\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "... Generated these...\n",
      "self.BehTrialMapList [(1, 0)]\n",
      "self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27), 27: (0, 28), 28: (0, 29), 29: (0, 30), 30: (0, 31), 31: (0, 32), 32: (0, 33), 33: (0, 34), 34: (0, 35), 35: (0, 36), 36: (0, 37), 37: (0, 38), 38: (0, 39), 39: (0, 40), 40: (0, 41), 41: (0, 42), 42: (0, 43), 43: (0, 44), 44: (0, 45), 45: (0, 46), 46: (0, 47), 47: (0, 48), 48: (0, 49), 49: (0, 50), 50: (0, 51), 51: (0, 52), 52: (0, 53), 53: (0, 54), 54: (0, 55), 55: (0, 56), 56: (0, 57), 57: (0, 58), 58: (0, 59), 59: (0, 60), 60: (0, 61), 61: (0, 62), 62: (0, 63), 63: (0, 64), 64: (0, 65), 65: (0, 66), 66: (0, 67), 67: (0, 68), 68: (0, 69), 69: (0, 70), 70: (0, 71), 71: (0, 72), 72: (0, 73), 73: (0, 74), 74: (0, 75), 75: (0, 76), 76: (0, 77), 77: (0, 78), 78: (0, 79), 79: (0, 80), 80: (0, 81), 81: (0, 82), 82: (0, 83), 83: (0, 84), 84: (0, 85), 85: (0, 86), 86: (0, 87), 87: (0, 88), 88: (0, 89), 89: (0, 90), 90: (0, 91), 91: (0, 92), 92: (0, 93), 93: (0, 94), 94: (0, 95), 95: (0, 96), 96: (0, 97), 97: (0, 98), 98: (0, 99), 99: (0, 100), 100: (0, 101), 101: (0, 102), 102: (0, 103), 103: (0, 104), 104: (0, 105), 105: (0, 106), 106: (0, 107), 107: (0, 108), 108: (0, 109), 109: (0, 110), 110: (0, 111), 111: (0, 112), 112: (0, 113), 113: (0, 114), 114: (0, 115), 115: (0, 116), 116: (0, 117), 117: (0, 118), 118: (0, 119), 119: (0, 120), 120: (0, 121), 121: (0, 122), 122: (0, 123), 123: (0, 124), 124: (0, 125), 125: (0, 126), 126: (0, 127), 127: (0, 128), 128: (0, 129), 129: (0, 130), 130: (0, 131), 131: (0, 132), 132: (0, 133), 133: (0, 134), 134: (0, 135), 135: (0, 136), 136: (0, 137), 137: (0, 138), 138: (0, 139), 139: (0, 140), 140: (0, 141)}\n",
      "Generated self._MapperTrialcode2TrialToTrial!\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "Extracted successfully for session:  1\n",
      "Generated index mappers!\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/DfScalar.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/fr_sm_times.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/DS.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/Params.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/ParamsGlobals.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/Sites.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_0/Trials.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/DfScalar.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/fr_sm_times.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/DS.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/Params.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/ParamsGlobals.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/Sites.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bysubstroke/Diego-230616-sess_1/Trials.pkl\n",
      "This many vals across loaded session\n",
      "0 : 460928\n",
      "1 : 82800\n",
      "Done!, new len of dataset 569\n",
      "tests passed\n",
      "Assigning to SP.Params this item:\n",
      "{'which_level': 'substroke', '_list_events': ['00_substrk'], 'list_events_uniqnames': ['00_substrk'], 'list_features_extraction': ['shape', 'index_within_stroke', 'circularity_binned', 'distcum_binned', 'angle_binned', 'dist_angle'], 'list_features_get_conjunction': [], 'list_pre_dur': [-0.6], 'list_post_dur': [0.6], 'map_var_to_othervars': None, 'strokes_only_keep_single': None, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'SPIKES_VERSION': 'tdt', 'map_var_to_levels': None}\n",
      "tests passed\n",
      "Assigning to SP.ParamsGlobals this item:\n",
      "{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.6, 'POST_DUR_CALC': 0.6, 'list_events': ['00_substrk'], 'list_pre_dur': [-0.6], 'list_post_dur': [0.6]}\n",
      "Keeping this many sites that pass fr thresh:\n",
      "409 / 409\n",
      "Using threshold:  1.5\n",
      "Updated self.Sites\n",
      "Done!, new len of dataset 581\n",
      "Dataset preprocess, these params:\n",
      "{'DO_CHARSEQ_VER': None, 'EXTRACT_EPOCHSETS': False, 'EXTRACT_EPOCHSETS_trial_label': None, 'EXTRACT_EPOCHSETS_n_max_epochs': None, 'EXTRACT_EPOCHSETS_merge_sets': None, 'taskgroup_reassign_simple_neural': False, 'preprocess_steps_append': ['remove_online_abort', 'beh_strokes_at_least_one'], 'remove_aborts': False, 'list_superv_keep': None, 'list_superv_keep_full': None, 'DO_SCORE_SEQUENCE_VER': None, 'list_epoch_merge': [], 'epoch_merge_key': None, 'DO_EXTRACT_EPOCHKIND': False, 'datasetstrokes_extract_to_prune_trial': None, 'datasetstrokes_extract_to_prune_stroke': None, 'substrokes_features_do_extraction': True}\n",
      "Appended columns gridsize!\n",
      "Num nan/total, for angle_overall\n",
      "580 / 581\n",
      "Num nan/total, for num_strokes_beh\n",
      "0 / 581\n",
      "Num nan/total, for num_strokes_task\n",
      "0 / 581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gorilla1/code/pythonlib/pythonlib/drawmodel/features.py:183: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return [1-p/t for p,t in zip(displace,distance)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nan/total, for circ\n",
      "1 / 581\n",
      "Num nan/total, for dist\n",
      "0 / 581\n",
      "Added these features:\n",
      "['FEAT_angle_overall', 'FEAT_num_strokes_beh', 'FEAT_num_strokes_task', 'FEAT_circ', 'FEAT_dist']\n",
      ".. Appended new column 'char_seq', version: task_matlab\n",
      "Starting length of D.Dat: 581\n",
      "--BEFORE REMOVE; existing supervision_stage_concise:\n",
      "off|0||1111|0    581\n",
      "Name: supervision_stage_concise, dtype: int64\n",
      "############ TAKING ONLY NO SUPERVISION TRIALS\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['no_supervision']\n",
      "-- Len of D, before applying this param: no_supervision, ... 581\n",
      "after: 581\n",
      "Dataset final len: 581\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['remove_online_abort', 'beh_strokes_at_least_one']\n",
      "-- Len of D, before applying this param: remove_online_abort, ... 581\n",
      "after: 567\n",
      "-- Len of D, before applying this param: beh_strokes_at_least_one, ... 567\n",
      "after: 567\n",
      "SAVING at:  /gorilla1/analyses/main/substrokes_preprocess/Diego_230616_priminvar5b\n",
      "clean_preprocess_data...\n",
      "len of DS.Dat = 1308, before running... stroke_too_short\n",
      "Doing...: stroke_too_short\n",
      "New len:  1301\n",
      "len of DS.Dat = 1301, before running... stroke_too_quick\n",
      "Doing...: stroke_too_quick\n",
      "New len:  1301\n",
      "['index_within_stroke|distcum_binned|angle_binned|circ_signed_binned|velocity_binned|shape', \"(0, 1.0, 1.0, 2.0, 1.0, 'Lcentered-4-1-0') : 4\", \"(0, 1.0, 1.0, 2.0, 1.0, 'Lcentered-4-2-0') : 3\", \"(0, 1.0, 1.0, 2.0, 1.0, 'squiggle3-3-1-1') : 15\", \"(0, 1.0, 1.0, 2.0, 1.0, 'zigzagSq-1-2-1') : 3\", \"(0, 1.0, 1.0, 2.0, 2.0, 'Lcentered-4-2-0') : 9\", \"(0, 1.0, 2.0, 2.0, 1.0, 'Lcentered-4-3-0') : 19\", \"(0, 1.0, 2.0, 2.0, 1.0, 'V-2-2-0') : 1\", \"(0, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-1-1') : 1\", \"(0, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-2-0') : 20\", \"(0, 1.0, 2.0, 2.0, 1.0, 'usquare-1-2-0') : 12\", \"(0, 1.0, 2.0, 2.0, 1.0, 'zigzagSq-1-1-0') : 15\", \"(0, 1.0, 2.0, 2.0, 2.0, 'V-2-2-0') : 3\", \"(0, 1.0, 2.0, 2.0, 2.0, 'squiggle3-3-2-0') : 1\", \"(0, 1.0, 2.0, 2.0, 2.0, 'usquare-1-2-0') : 2\", \"(0, 1.0, 3.0, 2.0, 1.0, 'Lcentered-4-3-0') : 2\", \"(0, 1.0, 3.0, 2.0, 1.0, 'usquare-1-2-0') : 9\", \"(0, 1.0, 3.0, 2.0, 1.0, 'zigzagSq-1-1-0') : 5\", \"(0, 1.0, 4.0, 2.0, 1.0, 'arcdeep-4-3-0') : 1\", \"(0, 1.0, 5.0, 2.0, 1.0, 'V-2-3-0') : 6\", \"(0, 1.0, 5.0, 2.0, 1.0, 'arcdeep-4-3-0') : 20\", \"(0, 1.0, 5.0, 2.0, 1.0, 'circle-6-1-0') : 12\", \"(0, 1.0, 5.0, 2.0, 1.0, 'usquare-1-3-0') : 3\", \"(0, 1.0, 5.0, 2.0, 2.0, 'V-2-3-0') : 2\", \"(0, 1.0, 5.0, 2.0, 2.0, 'arcdeep-4-3-0') : 2\", \"(0, 1.0, 5.0, 2.0, 2.0, 'circle-6-1-0') : 3\", \"(0, 1.0, 5.0, 2.0, 2.0, 'usquare-1-3-0') : 3\", \"(0, 1.0, 6.0, 2.0, 1.0, 'Lcentered-4-4-0') : 10\", \"(0, 1.0, 6.0, 2.0, 1.0, 'circle-6-1-0') : 3\", \"(0, 1.0, 6.0, 2.0, 1.0, 'line-8-2-0') : 9\", \"(0, 1.0, 6.0, 2.0, 1.0, 'zigzagSq-1-1-1') : 14\", \"(0, 1.0, 6.0, 2.0, 2.0, 'Lcentered-4-4-0') : 2\", \"(0, 1.0, 6.0, 2.0, 2.0, 'circle-6-1-0') : 3\", \"(0, 1.0, 6.0, 3.0, 1.0, 'circle-6-1-0') : 1\", \"(0, 1.0, 7.0, 2.0, 1.0, 'Lcentered-4-4-0') : 10\", \"(0, 1.0, 7.0, 2.0, 1.0, 'V-2-4-0') : 6\", \"(0, 1.0, 7.0, 2.0, 1.0, 'line-8-2-0') : 14\", \"(0, 1.0, 7.0, 2.0, 1.0, 'line-8-4-0') : 7\", \"(0, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-1-0') : 11\", \"(0, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-2-1') : 21\", \"(0, 1.0, 7.0, 2.0, 1.0, 'zigzagSq-1-1-1') : 8\", \"(0, 1.0, 7.0, 2.0, 2.0, 'squiggle3-3-1-0') : 8\", \"(0, 1.0, 7.0, 2.0, 2.0, 'zigzagSq-1-2-0') : 11\", \"(0, 1.0, 8.0, 2.0, 1.0, 'Lcentered-4-1-0') : 18\", \"(0, 1.0, 8.0, 2.0, 1.0, 'line-8-4-0') : 13\", \"(0, 1.0, 8.0, 2.0, 1.0, 'squiggle3-3-1-1') : 1\", \"(0, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-2-1') : 17\", \"(0, 1.0, 8.0, 2.0, 2.0, 'Lcentered-4-2-0') : 10\", \"(0, 1.0, 8.0, 2.0, 2.0, 'zigzagSq-1-2-0') : 10\", \"(0, 1.0, 8.0, 3.0, 1.0, 'zigzagSq-1-2-1') : 2\", \"(0, 2.0, 1.0, 1.0, 1.0, 'arcdeep-4-2-0') : 22\", \"(0, 2.0, 1.0, 2.0, 1.0, 'line-8-1-0') : 10\", \"(0, 2.0, 1.0, 2.0, 1.0, 'line-8-3-0') : 15\", \"(0, 2.0, 1.0, 2.0, 2.0, 'line-8-1-0') : 1\", \"(0, 2.0, 2.0, 2.0, 1.0, 'Lcentered-4-3-0') : 1\", \"(0, 2.0, 2.0, 2.0, 1.0, 'V-2-2-0') : 3\", \"(0, 2.0, 2.0, 2.0, 1.0, 'line-8-3-0') : 7\", \"(0, 2.0, 2.0, 2.0, 2.0, 'V-2-2-0') : 17\", \"(0, 2.0, 5.0, 2.0, 1.0, 'V-2-3-0') : 2\", \"(0, 2.0, 5.0, 2.0, 1.0, 'usquare-1-3-0') : 3\", \"(0, 2.0, 5.0, 2.0, 2.0, 'V-2-3-0') : 12\", \"(0, 2.0, 5.0, 2.0, 2.0, 'usquare-1-3-0') : 12\", \"(0, 2.0, 7.0, 2.0, 1.0, 'V-2-4-0') : 16\", \"(0, 2.0, 7.0, 2.0, 1.0, 'arcdeep-4-4-0') : 4\", \"(0, 2.0, 7.0, 2.0, 1.0, 'line-8-4-0') : 2\", \"(0, 2.0, 7.0, 3.0, 1.0, 'arcdeep-4-4-0') : 8\", \"(0, 2.0, 7.0, 3.0, 1.0, 'usquare-1-4-0') : 15\", \"(0, 2.0, 7.0, 3.0, 2.0, 'usquare-1-4-0') : 7\", \"(0, 2.0, 8.0, 2.0, 1.0, 'arcdeep-4-4-0') : 1\", \"(0, 2.0, 8.0, 2.0, 1.0, 'line-8-1-0') : 10\", \"(0, 2.0, 8.0, 2.0, 2.0, 'line-8-1-0') : 1\", \"(0, 2.0, 8.0, 3.0, 1.0, 'arcdeep-4-4-0') : 10\", \"(1, 1.0, 1.0, 1.0, 1.0, 'squiggle3-3-2-1') : 9\", \"(1, 1.0, 1.0, 2.0, 1.0, 'Lcentered-4-4-0') : 4\", \"(1, 1.0, 1.0, 2.0, 1.0, 'squiggle3-3-2-1') : 1\", \"(1, 1.0, 1.0, 2.0, 2.0, 'Lcentered-4-3-0') : 5\", \"(1, 1.0, 1.0, 2.0, 2.0, 'usquare-1-2-0') : 11\", \"(1, 1.0, 1.0, 3.0, 1.0, 'squiggle3-3-2-1') : 1\", \"(1, 1.0, 2.0, 2.0, 1.0, 'Lcentered-4-1-0') : 7\", \"(1, 1.0, 2.0, 2.0, 1.0, 'V-2-4-0') : 16\", \"(1, 1.0, 2.0, 2.0, 1.0, 'arcdeep-4-4-0') : 21\", \"(1, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-1-1') : 1\", \"(1, 1.0, 2.0, 2.0, 1.0, 'usquare-1-4-0') : 9\", \"(1, 1.0, 2.0, 2.0, 2.0, 'arcdeep-4-4-0') : 1\", \"(1, 1.0, 2.0, 2.0, 2.0, 'usquare-1-4-0') : 9\", \"(1, 1.0, 2.0, 3.0, 1.0, 'arcdeep-4-4-0') : 1\", \"(1, 1.0, 3.0, 2.0, 1.0, 'zigzagSq-1-2-1') : 8\", \"(1, 1.0, 3.0, 2.0, 2.0, 'zigzagSq-1-2-1') : 1\", \"(1, 1.0, 5.0, 2.0, 1.0, 'squiggle3-3-1-0') : 17\", \"(1, 1.0, 5.0, 2.0, 1.0, 'zigzagSq-1-2-0') : 5\", \"(1, 1.0, 5.0, 2.0, 2.0, 'squiggle3-3-1-0') : 1\", \"(1, 1.0, 5.0, 2.0, 2.0, 'zigzagSq-1-2-0') : 2\", \"(1, 1.0, 5.0, 3.0, 2.0, 'squiggle3-3-1-0') : 1\", \"(1, 1.0, 6.0, 2.0, 1.0, 'Lcentered-4-2-0') : 1\", \"(1, 1.0, 6.0, 2.0, 1.0, 'line-8-2-0') : 1\", \"(1, 1.0, 6.0, 2.0, 1.0, 'zigzagSq-1-2-0') : 7\", \"(1, 1.0, 6.0, 2.0, 2.0, 'zigzagSq-1-2-0') : 5\", \"(1, 1.0, 6.0, 3.0, 1.0, 'arcdeep-4-3-0') : 1\", \"(1, 1.0, 6.0, 3.0, 1.0, 'zigzagSq-1-2-0') : 2\", \"(1, 1.0, 7.0, 2.0, 1.0, 'Lcentered-4-2-0') : 19\", \"(1, 1.0, 7.0, 2.0, 1.0, 'V-2-2-0') : 3\", \"(1, 1.0, 7.0, 2.0, 1.0, 'arcdeep-4-2-0') : 22\", \"(1, 1.0, 7.0, 2.0, 1.0, 'arcdeep-4-3-0') : 5\", \"(1, 1.0, 7.0, 2.0, 1.0, 'line-8-2-0') : 19\", \"(1, 1.0, 7.0, 2.0, 1.0, 'line-8-4-0') : 14\", \"(1, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-2-0') : 21\", \"(1, 1.0, 7.0, 2.0, 2.0, 'Lcentered-4-2-0') : 1\", \"(1, 1.0, 7.0, 2.0, 2.0, 'V-2-2-0') : 1\", \"(1, 1.0, 7.0, 2.0, 2.0, 'arcdeep-4-3-0') : 1\", \"(1, 1.0, 7.0, 2.0, 2.0, 'line-8-2-0') : 3\", \"(1, 1.0, 7.0, 2.0, 2.0, 'line-8-4-0') : 2\", \"(1, 1.0, 7.0, 3.0, 1.0, 'arcdeep-4-3-0') : 12\", \"(1, 1.0, 7.0, 3.0, 2.0, 'arcdeep-4-3-0') : 1\", \"(1, 1.0, 7.0, 3.0, 2.0, 'circle-6-1-0') : 1\", \"(1, 1.0, 8.0, 2.0, 1.0, 'Lcentered-4-3-0') : 7\", \"(1, 1.0, 8.0, 2.0, 1.0, 'Lcentered-4-4-0') : 16\", \"(1, 1.0, 8.0, 2.0, 1.0, 'V-2-3-0') : 8\", \"(1, 1.0, 8.0, 2.0, 1.0, 'line-8-4-0') : 4\", \"(1, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-1-0') : 17\", \"(1, 1.0, 8.0, 2.0, 2.0, 'Lcentered-4-3-0') : 5\", \"(1, 1.0, 8.0, 2.0, 2.0, 'Lcentered-4-4-0') : 1\", \"(1, 1.0, 8.0, 2.0, 2.0, 'line-8-4-0') : 2\", \"(1, 1.0, 8.0, 2.0, 2.0, 'usquare-1-2-0') : 10\", \"(1, 1.0, 8.0, 3.0, 1.0, 'circle-6-1-0') : 2\", \"(1, 1.0, 8.0, 3.0, 2.0, 'circle-6-1-0') : 3\", \"(1, 2.0, 1.0, 1.0, 1.0, 'squiggle3-3-2-1') : 8\", \"(1, 2.0, 1.0, 1.0, 1.0, 'zigzagSq-1-1-1') : 6\", \"(1, 2.0, 1.0, 1.0, 2.0, 'zigzagSq-1-1-1') : 11\", \"(1, 2.0, 1.0, 2.0, 2.0, 'Lcentered-4-3-0') : 4\", \"(1, 2.0, 1.0, 2.0, 2.0, 'usquare-1-2-0') : 2\", \"(1, 2.0, 1.0, 3.0, 1.0, 'squiggle3-3-2-1') : 2\", \"(1, 2.0, 1.0, 3.0, 2.0, 'zigzagSq-1-1-1') : 1\", \"(1, 2.0, 2.0, 1.0, 1.0, 'squiggle3-3-1-1') : 9\", \"(1, 2.0, 2.0, 2.0, 1.0, 'Lcentered-4-1-0') : 9\", \"(1, 2.0, 2.0, 2.0, 1.0, 'V-2-4-0') : 5\", \"(1, 2.0, 2.0, 2.0, 1.0, 'squiggle3-3-1-1') : 6\", \"(1, 2.0, 2.0, 2.0, 2.0, 'Lcentered-4-1-0') : 5\", \"(1, 2.0, 2.0, 2.0, 2.0, 'V-2-4-0') : 1\", \"(1, 2.0, 2.0, 2.0, 2.0, 'squiggle3-3-1-1') : 4\", \"(1, 2.0, 2.0, 2.0, 2.0, 'usquare-1-4-0') : 4\", \"(1, 2.0, 2.0, 3.0, 1.0, 'squiggle3-3-1-1') : 1\", \"(1, 2.0, 2.0, 3.0, 2.0, 'Lcentered-4-1-0') : 1\", \"(1, 2.0, 3.0, 1.0, 1.0, 'squiggle3-3-1-1') : 1\", \"(1, 2.0, 3.0, 2.0, 1.0, 'zigzagSq-1-2-1') : 8\", \"(1, 2.0, 3.0, 2.0, 2.0, 'zigzagSq-1-2-1') : 5\", \"(1, 2.0, 7.0, 2.0, 1.0, 'Lcentered-4-2-0') : 1\", \"(1, 2.0, 7.0, 2.0, 1.0, 'V-2-2-0') : 20\", \"(1, 2.0, 7.0, 3.0, 1.0, 'arcdeep-4-3-0') : 1\", \"(1, 2.0, 7.0, 3.0, 1.0, 'usquare-1-3-0') : 1\", \"(1, 2.0, 7.0, 3.0, 2.0, 'arcdeep-4-3-0') : 2\", \"(1, 2.0, 8.0, 1.0, 2.0, 'zigzagSq-1-1-1') : 4\", \"(1, 2.0, 8.0, 2.0, 1.0, 'V-2-3-0') : 10\", \"(1, 2.0, 8.0, 2.0, 1.0, 'circle-6-1-0') : 1\", \"(1, 2.0, 8.0, 2.0, 1.0, 'usquare-1-3-0') : 4\", \"(1, 2.0, 8.0, 2.0, 1.0, 'zigzagSq-1-1-0') : 2\", \"(1, 2.0, 8.0, 2.0, 2.0, 'Lcentered-4-3-0') : 1\", \"(1, 2.0, 8.0, 2.0, 2.0, 'Lcentered-4-4-0') : 1\", \"(1, 2.0, 8.0, 2.0, 2.0, 'V-2-3-0') : 4\", \"(1, 2.0, 8.0, 2.0, 2.0, 'usquare-1-3-0') : 3\", \"(1, 2.0, 8.0, 2.0, 2.0, 'zigzagSq-1-1-0') : 1\", \"(1, 2.0, 8.0, 3.0, 1.0, 'usquare-1-3-0') : 10\", \"(1, 2.0, 8.0, 3.0, 2.0, 'circle-6-1-0') : 15\", \"(1, 2.0, 8.0, 3.0, 2.0, 'usquare-1-3-0') : 3\", \"(2, 1.0, 1.0, 2.0, 1.0, 'squiggle3-3-2-0') : 3\", \"(2, 1.0, 1.0, 2.0, 1.0, 'zigzagSq-1-2-0') : 1\", \"(2, 1.0, 1.0, 2.0, 1.0, 'zigzagSq-1-2-1') : 1\", \"(2, 1.0, 1.0, 3.0, 1.0, 'squiggle3-3-2-0') : 5\", \"(2, 1.0, 2.0, 1.0, 1.0, 'zigzagSq-1-1-0') : 3\", \"(2, 1.0, 2.0, 2.0, 1.0, 'circle-6-1-0') : 2\", \"(2, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-2-0') : 13\", \"(2, 1.0, 2.0, 2.0, 1.0, 'zigzagSq-1-1-0') : 14\", \"(2, 1.0, 2.0, 2.0, 2.0, 'circle-6-1-0') : 7\", \"(2, 1.0, 2.0, 3.0, 1.0, 'zigzagSq-1-1-0') : 3\", \"(2, 1.0, 3.0, 2.0, 1.0, 'circle-6-1-0') : 1\", \"(2, 1.0, 3.0, 2.0, 2.0, 'circle-6-1-0') : 11\", \"(2, 1.0, 6.0, 2.0, 1.0, 'usquare-1-2-0') : 1\", \"(2, 1.0, 6.0, 2.0, 1.0, 'zigzagSq-1-1-1') : 5\", \"(2, 1.0, 7.0, 2.0, 1.0, 'arcdeep-4-3-0') : 2\", \"(2, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-1-0') : 2\", \"(2, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-2-1') : 18\", \"(2, 1.0, 7.0, 2.0, 1.0, 'usquare-1-2-0') : 20\", \"(2, 1.0, 7.0, 2.0, 1.0, 'zigzagSq-1-1-1') : 16\", \"(2, 1.0, 7.0, 2.0, 2.0, 'usquare-1-2-0') : 2\", \"(2, 1.0, 8.0, 1.0, 1.0, 'zigzagSq-1-2-1') : 1\", \"(2, 1.0, 8.0, 2.0, 1.0, 'arcdeep-4-3-0') : 17\", \"(2, 1.0, 8.0, 2.0, 1.0, 'squiggle3-3-1-0') : 16\", \"(2, 1.0, 8.0, 2.0, 1.0, 'squiggle3-3-2-1') : 2\", \"(2, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-2-0') : 20\", \"(2, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-2-1') : 20\", \"(2, 1.0, 8.0, 2.0, 2.0, 'arcdeep-4-3-0') : 4\", \"(2, 2.0, 2.0, 2.0, 2.0, 'circle-6-1-0') : 1\", \"(2, 2.0, 8.0, 1.0, 2.0, 'squiggle3-3-1-0') : 1\"]\n",
      "Saved to:  /gorilla1/analyses/main/substrokes_preprocess/Diego_230616_priminvar5b/plots_during_anova_params/substroke_features_groupings.txt\n",
      "(0, 1.0, 1.0, 2.0, 1.0, 'Lcentered-4-1-0') :     4\n",
      "(0, 1.0, 1.0, 2.0, 1.0, 'Lcentered-4-2-0') :     3\n",
      "(0, 1.0, 1.0, 2.0, 1.0, 'squiggle3-3-1-1') :     15\n",
      "(0, 1.0, 1.0, 2.0, 1.0, 'zigzagSq-1-2-1') :     3\n",
      "(0, 1.0, 1.0, 2.0, 2.0, 'Lcentered-4-2-0') :     9\n",
      "(0, 1.0, 2.0, 2.0, 1.0, 'Lcentered-4-3-0') :     19\n",
      "(0, 1.0, 2.0, 2.0, 1.0, 'V-2-2-0') :     1\n",
      "(0, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-1-1') :     1\n",
      "(0, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-2-0') :     20\n",
      "(0, 1.0, 2.0, 2.0, 1.0, 'usquare-1-2-0') :     12\n",
      "(0, 1.0, 2.0, 2.0, 1.0, 'zigzagSq-1-1-0') :     15\n",
      "(0, 1.0, 2.0, 2.0, 2.0, 'V-2-2-0') :     3\n",
      "(0, 1.0, 2.0, 2.0, 2.0, 'squiggle3-3-2-0') :     1\n",
      "(0, 1.0, 2.0, 2.0, 2.0, 'usquare-1-2-0') :     2\n",
      "(0, 1.0, 3.0, 2.0, 1.0, 'Lcentered-4-3-0') :     2\n",
      "(0, 1.0, 3.0, 2.0, 1.0, 'usquare-1-2-0') :     9\n",
      "(0, 1.0, 3.0, 2.0, 1.0, 'zigzagSq-1-1-0') :     5\n",
      "(0, 1.0, 4.0, 2.0, 1.0, 'arcdeep-4-3-0') :     1\n",
      "(0, 1.0, 5.0, 2.0, 1.0, 'V-2-3-0') :     6\n",
      "(0, 1.0, 5.0, 2.0, 1.0, 'arcdeep-4-3-0') :     20\n",
      "(0, 1.0, 5.0, 2.0, 1.0, 'circle-6-1-0') :     12\n",
      "(0, 1.0, 5.0, 2.0, 1.0, 'usquare-1-3-0') :     3\n",
      "(0, 1.0, 5.0, 2.0, 2.0, 'V-2-3-0') :     2\n",
      "(0, 1.0, 5.0, 2.0, 2.0, 'arcdeep-4-3-0') :     2\n",
      "(0, 1.0, 5.0, 2.0, 2.0, 'circle-6-1-0') :     3\n",
      "(0, 1.0, 5.0, 2.0, 2.0, 'usquare-1-3-0') :     3\n",
      "(0, 1.0, 6.0, 2.0, 1.0, 'Lcentered-4-4-0') :     10\n",
      "(0, 1.0, 6.0, 2.0, 1.0, 'circle-6-1-0') :     3\n",
      "(0, 1.0, 6.0, 2.0, 1.0, 'line-8-2-0') :     9\n",
      "(0, 1.0, 6.0, 2.0, 1.0, 'zigzagSq-1-1-1') :     14\n",
      "(0, 1.0, 6.0, 2.0, 2.0, 'Lcentered-4-4-0') :     2\n",
      "(0, 1.0, 6.0, 2.0, 2.0, 'circle-6-1-0') :     3\n",
      "(0, 1.0, 6.0, 3.0, 1.0, 'circle-6-1-0') :     1\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'Lcentered-4-4-0') :     10\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'V-2-4-0') :     6\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'line-8-2-0') :     14\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'line-8-4-0') :     7\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-1-0') :     11\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-2-1') :     21\n",
      "(0, 1.0, 7.0, 2.0, 1.0, 'zigzagSq-1-1-1') :     8\n",
      "(0, 1.0, 7.0, 2.0, 2.0, 'squiggle3-3-1-0') :     8\n",
      "(0, 1.0, 7.0, 2.0, 2.0, 'zigzagSq-1-2-0') :     11\n",
      "(0, 1.0, 8.0, 2.0, 1.0, 'Lcentered-4-1-0') :     18\n",
      "(0, 1.0, 8.0, 2.0, 1.0, 'line-8-4-0') :     13\n",
      "(0, 1.0, 8.0, 2.0, 1.0, 'squiggle3-3-1-1') :     1\n",
      "(0, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-2-1') :     17\n",
      "(0, 1.0, 8.0, 2.0, 2.0, 'Lcentered-4-2-0') :     10\n",
      "(0, 1.0, 8.0, 2.0, 2.0, 'zigzagSq-1-2-0') :     10\n",
      "(0, 1.0, 8.0, 3.0, 1.0, 'zigzagSq-1-2-1') :     2\n",
      "(0, 2.0, 1.0, 1.0, 1.0, 'arcdeep-4-2-0') :     22\n",
      "(0, 2.0, 1.0, 2.0, 1.0, 'line-8-1-0') :     10\n",
      "(0, 2.0, 1.0, 2.0, 1.0, 'line-8-3-0') :     15\n",
      "(0, 2.0, 1.0, 2.0, 2.0, 'line-8-1-0') :     1\n",
      "(0, 2.0, 2.0, 2.0, 1.0, 'Lcentered-4-3-0') :     1\n",
      "(0, 2.0, 2.0, 2.0, 1.0, 'V-2-2-0') :     3\n",
      "(0, 2.0, 2.0, 2.0, 1.0, 'line-8-3-0') :     7\n",
      "(0, 2.0, 2.0, 2.0, 2.0, 'V-2-2-0') :     17\n",
      "(0, 2.0, 5.0, 2.0, 1.0, 'V-2-3-0') :     2\n",
      "(0, 2.0, 5.0, 2.0, 1.0, 'usquare-1-3-0') :     3\n",
      "(0, 2.0, 5.0, 2.0, 2.0, 'V-2-3-0') :     12\n",
      "(0, 2.0, 5.0, 2.0, 2.0, 'usquare-1-3-0') :     12\n",
      "(0, 2.0, 7.0, 2.0, 1.0, 'V-2-4-0') :     16\n",
      "(0, 2.0, 7.0, 2.0, 1.0, 'arcdeep-4-4-0') :     4\n",
      "(0, 2.0, 7.0, 2.0, 1.0, 'line-8-4-0') :     2\n",
      "(0, 2.0, 7.0, 3.0, 1.0, 'arcdeep-4-4-0') :     8\n",
      "(0, 2.0, 7.0, 3.0, 1.0, 'usquare-1-4-0') :     15\n",
      "(0, 2.0, 7.0, 3.0, 2.0, 'usquare-1-4-0') :     7\n",
      "(0, 2.0, 8.0, 2.0, 1.0, 'arcdeep-4-4-0') :     1\n",
      "(0, 2.0, 8.0, 2.0, 1.0, 'line-8-1-0') :     10\n",
      "(0, 2.0, 8.0, 2.0, 2.0, 'line-8-1-0') :     1\n",
      "(0, 2.0, 8.0, 3.0, 1.0, 'arcdeep-4-4-0') :     10\n",
      "(1, 1.0, 1.0, 1.0, 1.0, 'squiggle3-3-2-1') :     9\n",
      "(1, 1.0, 1.0, 2.0, 1.0, 'Lcentered-4-4-0') :     4\n",
      "(1, 1.0, 1.0, 2.0, 1.0, 'squiggle3-3-2-1') :     1\n",
      "(1, 1.0, 1.0, 2.0, 2.0, 'Lcentered-4-3-0') :     5\n",
      "(1, 1.0, 1.0, 2.0, 2.0, 'usquare-1-2-0') :     11\n",
      "(1, 1.0, 1.0, 3.0, 1.0, 'squiggle3-3-2-1') :     1\n",
      "(1, 1.0, 2.0, 2.0, 1.0, 'Lcentered-4-1-0') :     7\n",
      "(1, 1.0, 2.0, 2.0, 1.0, 'V-2-4-0') :     16\n",
      "(1, 1.0, 2.0, 2.0, 1.0, 'arcdeep-4-4-0') :     21\n",
      "(1, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-1-1') :     1\n",
      "(1, 1.0, 2.0, 2.0, 1.0, 'usquare-1-4-0') :     9\n",
      "(1, 1.0, 2.0, 2.0, 2.0, 'arcdeep-4-4-0') :     1\n",
      "(1, 1.0, 2.0, 2.0, 2.0, 'usquare-1-4-0') :     9\n",
      "(1, 1.0, 2.0, 3.0, 1.0, 'arcdeep-4-4-0') :     1\n",
      "(1, 1.0, 3.0, 2.0, 1.0, 'zigzagSq-1-2-1') :     8\n",
      "(1, 1.0, 3.0, 2.0, 2.0, 'zigzagSq-1-2-1') :     1\n",
      "(1, 1.0, 5.0, 2.0, 1.0, 'squiggle3-3-1-0') :     17\n",
      "(1, 1.0, 5.0, 2.0, 1.0, 'zigzagSq-1-2-0') :     5\n",
      "(1, 1.0, 5.0, 2.0, 2.0, 'squiggle3-3-1-0') :     1\n",
      "(1, 1.0, 5.0, 2.0, 2.0, 'zigzagSq-1-2-0') :     2\n",
      "(1, 1.0, 5.0, 3.0, 2.0, 'squiggle3-3-1-0') :     1\n",
      "(1, 1.0, 6.0, 2.0, 1.0, 'Lcentered-4-2-0') :     1\n",
      "(1, 1.0, 6.0, 2.0, 1.0, 'line-8-2-0') :     1\n",
      "(1, 1.0, 6.0, 2.0, 1.0, 'zigzagSq-1-2-0') :     7\n",
      "(1, 1.0, 6.0, 2.0, 2.0, 'zigzagSq-1-2-0') :     5\n",
      "(1, 1.0, 6.0, 3.0, 1.0, 'arcdeep-4-3-0') :     1\n",
      "(1, 1.0, 6.0, 3.0, 1.0, 'zigzagSq-1-2-0') :     2\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'Lcentered-4-2-0') :     19\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'V-2-2-0') :     3\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'arcdeep-4-2-0') :     22\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'arcdeep-4-3-0') :     5\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'line-8-2-0') :     19\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'line-8-4-0') :     14\n",
      "(1, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-2-0') :     21\n",
      "(1, 1.0, 7.0, 2.0, 2.0, 'Lcentered-4-2-0') :     1\n",
      "(1, 1.0, 7.0, 2.0, 2.0, 'V-2-2-0') :     1\n",
      "(1, 1.0, 7.0, 2.0, 2.0, 'arcdeep-4-3-0') :     1\n",
      "(1, 1.0, 7.0, 2.0, 2.0, 'line-8-2-0') :     3\n",
      "(1, 1.0, 7.0, 2.0, 2.0, 'line-8-4-0') :     2\n",
      "(1, 1.0, 7.0, 3.0, 1.0, 'arcdeep-4-3-0') :     12\n",
      "(1, 1.0, 7.0, 3.0, 2.0, 'arcdeep-4-3-0') :     1\n",
      "(1, 1.0, 7.0, 3.0, 2.0, 'circle-6-1-0') :     1\n",
      "(1, 1.0, 8.0, 2.0, 1.0, 'Lcentered-4-3-0') :     7\n",
      "(1, 1.0, 8.0, 2.0, 1.0, 'Lcentered-4-4-0') :     16\n",
      "(1, 1.0, 8.0, 2.0, 1.0, 'V-2-3-0') :     8\n",
      "(1, 1.0, 8.0, 2.0, 1.0, 'line-8-4-0') :     4\n",
      "(1, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-1-0') :     17\n",
      "(1, 1.0, 8.0, 2.0, 2.0, 'Lcentered-4-3-0') :     5\n",
      "(1, 1.0, 8.0, 2.0, 2.0, 'Lcentered-4-4-0') :     1\n",
      "(1, 1.0, 8.0, 2.0, 2.0, 'line-8-4-0') :     2\n",
      "(1, 1.0, 8.0, 2.0, 2.0, 'usquare-1-2-0') :     10\n",
      "(1, 1.0, 8.0, 3.0, 1.0, 'circle-6-1-0') :     2\n",
      "(1, 1.0, 8.0, 3.0, 2.0, 'circle-6-1-0') :     3\n",
      "(1, 2.0, 1.0, 1.0, 1.0, 'squiggle3-3-2-1') :     8\n",
      "(1, 2.0, 1.0, 1.0, 1.0, 'zigzagSq-1-1-1') :     6\n",
      "(1, 2.0, 1.0, 1.0, 2.0, 'zigzagSq-1-1-1') :     11\n",
      "(1, 2.0, 1.0, 2.0, 2.0, 'Lcentered-4-3-0') :     4\n",
      "(1, 2.0, 1.0, 2.0, 2.0, 'usquare-1-2-0') :     2\n",
      "(1, 2.0, 1.0, 3.0, 1.0, 'squiggle3-3-2-1') :     2\n",
      "(1, 2.0, 1.0, 3.0, 2.0, 'zigzagSq-1-1-1') :     1\n",
      "(1, 2.0, 2.0, 1.0, 1.0, 'squiggle3-3-1-1') :     9\n",
      "(1, 2.0, 2.0, 2.0, 1.0, 'Lcentered-4-1-0') :     9\n",
      "(1, 2.0, 2.0, 2.0, 1.0, 'V-2-4-0') :     5\n",
      "(1, 2.0, 2.0, 2.0, 1.0, 'squiggle3-3-1-1') :     6\n",
      "(1, 2.0, 2.0, 2.0, 2.0, 'Lcentered-4-1-0') :     5\n",
      "(1, 2.0, 2.0, 2.0, 2.0, 'V-2-4-0') :     1\n",
      "(1, 2.0, 2.0, 2.0, 2.0, 'squiggle3-3-1-1') :     4\n",
      "(1, 2.0, 2.0, 2.0, 2.0, 'usquare-1-4-0') :     4\n",
      "(1, 2.0, 2.0, 3.0, 1.0, 'squiggle3-3-1-1') :     1\n",
      "(1, 2.0, 2.0, 3.0, 2.0, 'Lcentered-4-1-0') :     1\n",
      "(1, 2.0, 3.0, 1.0, 1.0, 'squiggle3-3-1-1') :     1\n",
      "(1, 2.0, 3.0, 2.0, 1.0, 'zigzagSq-1-2-1') :     8\n",
      "(1, 2.0, 3.0, 2.0, 2.0, 'zigzagSq-1-2-1') :     5\n",
      "(1, 2.0, 7.0, 2.0, 1.0, 'Lcentered-4-2-0') :     1\n",
      "(1, 2.0, 7.0, 2.0, 1.0, 'V-2-2-0') :     20\n",
      "(1, 2.0, 7.0, 3.0, 1.0, 'arcdeep-4-3-0') :     1\n",
      "(1, 2.0, 7.0, 3.0, 1.0, 'usquare-1-3-0') :     1\n",
      "(1, 2.0, 7.0, 3.0, 2.0, 'arcdeep-4-3-0') :     2\n",
      "(1, 2.0, 8.0, 1.0, 2.0, 'zigzagSq-1-1-1') :     4\n",
      "(1, 2.0, 8.0, 2.0, 1.0, 'V-2-3-0') :     10\n",
      "(1, 2.0, 8.0, 2.0, 1.0, 'circle-6-1-0') :     1\n",
      "(1, 2.0, 8.0, 2.0, 1.0, 'usquare-1-3-0') :     4\n",
      "(1, 2.0, 8.0, 2.0, 1.0, 'zigzagSq-1-1-0') :     2\n",
      "(1, 2.0, 8.0, 2.0, 2.0, 'Lcentered-4-3-0') :     1\n",
      "(1, 2.0, 8.0, 2.0, 2.0, 'Lcentered-4-4-0') :     1\n",
      "(1, 2.0, 8.0, 2.0, 2.0, 'V-2-3-0') :     4\n",
      "(1, 2.0, 8.0, 2.0, 2.0, 'usquare-1-3-0') :     3\n",
      "(1, 2.0, 8.0, 2.0, 2.0, 'zigzagSq-1-1-0') :     1\n",
      "(1, 2.0, 8.0, 3.0, 1.0, 'usquare-1-3-0') :     10\n",
      "(1, 2.0, 8.0, 3.0, 2.0, 'circle-6-1-0') :     15\n",
      "(1, 2.0, 8.0, 3.0, 2.0, 'usquare-1-3-0') :     3\n",
      "(2, 1.0, 1.0, 2.0, 1.0, 'squiggle3-3-2-0') :     3\n",
      "(2, 1.0, 1.0, 2.0, 1.0, 'zigzagSq-1-2-0') :     1\n",
      "(2, 1.0, 1.0, 2.0, 1.0, 'zigzagSq-1-2-1') :     1\n",
      "(2, 1.0, 1.0, 3.0, 1.0, 'squiggle3-3-2-0') :     5\n",
      "(2, 1.0, 2.0, 1.0, 1.0, 'zigzagSq-1-1-0') :     3\n",
      "(2, 1.0, 2.0, 2.0, 1.0, 'circle-6-1-0') :     2\n",
      "(2, 1.0, 2.0, 2.0, 1.0, 'squiggle3-3-2-0') :     13\n",
      "(2, 1.0, 2.0, 2.0, 1.0, 'zigzagSq-1-1-0') :     14\n",
      "(2, 1.0, 2.0, 2.0, 2.0, 'circle-6-1-0') :     7\n",
      "(2, 1.0, 2.0, 3.0, 1.0, 'zigzagSq-1-1-0') :     3\n",
      "(2, 1.0, 3.0, 2.0, 1.0, 'circle-6-1-0') :     1\n",
      "(2, 1.0, 3.0, 2.0, 2.0, 'circle-6-1-0') :     11\n",
      "(2, 1.0, 6.0, 2.0, 1.0, 'usquare-1-2-0') :     1\n",
      "(2, 1.0, 6.0, 2.0, 1.0, 'zigzagSq-1-1-1') :     5\n",
      "(2, 1.0, 7.0, 2.0, 1.0, 'arcdeep-4-3-0') :     2\n",
      "(2, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-1-0') :     2\n",
      "(2, 1.0, 7.0, 2.0, 1.0, 'squiggle3-3-2-1') :     18\n",
      "(2, 1.0, 7.0, 2.0, 1.0, 'usquare-1-2-0') :     20\n",
      "(2, 1.0, 7.0, 2.0, 1.0, 'zigzagSq-1-1-1') :     16\n",
      "(2, 1.0, 7.0, 2.0, 2.0, 'usquare-1-2-0') :     2\n",
      "(2, 1.0, 8.0, 1.0, 1.0, 'zigzagSq-1-2-1') :     1\n",
      "(2, 1.0, 8.0, 2.0, 1.0, 'arcdeep-4-3-0') :     17\n",
      "(2, 1.0, 8.0, 2.0, 1.0, 'squiggle3-3-1-0') :     16\n",
      "(2, 1.0, 8.0, 2.0, 1.0, 'squiggle3-3-2-1') :     2\n",
      "(2, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-2-0') :     20\n",
      "(2, 1.0, 8.0, 2.0, 1.0, 'zigzagSq-1-2-1') :     20\n",
      "(2, 1.0, 8.0, 2.0, 2.0, 'arcdeep-4-3-0') :     4\n",
      "(2, 2.0, 2.0, 2.0, 2.0, 'circle-6-1-0') :     1\n",
      "(2, 2.0, 8.0, 1.0, 2.0, 'squiggle3-3-1-0') :     1\n",
      "Starting len dfscalar:  534972\n",
      "Ending len dfscalar:  527610\n",
      " --- Pruning SP.DfScalar to match DS... start len:  527610\n",
      "... End len:  525565\n",
      "Appending...  aborted\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "aborted\n",
      "Appending...  FEAT_num_strokes_task\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "FEAT_num_strokes_task\n",
      "Appending...  FEAT_num_strokes_beh\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "FEAT_num_strokes_beh\n",
      "Appending...  character\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "character\n",
      "Appending...  probe\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "probe\n",
      "Appending...  supervision_stage_concise\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "supervision_stage_concise\n",
      "Appending...  epoch_orig\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "epoch_orig\n",
      "Appending...  epoch\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "epoch\n",
      "Appending...  taskgroup\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskgroup\n",
      "Appending...  origin\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "origin\n",
      "Appending...  donepos\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "donepos\n",
      "Appending...  seqc_0_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_shape\n",
      "Appending...  seqc_0_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_loc\n",
      "Appending...  seqc_1_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_shape\n",
      "Appending...  seqc_1_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_loc\n",
      "Appending...  seqc_nstrokes_beh\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_nstrokes_beh\n",
      "Appending...  seqc_nstrokes_task\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_nstrokes_task\n",
      "Appending...  stroke_index_fromlast_tskstks\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "stroke_index_fromlast_tskstks\n",
      "Appending...  stroke_index_semantic_tskstks\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "stroke_index_semantic_tskstks\n",
      "Appending...  CTXT_loc_next\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "CTXT_loc_next\n",
      "Appending...  CTXT_shape_next\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "CTXT_shape_next\n",
      "Appending...  CTXT_loc_prev\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "CTXT_loc_prev\n",
      "Appending...  CTXT_shape_prev\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "CTXT_shape_prev\n",
      "Appending...  gap_from_prev_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "gap_from_prev_angle_binned\n",
      "Appending...  gap_to_next_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "gap_to_next_angle_binned\n",
      "Appending...  gap_from_prev_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "gap_from_prev_angle\n",
      "Appending...  gap_to_next_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "gap_to_next_angle\n",
      "Appending...  circ_signed\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "circ_signed\n",
      "Appending...  velocity\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "velocity\n",
      "Appending...  distcum\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "distcum\n",
      "Appending...  angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "angle\n",
      "Appending...  circ_signed_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "circ_signed_binned\n",
      "Appending...  velocity_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "velocity_binned\n",
      "Appending...  di_an_ci_ve_bin\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "di_an_ci_ve_bin\n",
      "Colected 1285 out of 1285 datapts.\n",
      "NOTE: missed datapts are likely because of removed outliers\n",
      "00_substrk M1 (0.0, 0.2)\n",
      "Sites for this bregion  M1\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62]\n",
      "00_substrk  --  M1  --  (0.0, 0.2)  -- (data shape:) (58, 1285, 20)\n",
      "00_substrk PMv (0.0, 0.2)\n",
      "Sites for this bregion  PMv\n",
      "[65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]\n",
      "00_substrk  --  PMv  --  (0.0, 0.2)  -- (data shape:) (63, 1285, 20)\n",
      "00_substrk PMd (0.0, 0.2)\n",
      "Sites for this bregion  PMd\n",
      "[129, 130, 131, 134, 135, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 157, 158, 159, 161, 165, 166, 168, 172, 177, 178, 182, 184, 186, 191]\n",
      "00_substrk  --  PMd  --  (0.0, 0.2)  -- (data shape:) (29, 1285, 20)\n",
      "00_substrk dlPFC (0.0, 0.2)\n",
      "Sites for this bregion  dlPFC\n",
      "[225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256]\n",
      "00_substrk  --  dlPFC  --  (0.0, 0.2)  -- (data shape:) (31, 1285, 20)\n",
      "00_substrk vlPFC (0.0, 0.2)\n",
      "Sites for this bregion  vlPFC\n",
      "[257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 292, 293, 294, 295, 296, 297, 298, 300, 302, 303, 304, 305, 307, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320]\n",
      "00_substrk  --  vlPFC  --  (0.0, 0.2)  -- (data shape:) (57, 1285, 20)\n",
      "00_substrk FP (0.0, 0.2)\n",
      "Sites for this bregion  FP\n",
      "[321, 324, 325, 326, 327, 328, 329, 330, 331, 334, 336, 337, 341, 342, 343, 344, 345, 347, 348, 349, 350, 351, 352, 353, 355, 356, 357, 358, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384]\n",
      "00_substrk  --  FP  --  (0.0, 0.2)  -- (data shape:) (52, 1285, 20)\n",
      "00_substrk SMA (0.0, 0.2)\n",
      "Sites for this bregion  SMA\n",
      "[385, 386, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 435, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448]\n",
      "00_substrk  --  SMA  --  (0.0, 0.2)  -- (data shape:) (60, 1285, 20)\n",
      "00_substrk preSMA (0.0, 0.2)\n",
      "Sites for this bregion  preSMA\n",
      "[449, 450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 462, 463, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 510, 511, 512]\n",
      "00_substrk  --  preSMA  --  (0.0, 0.2)  -- (data shape:) (59, 1285, 20)\n"
     ]
    }
   ],
   "source": [
    "# Load q_params\n",
    "from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "q_params = rsagood_questions_dict(animal, date, question)[question]\n",
    "# Load data\n",
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "\n",
    "combine_into_larger_areas = True\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\"\n",
    "HACK_RENAME_SHAPES = False\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                          which_level=which_level,\n",
    "                                        bin_by_time_dur = bin_by_time_dur, \n",
    "                                        bin_by_time_slide = bin_by_time_slide,\n",
    "                                          combine_into_larger_areas = combine_into_larger_areas,\n",
    "                                          exclude_bad_areas = exclude_bad_areas,\n",
    "                                          SPIKES_VERSION = SPIKES_VERSION,\n",
    "                                          HACK_RENAME_SHAPES = HACK_RENAME_SHAPES)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:33:00.531964276Z",
     "start_time": "2024-02-14T03:27:49.470567571Z"
    }
   },
   "id": "653547aa7482c726"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis approach 1 - separate dPCA for each PA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1f8d1b0ba0b1917"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_statespace_2d_overlaying_all_othervar\n",
    "\n",
    "keep_all_margs = True\n",
    "PLOT = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:34:50.242914741Z",
     "start_time": "2024-02-14T03:34:50.193659405Z"
    }
   },
   "id": "1d431ce30f60c726"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Get results for a single pa\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:34:50.531816659Z",
     "start_time": "2024-02-14T03:34:50.468125977Z"
    }
   },
   "id": "ada23ce42d219610"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if True:\n",
    "    # Overwrite effect var\n",
    "    effect_vars = [\"shape\"]\n",
    "    q_params[\"effect_vars\"] = effect_vars\n",
    "else:\n",
    "    effect_vars = q_params[\"effect_vars\"]\n",
    "        \n",
    "PLOT = False\n",
    "\n",
    "##### savedir\n",
    "from pythonlib.tools.listtools import stringify_list\n",
    "a = stringify_list(list_time_windows, return_as_str=True)\n",
    "b = stringify_list(events_keep, return_as_str=True)\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSES}/{animal}-{date}/{question}/{a}--{b}--{keep_all_margs}\"\n",
    "\n",
    "RES = []\n",
    "for i, row in DFallpa.iterrows():\n",
    "\n",
    "    pa = row[\"pa\"]\n",
    "    br = row[\"bregion\"]\n",
    "    wl = row[\"which_level\"]\n",
    "    ev = row[\"event\"]\n",
    "    tw = row[\"twind\"]\n",
    "\n",
    "\n",
    "    # Clean up PA\n",
    "    from neuralmonkey.analyses.rsa import preprocess_rsa_prepare_popanal_wrapper\n",
    "    pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "    \n",
    "    # Compute dPCs, and project data.\n",
    "    dpca, Z, R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca, panorm = dpca_compute_pa_to_space(pa, effect_vars, keep_all_margs=keep_all_margs)\n",
    "    plothelper_get_variables(Z, effect_vars, params_dpca) # Add variables to params\n",
    "        \n",
    "    # Znew = dpca.transform(R)\n",
    "    # for k in Znew.keys():\n",
    "    #     assert np.all(Z[k] == Znew[k]) \n",
    "\n",
    "    if PLOT:\n",
    "    \n",
    "        savedir = f\"{SAVEDIR}/each_pa/{wl}-{ev}-{br}-{tw}\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        print(\" *** Saving to:\", savedir)\n",
    "        \n",
    "        from pythonlib.tools.expttools import writeDictToTxt, writeDictToYaml\n",
    "        path = f\"{savedir}/res_check_effectvars.txt\"\n",
    "        writeDictToTxt(res_check_effectvars, path)\n",
    "        path = f\"{savedir}/res_check_tasksets.txt\"\n",
    "        writeDictToTxt(res_check_tasksets, path)\n",
    "        \n",
    "        params_dpca[\"data_shape-trial_N_features_time\"] = trialR.shape\n",
    "        writeDictToYaml(params_dpca, f\"{savedir}/params.yaml\")\n",
    "    \n",
    "        plot_all_results_single(dpca, Z, effect_vars, params_dpca, savedir)\n",
    "\n",
    "\n",
    "    RES.append({\n",
    "        \"dpca\":dpca,\n",
    "        \"bregion\":br,\n",
    "        \"which_level\":wl,\n",
    "        \"event\":ev,\n",
    "        \"twind\":tw,\n",
    "        \"explained_var\":dpca.explained_variance_ratio_,\n",
    "        \"marginalizations\":list(dpca.marginalizations.keys()),\n",
    "        \"params_dpca\":params_dpca,\n",
    "        \"map_var_to_lev\":map_var_to_lev,\n",
    "        \"map_grp_to_idx\":map_grp_to_idx,\n",
    "        # \"Z\":Z,\n",
    "        \"R\":R,\n",
    "        \"trialR\":trialR,        \n",
    "        \"times\":pa.Times,\n",
    "        \"panorm\":panorm\n",
    "    })\n",
    "\n",
    "assert False\n",
    "# Dont conver to df, too slow\n",
    "# DFRES = pd.DataFrame(RES)\n",
    "\n",
    "if PLOT:\n",
    "    from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_mult\n",
    "    plot_all_results_mult(DFRES, SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc46cc695323b096"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## IGNORE THIS\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_single\n",
    "\n",
    "savedir = f\"{SAVEDIR}/each_pa/{wl}-{ev}-{br}-{tw}\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "print(\" *** Saving to:\", savedir)\n",
    "\n",
    "from pythonlib.tools.expttools import writeDictToTxt, writeDictToYaml\n",
    "path = f\"{savedir}/res_check_effectvars.txt\"\n",
    "writeDictToTxt(res_check_effectvars, path)\n",
    "path = f\"{savedir}/res_check_tasksets.txt\"\n",
    "writeDictToTxt(res_check_tasksets, path)\n",
    "\n",
    "params_dpca[\"data_shape-trial_N_features_time\"] = trialR.shape\n",
    "writeDictToYaml(params_dpca, f\"{savedir}/params.yaml\")\n",
    "\n",
    "plot_all_results_single(dpca, Z, effect_vars, params_dpca, savedir)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "430c24a43e667e31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis Approach: Substrokes, in speciifc space, overlay "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d76e42acd3eaa3d3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##### Single trial analysis\n",
    "from neuralmonkey.scripts.analy_dpca_script_substrokes import transform_from_pa, transform_trial\n",
    "\n",
    "def pa_norm_preprocess_and_project(DFallpa, br, ev, effect_vars, keep_all_margs, prune_to_first_substroke=True): \n",
    "    \"\"\" Many steps - extract pa, prorocess (eg norm), consteruct dpCA space given this effect_vars (list),\n",
    "    and agg in time, and then project to the space. \n",
    "    \"\"\"\n",
    "    from pythonlib.tools.pandastools import append_col_with_grp_index \n",
    "    from neuralmonkey.analyses.rsa import preprocess_rsa_prepare_popanal_wrapper\n",
    "    from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "    \n",
    "    tmp = DFallpa[(DFallpa[\"bregion\"]==br) & (DFallpa[\"event\"]==ev)]\n",
    "    assert len(tmp)==1\n",
    "    pa = tmp[\"pa\"].values[0]\n",
    "    \n",
    "    pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"shape\", \"index_within_stroke\"], \"shape_idx\", False)\n",
    "    pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"distcum_binned\", \"angle_binned\", \"circularity_binned\"], \"di_an_ci_binned\", False)\n",
    "    \n",
    "    # Clean up PA\n",
    "    pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "    \n",
    "    # Restrict analysis to just first substroke\n",
    "    if prune_to_first_substroke:\n",
    "        pa = pa.slice_by_labels(\"trials\", \"index_within_stroke\", [0])\n",
    "        \n",
    "    # Compute dPCs, and project data.\n",
    "    dpca, Z, R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca, panorm = dpca_compute_pa_to_space(pa, effect_vars, keep_all_margs=keep_all_margs)\n",
    "    plothelper_get_variables(Z, effect_vars, params_dpca) # Add variables to params\n",
    "    \n",
    "    # Mean over time\n",
    "    panorm_scal = panorm.agg_wrapper(\"times\")\n",
    "    trialX = panorm_scal.X\n",
    "\n",
    "    # project to spaces\n",
    "    if effect_vars == [\"shape\"]:\n",
    "        marginalization=\"s\"\n",
    "    elif effect_vars == [\"dist_angle\"]:\n",
    "        marginalization=\"m\"\n",
    "    else:\n",
    "        assert False\n",
    "    trialX_proj = transform_from_pa(dpca, panorm_scal, marginalization)\n",
    "    \n",
    "    return trialX, trialX_proj, panorm_scal, params_dpca\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T03:34:55.558838520Z",
     "start_time": "2024-02-14T03:34:55.501284026Z"
    }
   },
   "id": "9d757a1a053711fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### [TEMP] testing out - plotting with continuous variables for motor "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfdc8b3cde76065b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from pythonlib.tools.plottools import savefig\n",
    "# from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "# import os\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# from neuralmonkey.scripts.analy_dpca_script_quick import plot_statespace_2d_overlaying_all_othervar\n",
    "# \n",
    "# # Get results for a single pa\n",
    "# from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "# from neuralmonkey.analyses.state_space_good import trajgood_plot_colorby_splotby_scalar, trajgood_plot_colorby_splotby_scalar_helper\n",
    "# \n",
    "# from pythonlib.tools.plottools import savefig\n",
    "# from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "# from neuralmonkey.analyses.rsa import preprocess_rsa_prepare_popanal_wrapper\n",
    "# \n",
    "# from pythonlib.tools.listtools import stringify_list\n",
    "# \n",
    "# # Load q_params\n",
    "# from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "# # Load data\n",
    "# from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "# \n",
    "# SAVEDIR_ANALYSES = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/dPCA\"\n",
    "# keep_all_margs = False\n",
    "# \n",
    "# ############### PARAMS\n",
    "# # animal = \"Diego\"\n",
    "# # dates = [230616, 230618, 230619]\n",
    "# # animal = \"Pancho\"\n",
    "# # dates = [220716, 220715, 220717, 220718, 220719]\n",
    "# \n",
    "# list_failures =[]\n",
    "# list_errs = []\n",
    "# animal = \"Diego\"\n",
    "# date = 230616\n",
    "# exclude_bad_areas = True\n",
    "# SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "# bin_by_time_dur = 0.05\n",
    "# bin_by_time_slide = 0.025\n",
    "# which_level = \"substroke\"\n",
    "# \n",
    "# # METHOD 1 - Standard, running separately for each PA\n",
    "# question = \"SS_shape\"\n",
    "# slice_agg_slices = None\n",
    "# slice_agg_vars_to_split = None\n",
    "# slice_agg_concat_dim = None\n",
    "# \n",
    "# list_time_windows = [(-0.1, 0.1)]\n",
    "# events_keep = [\"00_substrk\"]\n",
    "# # list_time_windows = [(-0.3, 0.)]\n",
    "# # events_keep = [\"06_on_strokeidx_0\"]\n",
    "# print(list_time_windows)\n",
    "#         \n",
    "# q_params = rsagood_questions_dict(animal, date, question)[question]\n",
    "# \n",
    "# DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "#                                           which_level=which_level,\n",
    "#                                         bin_by_time_dur = bin_by_time_dur, \n",
    "#                                         bin_by_time_slide = bin_by_time_slide)\n",
    "# list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "# ev = \"00_substrk\"\n",
    "# br = \"PMv\"\n",
    "# SAVEDIR = \"/tmp\"\n",
    "# \n",
    "# tmp = DFallpa[(DFallpa[\"bregion\"]==br) & (DFallpa[\"event\"]==ev)]\n",
    "# assert len(tmp)==1\n",
    "# pa = tmp[\"pa\"].values[0]\n",
    "# \n",
    "# pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"shape\", \"index_within_stroke\"], \"shape_idx\", False)\n",
    "# \n",
    "# # Clean up PA\n",
    "# pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "# \n",
    "# # Restrict analysis to just first substroke\n",
    "# if True:\n",
    "#     pa = pa.slice_by_labels(\"trials\", \"index_within_stroke\", [0])\n",
    "# \n",
    "# for effect_vars in [\n",
    "#     [\"shape\"],\n",
    "#     [\"dist_angle\"]\n",
    "# ]:\n",
    "#     \n",
    "#     if effect_vars == [\"shape\"]:\n",
    "#         marginalization=\"s\"\n",
    "#     elif effect_vars == [\"dist_angle\"]:\n",
    "#         marginalization=\"m\"\n",
    "#     else:\n",
    "#         assert False\n",
    "# \n",
    "#     savedir = f\"{SAVEDIR}/scatter_color_diff_ways/effect_vars-{'|'.join(effect_vars)}\"\n",
    "#     os.makedirs(savedir, exist_ok=True)\n",
    "# \n",
    "#     # Compute dPCs, and project data.\n",
    "#     dpca, Z, R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca, panorm = dpca_compute_pa_to_space(pa, effect_vars, keep_all_margs=keep_all_margs)\n",
    "#     plothelper_get_variables(Z, effect_vars, params_dpca) # Add variables to params\n",
    "#     \n",
    "#     # First, convert to final data using PA (e.g., scalar)\n",
    "#     pathis = panorm.agg_wrapper(\"times\")\n",
    "#     \n",
    "#     dflab = pathis.Xlabels[\"trials\"]\n",
    "#     \n",
    "#     trialX_proj = transform_from_pa(dpca, pathis, marginalization)\n",
    "#     dim1 = 0\n",
    "#     dim2 = 1\n",
    "#     xs = trialX_proj[:, dim1]\n",
    "#     ys = trialX_proj[:, dim2]\n",
    "# \n",
    "#     for color_var in [\"angle\", \"distcum\", \"circ_signed\", \"velocity\"]:\n",
    "#         for subplot_var in [\"shape\"]:\n",
    "#             # color_var = \"seqc_0_shape\"\n",
    "#             # subplot_var = \"seqc_0_loc\"\n",
    "#             # color_var = \"shape\"\n",
    "#             # subplot_var = \"bregion\"\n",
    "#             \n",
    "#             # color_var = \"di_an_ci_binned\"\n",
    "#             # subplot_var = \"bregion\"\n",
    "#             \n",
    "#             labels = dflab[color_var]\n",
    "#             \n",
    "#             \n",
    "#             fig, axes = trajgood_plot_colorby_splotby_scalar_helper(xs, ys, labels, dflab[subplot_var],\n",
    "#                                                         color_var, subplot_var, overlay_mean=False, SIZE=7)\n",
    "#             savefig(fig, f\"{savedir}/{br}-color_{color_var}-splot_{subplot_var}.pdf\")\n",
    "#             \n",
    "#             plt.close(\"all\")\n",
    "#                     "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8107a5a352c2414f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Good code below"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e643710219cc014c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_statespace_2d_overlaying_all_othervar\n",
    "\n",
    "# Get results for a single pa\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "from neuralmonkey.analyses.state_space_good import trajgood_plot_colorby_splotby_scalar, trajgood_plot_colorby_splotby_scalar_helper\n",
    "\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "from neuralmonkey.analyses.rsa import preprocess_rsa_prepare_popanal_wrapper\n",
    "\n",
    "from pythonlib.tools.listtools import stringify_list\n",
    "\n",
    "# Load q_params\n",
    "from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "# Load data\n",
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "\n",
    "SAVEDIR_ANALYSES = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/dPCA\"\n",
    "keep_all_margs = False\n",
    "\n",
    "############### PARAMS\n",
    "list_failures =[]\n",
    "list_errs = []\n",
    "\n",
    "for animal in [\"Pancho\", \"Diego\"]:\n",
    "    if animal==\"Diego\":\n",
    "        dates = [230615, 230616, 230618, 230619]\n",
    "    elif animal==\"Pancho\":\n",
    "        dates = [220716, 220715, 220718, 220719, 220918, 221217]\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    for date in dates:\n",
    "        try:\n",
    "            exclude_bad_areas = True\n",
    "            SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "            bin_by_time_dur = 0.05\n",
    "            bin_by_time_slide = 0.025\n",
    "            which_level = \"substroke\"\n",
    "            \n",
    "            if False:\n",
    "                # METHOD 2 - Merging across time windows into single PA bofre doing dPCA\n",
    "                question = \"SP_shape_loc_TIME\"\n",
    "                slice_agg_slices = [\n",
    "                    (\"trial\", \"03_samp\", (-0.3, 0.5)),\n",
    "                    (\"trial\", \"04_go_cue\", (-0.45, 0.25)),\n",
    "                    (\"trial\", \"06_on_strokeidx_0\", (-0.25, 0.7))\n",
    "                ]\n",
    "                slice_agg_vars_to_split = [\"bregion\"]\n",
    "                slice_agg_concat_dim = \"times\"\n",
    "                \n",
    "                list_time_windows = [sl[2] for sl in slice_agg_slices]\n",
    "                events_keep = list(set([sl[1] for sl in slice_agg_slices]))\n",
    "                print(list_time_windows)\n",
    "            else:\n",
    "                # METHOD 1 - Standard, running separately for each PA\n",
    "                question = \"SS_shape\"\n",
    "                slice_agg_slices = None\n",
    "                slice_agg_vars_to_split = None\n",
    "                slice_agg_concat_dim = None\n",
    "                \n",
    "                list_time_windows = [(-0.3, 0.)]\n",
    "                events_keep = [\"00_substrk\"]\n",
    "                # list_time_windows = [(-0.3, 0.)]\n",
    "                # events_keep = [\"06_on_strokeidx_0\"]\n",
    "                print(list_time_windows)\n",
    "            \n",
    "            for list_time_windows in [\n",
    "                [(-0.2, 0.)],\n",
    "                [(0., 0.2)]\n",
    "                ]:\n",
    "                q_params = rsagood_questions_dict(animal, date, question)[question]\n",
    "                \n",
    "                DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                                          which_level=which_level,\n",
    "                                                        bin_by_time_dur = bin_by_time_dur, \n",
    "                                                        bin_by_time_slide = bin_by_time_slide)\n",
    "                \n",
    "                \n",
    "                \n",
    "                a = stringify_list(list_time_windows, return_as_str=True)\n",
    "                b = stringify_list(events_keep, return_as_str=True)\n",
    "                \n",
    "                SAVEDIR = f\"{SAVEDIR_ANALYSES}/{animal}-{date}/{question}/{a}--{b}--{keep_all_margs}\"\n",
    "                \n",
    "                list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "                # effect_vars = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "                # br = \"PMv\"\n",
    "                # ev = \"06_on_strokeidx_0\"\n",
    "                # br = \"PMv\"\n",
    "                ev = \"00_substrk\"\n",
    "                \n",
    "                for br in list_br:                    \n",
    "                    for effect_vars in [\n",
    "                        [\"shape\"],\n",
    "                        [\"dist_angle\"]\n",
    "                    ]:\n",
    "                        \n",
    "                        trialX, trialX_proj, panorm_scal, params_dpca = pa_norm_preprocess_and_project(DFallpa, \n",
    "                                                                                                       br, ev, effect_vars,\n",
    "                                                                                                       keep_all_margs,\n",
    "                                                                                                       prune_to_first_substroke=True) \n",
    "                        # plothelper_get_variables(Z, effect_vars, params_dpca) # Add variables to params\n",
    "                       \n",
    "                        # # tmp = DFallpa[(DFallpa[\"bregion\"]==br) & (DFallpa[\"event\"]==ev)]\n",
    "                        # # assert len(tmp)==1\n",
    "                        # # pa = tmp[\"pa\"].values[0]\n",
    "                        # # \n",
    "                        # # pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"shape\", \"index_within_stroke\"], \"shape_idx\", False)\n",
    "                        # # pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"distcum_binned\", \"angle_binned\", \"circularity_binned\"], \"di_an_ci_binned\", False)\n",
    "                        # # \n",
    "                        # # # Clean up PA\n",
    "                        # # pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "                        # # \n",
    "                        # # Restrict analysis to just first substroke\n",
    "                        # if True:\n",
    "                        #     pa = pa.slice_by_labels(\"trials\", \"index_within_stroke\", [0])\n",
    "                        # \n",
    "                        # if effect_vars == [\"shape\"]:\n",
    "                        #     marginalization=\"s\"\n",
    "                        # elif effect_vars == [\"dist_angle\"]:\n",
    "                        #     marginalization=\"m\"\n",
    "                        # else:\n",
    "                        #     assert False\n",
    "                        # \n",
    "                        # # Compute dPCs, and project data.\n",
    "                        # dpca, Z, R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca, panorm = dpca_compute_pa_to_space(pa, effect_vars, keep_all_margs=keep_all_margs)\n",
    "                        # plothelper_get_variables(Z, effect_vars, params_dpca) # Add variables to params\n",
    "                        # \n",
    "                        # trialX_proj = transform_from_pa(dpca, panorm_scal, marginalization)\n",
    "                        \n",
    "                        \n",
    "                        ##### Single trial analysis\n",
    "    \n",
    "                        # First, convert to final data using PA (e.g., scalar)\n",
    "                        # pathis = panorm.agg_wrapper(\"times\")\n",
    "                        # \n",
    "                        \n",
    "                        dflab = panorm_scal.Xlabels[\"trials\"]\n",
    "                        dim1 = 0\n",
    "                        dim2 = 1\n",
    "                        xs = trialX_proj[dim1, :]\n",
    "                        ys = trialX_proj[dim2, :]\n",
    "                    \n",
    "                        savedir = f\"{SAVEDIR}/scatter_color_diff_ways/effect_vars-{'|'.join(effect_vars)}\"\n",
    "                        os.makedirs(savedir, exist_ok=True)\n",
    "                        for color_var in [\"angle\", \"distcum\", \"circ_signed\", \"velocity\", \"shape\", \"di_an_ci_ve_bin\"]:\n",
    "                            for subplot_var in [\"bregion\", \"shape\", \"di_an_ci_ve_bin\"]:\n",
    "                                # color_var = \"seqc_0_shape\"\n",
    "                                # subplot_var = \"seqc_0_loc\"\n",
    "                                # color_var = \"shape\"\n",
    "                                # subplot_var = \"bregion\"\n",
    "                                \n",
    "                                # color_var = \"di_an_ci_binned\"\n",
    "                                # subplot_var = \"bregion\"\n",
    "                                \n",
    "                                labels = dflab[color_var]\n",
    "                                \n",
    "                                fig, axes = trajgood_plot_colorby_splotby_scalar_helper(xs, ys, labels, dflab[subplot_var],\n",
    "                                                                            color_var, subplot_var, overlay_mean=False, SIZE=6)\n",
    "                                savefig(fig, f\"{savedir}/{br}-color_{color_var}-splot_{subplot_var}.pdf\")\n",
    "                                \n",
    "                                plt.close(\"all\")\n",
    "        except Exception as err:\n",
    "            list_failures.append((animal, date))\n",
    "            list_errs.append(err)\n",
    "            pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c091f119ba207f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# APproach 2: Compare two datasets PA train vs. test (each across all areas)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d5b84e60c76582"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "import sys\n",
    "\n",
    "SAVEDIR_ANALYSES = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/dPCA\"\n",
    "\n",
    "############### PARAMS\n",
    "animal = \"Diego\"\n",
    "date = 230615\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "bin_by_time_dur = 0.05\n",
    "bin_by_time_slide = 0.025\n",
    "\n",
    "if True:\n",
    "    # Train/test using different time windows. The PA related stuff is same as standard approach.\n",
    "    question = \"SP_shape_loc\"\n",
    "    slice_agg_slices = None\n",
    "    slice_agg_vars_to_split = None\n",
    "    slice_agg_concat_dim = None\n",
    "    \n",
    "    # list_time_windows = [(0.2, 0.6), (-0.3, 0.3)]\n",
    "    list_time_windows = [(0.1, 0.3), (-0.3, 0.3)] # Immediate visual response\n",
    "    events_keep = [\"03_samp\", \"06_on_strokeidx_0\"]\n",
    "    print(list_time_windows)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac314ddd08abdb52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                bin_by_time_dur = bin_by_time_dur, \n",
    "                                bin_by_time_slide = bin_by_time_slide)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ee07806bedea051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "effect_vars = q_params[\"effect_vars\"]\n",
    "print(\"Effect vars:\", effect_vars)\n",
    "keep_all_margs = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dc3854756b47daa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_single\n",
    "list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "\n",
    "for keep_all_margs in [True, False]:\n",
    "    wl = \"trial\"\n",
    "    ev_train = events_keep[0]\n",
    "    tw_train = list_time_windows[0]\n",
    "    \n",
    "    ev_test = events_keep[1]\n",
    "    tw_test = list_time_windows[1]\n",
    "    \n",
    "    from pythonlib.tools.expttools import writeDictToTxt\n",
    "    SAVEDIR = f\"{SAVEDIR_ANALYSES}/{animal}-{date}/{question}/train_test_diff_evtw/TRAIN-{ev_train}-{tw_train}-TEST-{ev_test}-{tw_test}-keepallmarg_{keep_all_margs}\"\n",
    "    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "    params = {}\n",
    "    params[\"wl\"] = wl\n",
    "    params[\"ev_train\"] = ev_train\n",
    "    params[\"tw_train\"] = tw_train\n",
    "    params[\"ev_test\"] = ev_test\n",
    "    params[\"tw_test\"] = tw_test\n",
    "    writeDictToTxt(params, f\"{SAVEDIR}/params.txt\")\n",
    "    \n",
    "    ## Fit PCs and evaluate test data.\n",
    "    RES_TRAIN = []\n",
    "    RES_TEST = []\n",
    "    for br in list_br:\n",
    "        a = DFallpa[\"event\"] == ev_train\n",
    "        b = DFallpa[\"twind\"] == tw_train\n",
    "        c = DFallpa[\"bregion\"] == br\n",
    "        \n",
    "        tmp =DFallpa[(a & b & c)]\n",
    "        assert len(tmp)==1\n",
    "        pa_train = tmp[\"pa\"].values[0]\n",
    "    \n",
    "        a = DFallpa[\"event\"] == ev_test\n",
    "        b = DFallpa[\"twind\"] == tw_test\n",
    "        tmp =DFallpa[(a & b & c)]\n",
    "        assert len(tmp)==1\n",
    "        pa_test = tmp[\"pa\"].values[0]\n",
    "    \n",
    "        # Train\n",
    "        from dPCA import dPCA\n",
    "        R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca, PAnorm = preprocess_pa_to_frtensor(pa_train, effect_vars, keep_all_margs=keep_all_margs)\n",
    "        \n",
    "        labels = params_dpca[\"labels\"]\n",
    "        join = params_dpca[\"join\"]\n",
    "        n_components = params_dpca[\"n_components\"]\n",
    "        dpca = dPCA.dPCA(labels=labels, regularizer='auto', join=join, n_components=n_components)\n",
    "        dpca.protect = ['t']\n",
    "        \n",
    "        # Now fit the data (R) using the model we just instatiated. Note that we only need trial-to-trial data when we want to optimize over the regularization parameter.\n",
    "        Z = dpca.fit_transform(R,trialR)\n",
    "            \n",
    "        ############ PLOT TRAINING DATA\n",
    "        savedir = f\"{SAVEDIR}/each_bregion/{br}/figures_summary_TRAIN\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        # Test, by projecting new data onto this space\n",
    "        R, trialR, _, _, params_dpca, PAnorm = preprocess_pa_to_frtensor(pa_train, effect_vars, keep_all_margs=keep_all_margs)\n",
    "        Ztrain = dpca.transform(R)\n",
    "        plot_all_results_single(dpca, Ztrain, effect_vars, params_dpca, savedir)    \n",
    "        plt.close(\"all\")\n",
    "    \n",
    "        RES_TRAIN.append({\n",
    "            \"bregion\":br,\n",
    "            \"which_level\":wl,\n",
    "            \"event\":ev_train,\n",
    "            \"twind\":tw_train,\n",
    "            \"explained_var\":dpca.explained_variance_ratio_,\n",
    "            \"marginalizations\":list(dpca.marginalizations.keys()),\n",
    "            \"params_dpca\":params_dpca,\n",
    "            \"map_var_to_lev\":map_var_to_lev,\n",
    "            \"map_grp_to_idx\":map_grp_to_idx,\n",
    "            \"Z\":Ztrain\n",
    "        })\n",
    "    \n",
    "        ############ PLOT TESTING DATA\n",
    "        savedir = f\"{SAVEDIR}/each_bregion/{br}/figures_summary_TEST\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        # Test, by projecting new data onto this space\n",
    "        R, trialR, _, _, params_dpca, PAnorm = preprocess_pa_to_frtensor(pa_test, effect_vars, keep_all_margs=keep_all_margs)\n",
    "        Ztest = dpca.transform(R)\n",
    "        plot_all_results_single(dpca, Ztest, effect_vars, params_dpca, savedir)    \n",
    "        plt.close(\"all\")\n",
    "                \n",
    "        RES_TEST.append({\n",
    "            \"bregion\":br,\n",
    "            \"which_level\":wl,\n",
    "            \"event\":ev_test,\n",
    "            \"twind\":tw_test,\n",
    "            \"explained_var\":dpca.explained_variance_ratio_,\n",
    "            \"marginalizations\":list(dpca.marginalizations.keys()),\n",
    "            \"params_dpca\":params_dpca,\n",
    "            \"map_var_to_lev\":map_var_to_lev,\n",
    "            \"map_grp_to_idx\":map_grp_to_idx,\n",
    "            \"Z\":Ztest\n",
    "        })\n",
    "        \n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        #### PLOT SUMMARY for test and train.\n",
    "        from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_mult\n",
    "        \n",
    "        DFRES_TRAIN = pd.DataFrame(RES_TRAIN)\n",
    "        savedir = f\"{SAVEDIR}/train\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        plot_all_results_mult(DFRES_TRAIN, savedir)\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        DFRES_TEST = pd.DataFrame(RES_TEST)\n",
    "        savedir = f\"{SAVEDIR}/test\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        plot_all_results_mult(DFRES_TEST, savedir)\n",
    "        plt.close(\"all\")\n",
    "                \n",
    "                \n",
    "    ##### SUMMARY: Compare similarity of activity of test data, projected onto old subspace\n",
    "    # PARAMS\n",
    "    # - Time bins, indices into z[:,:, :, idx]\n",
    "    tbin_train = -1\n",
    "    tbin_test = 0\n",
    "    version_distance = \"_pearson_raw\"\n",
    "    PLOT_EACH_HEATMAP=False\n",
    "    \n",
    "    # RUN\n",
    "    savedir = f\"{SAVEDIR}/RSA\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "    \n",
    "    ts, margs, map_var_to_idx, map_margstr_to_var, map_var_lev_to_pcol, map_var_to_lev = plothelper_get_variables(Z, effect_vars, params_dpca)\n",
    "    \n",
    "    # RSA analysis.\n",
    "    map_grp_to_idx = params_dpca[\"map_grp_to_idx\"]\n",
    "    map_var_to_idx = params_dpca[\"map_var_to_idx\"]\n",
    "    margs = params_dpca[\"margs\"]\n",
    "    map_traintest_to_Z = {\n",
    "        \"train\":Ztrain,\n",
    "        \"test\":Ztest\n",
    "    }\n",
    "    SCORES = []\n",
    "    for marg in margs:\n",
    "        map_bregion_Ztrain = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TRAIN.iterrows()}\n",
    "        map_bregion_Ztest = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TEST.iterrows()} \n",
    "        \n",
    "        list_br = list(map_bregion_Ztrain.keys())\n",
    "        for br in list_br:    \n",
    "            ######## Collect z across all variables, and train/test.\n",
    "            tmp = []\n",
    "            for train_test in [\"train\", \"test\"]:\n",
    "                if train_test==\"train\":\n",
    "                    tbin = tbin_train\n",
    "                    Zthis = map_bregion_Ztrain[br]\n",
    "                elif train_test==\"test\":\n",
    "                    tbin = tbin_test                \n",
    "                    Zthis = map_bregion_Ztest[br]\n",
    "                else:\n",
    "                    assert False\n",
    "                    \n",
    "                # Collect data across effect vars\n",
    "                for grp, idx in map_grp_to_idx.items():\n",
    "                    dat = {}\n",
    "                    dat[\"bregion\"] = br\n",
    "                    for var, i in map_var_to_idx.items():\n",
    "                        dat[var] = grp[i]    \n",
    "                    dat[\"train_test\"] = train_test\n",
    "                    dat[\"z\"] = Zthis[:, idx[0], idx[1], tbin]\n",
    "                    tmp.append(dat)        \n",
    "            dftmp = pd.DataFrame(tmp)\n",
    "        \n",
    "        \n",
    "            ########### SCORE AND PLOT\n",
    "            # Convert to Cluster\n",
    "            from pythonlib.cluster.clustclass import Clusters\n",
    "            X = np.stack(dftmp[\"z\"]) # (ndat, ndims)\n",
    "            label_vars = effect_vars + [\"train_test\"]\n",
    "            labels = dftmp.loc[:, label_vars].values.tolist()\n",
    "            labels = [tuple(l) for l in labels]\n",
    "            labels_cols = list(range(X.shape[1]))\n",
    "            Clraw = Clusters(X, labels, labels_cols, \"rsa\", {\"label_vars\":label_vars})\n",
    "            Clsim = Clraw.distsimmat_convert(version_distance)\n",
    "                \n",
    "            if PLOT_EACH_HEATMAP:\n",
    "                # plot \n",
    "                fig, ax = Clraw.rsa_plot_heatmap(sort_order=(0,1,2), diverge=True)\n",
    "                savefig(fig, f\"{savedir}/rawheat-marg_{marg}-{br}.pdf\")\n",
    "                \n",
    "                fig, ax = Clsim.rsa_plot_heatmap(sort_order=(0,1,2))\n",
    "                savefig(fig, f\"{savedir}/rawdiff_pearson-marg_{marg}-{br}.pdf\")\n",
    "            \n",
    "            # compute score\n",
    "            res = []\n",
    "            for var_effect in effect_vars:\n",
    "                vars_all = label_vars\n",
    "                var_other = [v for v in effect_vars if not v==var_effect][0]\n",
    "                vars_test_invariance_over_dict = {\n",
    "                    \"same\":[var_other],\n",
    "                    \"diff\":[\"train_test\"]\n",
    "                }\n",
    "                out = Clsim.rsa_distmat_score_same_diff(var_effect, vars_all, vars_test_invariance_over_dict, PLOT=False)\n",
    "                \n",
    "                SCORES.append({\n",
    "                    \"EffS_CtxS\":out[\"EffS_CtxS\"],\n",
    "                    \"EffD_CtxS\":out[\"EffD_CtxS\"],\n",
    "                    \"EffS_CtxD\":out[\"EffS_CtxD\"],\n",
    "                    \"EffD_CtxD\":out[\"EffD_CtxD\"],\n",
    "                    \"bregion\":br,\n",
    "                    \"var_effect\":var_effect,                \n",
    "                    \"var_other\":var_other,\n",
    "                    \"vars_all\":vars_all,\n",
    "                    \"marginalization\":marg,\n",
    "                })\n",
    "                \n",
    "    ######### PLOT\n",
    "    dfscores = pd.DataFrame(SCORES)\n",
    "    \n",
    "    list_marg = dfscores[\"marginalization\"].unique()\n",
    "    import seaborn as sns\n",
    "    \n",
    "    ####\n",
    "    for yvar in [\n",
    "        \"EffS_CtxS\",\n",
    "        \"EffD_CtxS\",\n",
    "        \"EffS_CtxD\",\n",
    "        \"EffD_CtxD\"]:\n",
    "        \n",
    "        ncols =2\n",
    "        nrows = int(np.ceil(len(list_marg)/ncols))\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(12,nrows*5), sharey=True)\n",
    "        \n",
    "        for marg, ax in zip(list_marg, axes.flatten()):\n",
    "            dfthis = dfscores[dfscores[\"marginalization\"]==marg]\n",
    "            sns.barplot(data=dfthis, x=\"bregion\", y=yvar, ax=ax, hue=\"var_effect\")\n",
    "            ax.set_title(marg)\n",
    "        \n",
    "        print(\"Plotting for:\", yvar)\n",
    "        savefig(fig, f\"{savedir}/corr-test_vs_train-projected_on_train_space-{version_distance}-yvar_{yvar}.pdf\")  \n",
    "        plt.close(\"all\")\n",
    "    \n",
    "    #### Plot summary of scores.   \n",
    "    dfscores[\"score_CtxD\"] = dfscores[\"EffS_CtxD\"] - dfscores[\"EffD_CtxD\"]\n",
    "    fig = sns.catplot(data=dfscores, x=\"bregion\", y=\"score_CtxD\", hue=\"var_effect\", col=\"marginalization\", col_wrap=2, kind=\"bar\")\n",
    "    savefig(fig, f\"{savedir}/score_EffS_CtxD_minus_EffD_CtxD.pdf\")\n",
    "    \n",
    "    # --\n",
    "    from pythonlib.tools.pandastools import unpivot\n",
    "    value_vars = [\n",
    "                \"EffS_CtxS\",\n",
    "                \"EffD_CtxS\",\n",
    "                \"EffS_CtxD\",\n",
    "                \"EffD_CtxD\"]\n",
    "    \n",
    "    dfscores_long = unpivot(dfscores, [\"bregion\", \"var_effect\", \"var_other\", \"marginalization\", \"vars_all\"], value_vars, \"score_kind\", \"score\")\n",
    "    dfscores_long\n",
    "    score_kinds_keep = [\"EffD_CtxS\", \"EffS_CtxD\", \"EffD_CtxD\"]\n",
    "    dfscores_long = dfscores_long[dfscores_long[\"score_kind\"].isin(score_kinds_keep)].reset_index(drop=True)\n",
    "    fig = sns.catplot(data=dfscores_long, x=\"bregion\", y=\"score\", hue=\"score_kind\", col=\"var_effect\", row=\"marginalization\", kind=\"bar\")\n",
    "    savefig(fig, f\"{savedir}/all_effects.pdf\")\n",
    "    plt.close(\"all\")\n",
    "    \n",
    "    \n",
    "##### A single plot overlaying train and test, where one time bin for train is concatted to one for test.\n",
    "# [OLDER, beducase this is difficult to compare represntational structure of train vs. for test.\n",
    "\n",
    "# Construct a new fake Z, which makes Ztrain and Ztest be the two \"timepoints\"\n",
    "def _concat_ztrain_ztest(ztrain, ztest, tbin_train, tbin_test):\n",
    "    \"\"\" Given two matrics of (a,b,c,n) and (a,b,c,m), takes one \n",
    "    time slice each, to output (a,b,c,2)\n",
    "    \"\"\"\n",
    "    # tbin_train = -1\n",
    "    # tbin_test = 0\n",
    "    zfake = np.stack((ztrain[:,:,:,tbin_train], ztest[:,:,:,tbin_test]), axis=3)\n",
    "    return zfake\n",
    "\n",
    "savedir = f\"{SAVEDIR}/state_space_combined\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "# Note which time bins\n",
    "from pythonlib.tools.expttools import writeDictToTxt\n",
    "prms = {\n",
    "    \"tbin_train\":tbin_train,\n",
    "    \"tbin_test\":tbin_test,\n",
    "}\n",
    "writeDictToTxt(prms, f\"{savedir}/prms_tbins.txt\")\n",
    "\n",
    "SAME_AXES_ACROSS_MARGS = False\n",
    "\n",
    "for marg in margs:\n",
    "    map_bregion_Ztrain = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TRAIN.iterrows()}\n",
    "    map_bregion_Ztest = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TEST.iterrows()}     \n",
    "    for br in list_br:\n",
    "        ztrain = map_bregion_Ztrain[br]\n",
    "        ztest = map_bregion_Ztest[br]\n",
    "        var = map_margstr_to_var[marg]\n",
    "        \n",
    "        # get combined Z\n",
    "        Z = _concat_ztrain_ztest(ztrain, ztest, tbin_train, tbin_test)\n",
    "        Z = {marg:Z} # HACK\n",
    "        \n",
    "        # indvar_to_color_by = 0\n",
    "        for var_color_by in effect_vars:\n",
    "            if SAME_AXES_ACROSS_MARGS:\n",
    "                val_minmax = [zmin, zmax]\n",
    "            else:\n",
    "                val_minmax = None\n",
    "            fig, axes = plot_statespace_2d_overlaying_all_othervar(Z, marg, var_color_by, params_dpca, val_minmax, return_axes=True)\n",
    "            # for ax in axes.flatten():\n",
    "            #     ax.set_title()\n",
    "\n",
    "            savefig(fig, f\"{savedir}/2D-marg_{marg}-br_{br}-color_by_{var_color_by}-share_axes_{SAME_AXES_ACROSS_MARGS}.pdf\")\n",
    "            plt.close(\"all\")\n",
    "    \n",
    "\n",
    "#### BETTER PLOTS, seaprate plots for train and test.\n",
    "# First, collect all trajectories into dataframe, split up where each row is a single condition, with z shape (ndim, ntrials, ntimes), where ntrials is 1\n",
    "\n",
    "res = []\n",
    "for train_test in [\"train\", \"test\"]:\n",
    "    if train_test==\"train\":\n",
    "        DFRES_THIS = DFRES_TRAIN\n",
    "    elif train_test==\"test\":\n",
    "        DFRES_THIS = DFRES_TEST\n",
    "    else:\n",
    "        assert False\n",
    "    for _, row in DFRES_THIS.iterrows():\n",
    "        for marg, Zthis in row[\"Z\"].items():\n",
    "            # Collect data across effect vars\n",
    "            for grp, idx in map_grp_to_idx.items():    \n",
    "                z = Zthis[:, idx[0], idx[1], :] # (ndim, ntime)\n",
    "                z = z.reshape(z.shape[0], 1, z.shape[1])\n",
    "                \n",
    "                # Collect data\n",
    "                dat = {}\n",
    "                dat[\"train_test\"] = train_test\n",
    "                dat[\"marg\"] = marg\n",
    "                for var, i in map_var_to_idx.items():\n",
    "                    dat[var] = grp[i]    \n",
    "                dat[\"bregion\"] = row[\"bregion\"]\n",
    "                dat[\"which_level\"] = row[\"which_level\"]\n",
    "                dat[\"event\"] = row[\"event\"]\n",
    "                dat[\"twind\"] = row[\"twind\"]\n",
    "                dat[\"explained_var\"] = row[\"explained_var\"]\n",
    "                dat[\"params_dpca\"] = row[\"params_dpca\"]\n",
    "                dat[\"times\"] = row[\"params_dpca\"][\"times\"]\n",
    "                dat[\"z\"] = z\n",
    "                res.append(dat)               \n",
    "    \n",
    "DFRES_TRAINTEST = pd.DataFrame(res)\n",
    "\n",
    "### GOOD PLOT separately train vs. test, each doing both scalar and traj,\n",
    "# plotted on same space.\n",
    "list_tbin_test = [0, -1], # each does separate set of plots...\n",
    "### PLot params\n",
    "times_to_mark = [0]\n",
    "times_to_mark_markers = [\"d\"]\n",
    "# text_plot_pt1 = \"on\"\n",
    "text_plot_pt1 = None\n",
    "markersize=7\n",
    "marker=\"P\"\n",
    "time_bin_size = 0.05\n",
    "\n",
    "for tbin_test in list_tbin_test:\n",
    "\n",
    "    # Taske a single scalar time bin, differently for train and test\n",
    "    list_z_scalar = []\n",
    "    for i, row in DFRES_TRAINTEST.iterrows():\n",
    "        z = row[\"z\"] # (ndims, ntrials, ntimes)\n",
    "        if row[\"train_test\"]==\"train\":\n",
    "            tbin = tbin_train\n",
    "        elif row[\"train_test\"]==\"test\":\n",
    "            tbin = tbin_test\n",
    "        else:\n",
    "            assert False\n",
    "        list_z_scalar.append(z[:,:,tbin][:,:,None]) # (ndims, ntrials, 1)\n",
    "        \n",
    "    DFRES_TRAINTEST[\"z_scalar\"] = list_z_scalar\n",
    "        \n",
    "    \n",
    "    # SCALAR PLOTS: train vs. test\n",
    "    var_subplots = \"train_test\"\n",
    "    var_color_by = \"seqc_0_shape\"\n",
    "    for traj_or_scalar in [\"traj\", \"scalar\"]:\n",
    "    # for traj_or_scalar in [\"scalar\"]:\n",
    "        for marg in margs:\n",
    "            for bregion in list_br:\n",
    "                    \n",
    "                if bregion==\"vlPFC\":\n",
    "                    # Pick out specific df\n",
    "                    b = DFRES_TRAINTEST[\"marg\"]==marg\n",
    "                    c = DFRES_TRAINTEST[\"bregion\"]==bregion\n",
    "                    dfthis = DFRES_TRAINTEST[b & c]\n",
    "                    \n",
    "                    \n",
    "                    from neuralmonkey.analyses.state_space_good import trajgood_plot_colorby_splotby\n",
    "                    fig, axes = trajgood_plot_colorby_splotby(dfthis, var_color_by, var_subplots, dims=(0,1),\n",
    "                                                      traj_or_scalar=traj_or_scalar, mean_over_trials=True,\n",
    "                                                      times_to_mark=times_to_mark,\n",
    "                                                       times_to_mark_markers=times_to_mark_markers,\n",
    "                                                       time_bin_size=time_bin_size,\n",
    "                                                       markersize=markersize, marker=marker,\n",
    "                                                       text_plot_pt1=text_plot_pt1,\n",
    "                                                       alpha=0.2)\n",
    "                    \n",
    "                    savefig(fig, f\"{savedir}/2D_train_vs_test-{traj_or_scalar}-marg_{marg}-{bregion}-var_{var_color_by}-ovar_{var_subplots}-tbin_test_{tbin_test}.pdf\")\n",
    "                    plt.close(\"all\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7ff7d80d6e8f2ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute significance, based on decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7b719342bdae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 1st mixing component looks merely like noise. But to be sure, we can run a significance analysis:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bd3ae945d6e45fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_masks = dpca.significance_analysis(R,  trialR, n_shuffles=10, n_splits=10, n_consecutive=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e8bb918ae5ee6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_masks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b4231af5b2375d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can highlight the significant parts of the demixed components with a black bar underneath. Note that there is no significant analysis time, since there are no classes to compute the significance over."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c541a15e079a99f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_masks[\"st\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14d6a0758de5ba31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Z[\"s\"].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74e0e06dac7e97d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time = arange(T)\n",
    "\n",
    "figure(figsize=(16,7))\n",
    "subplot(131)\n",
    "\n",
    "for s in range(S):\n",
    "    plot(time,Z['t'][0,s])\n",
    "\n",
    "title('1st time component')\n",
    "    \n",
    "subplot(132)\n",
    "\n",
    "for s in range(S):\n",
    "    plot(time,Z['s'][0,s])\n",
    "\n",
    "imshow(significance_masks['s'][0][None,:],extent=[0,250,amin(Z['s'])-1,amin(Z['s'])-0.5],aspect='auto',cmap='gray_r',vmin=0,vmax=1)\n",
    "ylim([amin(Z['s'])-1,amax(Z['s'])+1])\n",
    "\n",
    "title('1st stimulus component')\n",
    "    \n",
    "subplot(133)\n",
    "\n",
    "for s in range(S):\n",
    "    plot(time,Z['st'][0,s])\n",
    "\n",
    "dZ = amax(Z['st'])-amin(Z['st'])\n",
    "imshow(significance_masks['st'][0][None,:],extent=[0,250,amin(Z['st'])-dZ/10.,amin(Z['st'])-dZ/5.],aspect='auto',cmap='gray_r',vmin=0,vmax=1)\n",
    "ylim([amin(Z['st'])-dZ/10.,amax(Z['st'])+dZ/10.])\n",
    "    \n",
    "title('1st mixing component')\n",
    "show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc6d4754706d43a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
