{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-02T23:51:36.508694217Z",
     "start_time": "2024-02-02T23:51:36.476910477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' Building from\\n_snippets_rsa.ipynb.\\n\\n\\n\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" \n",
    "2/2/23 - All methods realted to dPCA here.\n",
    "Thought of consoldiating into functions, but best to leave here, and isntaed I tried to carefully doc below.\n",
    "\n",
    "Script: analy_dpca_script_quick.py. It is \n",
    "\n",
    "Building from\n",
    "_snippets_rsa.ipynb.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis approach 1 - separate dPCA for each PA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a2b8c238905cf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_dpca_script_quick import preprocess_pa_to_frtensor\n",
    "from neuralmonkey.classes.snippets import load_and_concat_mult_snippets\n",
    "from neuralmonkey.classes.session import load_mult_session_helper\n",
    "import os\n",
    "import pandas as pd\n",
    "from neuralmonkey.analyses.state_space_good import snippets_extract_popanals_split_bregion_twind\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3750e537f858d4f3"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.3), (-0.3, 0.3)]\n"
     ]
    }
   ],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "import sys\n",
    "\n",
    "SAVEDIR_ANALYSES = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/dPCA\"\n",
    "\n",
    "############### PARAMS\n",
    "animal = \"Diego\"\n",
    "date = 230615\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "bin_by_time_dur = 0.05\n",
    "bin_by_time_slide = 0.025\n",
    "\n",
    "if False:\n",
    "    # METHOD 2 - Merging across time windows into single PA bofre doing dPCA\n",
    "    question = \"SP_shape_loc_TIME\"\n",
    "    slice_agg_slices = [\n",
    "        (\"trial\", \"03_samp\", (-0.3, 0.5)),\n",
    "        (\"trial\", \"04_go_cue\", (-0.45, 0.25)),\n",
    "        (\"trial\", \"06_on_strokeidx_0\", (-0.25, 0.7))\n",
    "    ]\n",
    "    slice_agg_vars_to_split = [\"bregion\"]\n",
    "    slice_agg_concat_dim = \"times\"\n",
    "    \n",
    "    list_time_windows = [sl[2] for sl in slice_agg_slices]\n",
    "    events_keep = list(set([sl[1] for sl in slice_agg_slices]))\n",
    "    print(list_time_windows)\n",
    "else:\n",
    "    # METHOD 1 - Standard, running separately for each PA\n",
    "    question = \"SP_shape_loc\"\n",
    "    slice_agg_slices = None\n",
    "    slice_agg_vars_to_split = None\n",
    "    slice_agg_concat_dim = None\n",
    "    \n",
    "    list_time_windows = [(-0.3, 0.5)]\n",
    "    events_keep = [\"06_on_strokeidx_0\"]\n",
    "    print(list_time_windows)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T23:52:00.032164911Z"
    }
   },
   "id": "21ee6a1220959299"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230615*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514\n",
      "session:  0\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230615*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514\n",
      "Beh Sessions hand netered (mapping: rec sess --> beh sess):  [2, 3]\n",
      "Beh Sessions that exist on this date:  {230615: [(2, 'priminvar5'), (3, 'priminvar5')]}\n",
      "------------------------------\n",
      "Loading this neural session: 0\n",
      "Loading these beh expts: ['priminvar5']\n",
      "Loading these beh sessions: [2]\n",
      "Using this beh_trial_map_list: [(1, 0)]\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230615*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514\n",
      "{'filename_components_hyphened': ['Diego', '230615', '104852'], 'basedirs': ['/mnt/Freiwald/ltian/recordings/Diego', '/mnt/Freiwald/ltian/recordings/Diego/230615'], 'basedirs_filenames': ['230615', 'Diego-230615-104852'], 'filename_final_ext': 'Diego-230615-104852', 'filename_final_noext': 'Diego-230615-104852'}\n",
      "FOund this path for spikes:  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852/spikes_tdt_quick-4\n",
      "== PATHS for this expt: \n",
      "raws  --  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852\n",
      "tank  --  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852/Diego-230615-104852\n",
      "spikes  --  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852/spikes_tdt_quick-4\n",
      "final_dir_name  --  Diego-230615-104852\n",
      "time  --  104852\n",
      "pathbase_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852\n",
      "tank_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/data_tank.pkl\n",
      "spikes_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/data_spikes.pkl\n",
      "datall_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/data_datall.pkl\n",
      "events_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/events_photodiode.pkl\n",
      "mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/mapper_st2dat.pkl\n",
      "figs_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/figs\n",
      "metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego\n",
      "cached_dir  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/cached\n",
      "Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego/230615.yaml\n",
      "updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')\n",
      "[_sitesdirty_update] skipping! since did not find:  sites_error_spikes\n",
      "Printing whether spikes gotten (o) or not (-) because of spike peak to trough\n",
      "-  1 68.01854476928712\n",
      "o  2 70.70354537963867\n",
      "-  3 66.50849380493165\n",
      "-  4 57.76460342407227\n",
      "o  5 73.68265075683595\n",
      "-  6 68.20471878051758\n",
      "o  7 78.90354690551759\n",
      "-  8 62.39042701721191\n",
      "-  9 59.04944381713867\n",
      "-  10 68.34433898925782\n",
      "o  11 81.764208984375\n",
      "o  12 87.30356063842777\n",
      "o  13 108.1294677734375\n",
      "o  14 103.42855377197267\n",
      "o  15 117.93960037231446\n",
      "o  16 73.12588119506836\n",
      "-  17 59.54491958618164\n",
      "-  18 65.48372421264648\n",
      "-  19 65.74356842041016\n",
      "-  20 59.769720458984374\n",
      "-  21 62.47888946533203\n",
      "-  22 63.65688972473145\n",
      "o  23 87.68884048461915\n",
      "-  24 52.66859436035156\n",
      "o  25 94.49290695190432\n",
      "o  26 486.21867980957035\n",
      "-  27 64.6438377380371\n",
      "-  28 49.46780433654785\n",
      "o  29 70.96325836181641\n",
      "-  30 58.31031875610351\n",
      "-  31 50.97849044799805\n",
      "o  32 98.10257873535156\n",
      "-  33 66.27867736816407\n",
      "-  34 61.39698791503907\n",
      "-  35 57.28780288696289\n",
      "-  36 68.02859115600586\n",
      "o  37 122.87753219604492\n",
      "-  38 60.14992446899414\n",
      "-  39 63.64472045898438\n",
      "-  40 60.76703224182129\n",
      "-  41 45.90305290222168\n",
      "-  42 62.36280059814453\n",
      "-  43 62.086449813842776\n",
      "o  44 80.99810638427735\n",
      "-  45 45.69289016723633\n",
      "o  46 665.4655700683594\n",
      "-  47 55.16219253540039\n",
      "o  48 311.9402099609375\n",
      "-  49 58.010955810546875\n",
      "-  50 55.39289894104004\n",
      "-  51 48.61270751953125\n",
      "-  52 53.69010543823242\n",
      "-  53 51.46963691711426\n",
      "-  54 50.95179328918457\n",
      "-  55 57.49219245910645\n",
      "-  56 51.4183837890625\n",
      "-  57 62.378352355957034\n",
      "-  58 50.10110778808594\n",
      "-  59 48.312966918945314\n",
      "-  60 50.954193878173825\n",
      "o  61 74.6736656188965\n",
      "-  62 46.24388580322266\n",
      "-  63 48.00024223327637\n",
      "-  64 38.633465957641604\n",
      "-  65 62.68130645751953\n",
      "o  66 74.75088806152344\n",
      "o  67 135.76459655761718\n",
      "o  68 71.60760955810547\n",
      "o  69 166.79046478271485\n",
      "o  70 284.3273010253906\n",
      "o  71 79.51764526367188\n",
      "o  72 111.96119461059571\n",
      "o  73 103.45640411376954\n",
      "o  74 73.04813537597657\n",
      "o  75 108.15852584838868\n",
      "o  76 115.18047332763672\n",
      "o  77 279.6900604248047\n",
      "o  78 111.15947723388672\n",
      "-  79 57.93798675537109\n",
      "o  80 79.95513763427735\n",
      "o  81 116.15733642578125\n",
      "-  82 63.58255653381348\n",
      "o  83 150.71048889160156\n",
      "-  84 49.01909332275391\n",
      "o  85 138.2831588745117\n",
      "o  86 75.03341598510742\n",
      "o  87 140.88814849853517\n",
      "o  88 85.80642776489258\n",
      "o  89 139.750634765625\n",
      "o  90 282.9432434082031\n",
      "-  91 66.09100570678712\n",
      "o  92 248.55428161621094\n",
      "o  93 118.64986419677734\n",
      "o  94 137.77769317626954\n",
      "o  95 966.2868408203125\n",
      "o  96 143.08221130371095\n",
      "o  97 128.61636657714843\n",
      "-  98 57.23359642028809\n",
      "-  99 64.55569229125976\n",
      "-  100 55.099754333496094\n",
      "o  101 105.17969131469727\n",
      "-  102 52.84023704528809\n",
      "o  103 192.95950775146486\n",
      "o  104 78.36800842285157\n",
      "o  105 229.44867858886718\n",
      "o  106 74.22925720214845\n",
      "o  107 178.64105072021485\n",
      "o  108 103.67030792236328\n",
      "o  109 133.58123168945312\n",
      "o  110 87.86111602783203\n",
      "o  111 109.80659332275391\n",
      "o  112 113.22560424804688\n",
      "o  113 81.16412200927736\n",
      "-  114 51.3268856048584\n",
      "o  115 153.92017974853516\n",
      "o  116 84.95477905273438\n",
      "-  117 38.5596736907959\n",
      "-  118 64.6371955871582\n",
      "-  119 63.581300354003915\n",
      "o  120 95.48431015014648\n",
      "o  121 76.8210418701172\n",
      "-  122 69.30460586547852\n",
      "o  123 101.81914520263672\n",
      "-  124 67.00924224853516\n",
      "o  125 139.3381591796875\n",
      "-  126 54.70916595458984\n",
      "o  127 97.77505340576172\n",
      "o  128 81.92488098144531\n",
      "o  129 73.8795539855957\n",
      "o  130 71.37371978759765\n",
      "o  131 227.8851516723633\n",
      "-  132 49.86910743713379\n",
      "-  133 50.14724006652832\n",
      "o  134 86.3914566040039\n",
      "-  135 64.29457092285156\n",
      "-  136 45.96000862121582\n",
      "o  137 79.03761367797851\n",
      "-  138 47.38686981201172\n",
      "-  139 44.46527633666992\n",
      "-  140 41.24186401367188\n",
      "-  141 31.846679115295412\n",
      "-  142 46.321733474731445\n",
      "-  143 45.81712875366211\n",
      "-  144 46.222552108764646\n",
      "-  145 61.815982055664065\n",
      "-  146 57.876733016967776\n",
      "o  147 84.49581146240234\n",
      "-  148 67.64021682739258\n",
      "o  149 70.96876068115235\n",
      "-  150 66.34149398803712\n",
      "o  151 78.68150939941407\n",
      "-  152 47.82974662780762\n",
      "o  153 70.4279411315918\n",
      "-  154 55.96977615356445\n",
      "o  155 87.20333480834961\n",
      "-  156 50.24308776855469\n",
      "-  157 57.655668640136724\n",
      "-  158 53.87454948425293\n",
      "-  159 69.96249389648438\n",
      "-  160 42.26073303222656\n",
      "-  161 62.05127716064453\n",
      "-  162 39.465227127075195\n",
      "-  163 33.27346229553223\n",
      "-  164 45.43802337646485\n",
      "-  165 53.274678039550786\n",
      "-  166 61.042501068115236\n",
      "-  167 44.728092193603516\n",
      "o  168 77.8807472229004\n",
      "-  169 46.06727523803711\n",
      "-  170 44.894892883300784\n",
      "-  171 45.754585647583006\n",
      "-  172 60.27477111816407\n",
      "-  173 34.424874496459964\n",
      "-  174 32.19661026000977\n",
      "-  175 36.82831954956055\n",
      "-  176 42.55092697143555\n",
      "-  177 55.47130737304688\n",
      "o  178 86.18456802368165\n",
      "-  179 67.37235946655278\n",
      "-  180 36.83117942810058\n",
      "-  181 39.04174270629883\n",
      "-  182 68.86146850585938\n",
      "-  183 39.222787094116214\n",
      "o  184 130.72119598388673\n",
      "-  185 38.55737075805664\n",
      "o  186 190.72922058105468\n",
      "-  187 34.9489543914795\n",
      "-  188 45.0610107421875\n",
      "-  189 40.518270111083986\n",
      "-  190 46.87524490356446\n",
      "-  191 50.79026031494141\n",
      "-  192 51.267426300048825\n",
      "o  225 77.56058654785157\n",
      "o  226 82.7285873413086\n",
      "o  227 84.05772705078125\n",
      "-  228 66.71783065795898\n",
      "o  229 71.53538970947265\n",
      "-  230 63.57716484069825\n",
      "o  231 186.74701232910158\n",
      "-  232 67.08758773803712\n",
      "o  233 76.8680015563965\n",
      "-  234 61.388885498046875\n",
      "o  235 81.77480621337891\n",
      "-  236 63.48461494445801\n",
      "o  237 75.12851486206057\n",
      "-  238 41.911467742919925\n",
      "o  239 78.64694366455079\n",
      "-  240 56.70206260681152\n",
      "-  241 53.26259994506836\n",
      "o  242 76.87218017578125\n",
      "o  243 83.20477752685547\n",
      "-  244 60.03693389892578\n",
      "o  245 82.90343170166017\n",
      "o  246 122.02561569213867\n",
      "-  247 67.07902450561524\n",
      "o  248 90.90301132202148\n",
      "-  249 65.28554916381836\n",
      "o  250 94.38531188964843\n",
      "-  251 68.82935791015628\n",
      "o  252 123.79354248046874\n",
      "o  253 167.61736450195312\n",
      "-  254 64.28251495361329\n",
      "-  255 60.43758544921875\n",
      "-  256 56.68421821594239\n",
      "o  257 79.76203002929688\n",
      "o  258 235.71522827148436\n",
      "o  259 122.42383651733401\n",
      "o  260 135.88284149169922\n",
      "o  261 89.28667221069335\n",
      "o  262 215.06690979003906\n",
      "o  263 99.94014282226563\n",
      "o  264 128.0217269897461\n",
      "o  265 112.99837646484374\n",
      "o  266 125.34024810791016\n",
      "o  267 87.38341903686523\n",
      "o  268 92.11457519531251\n",
      "o  269 149.79512939453124\n",
      "o  270 158.1888427734375\n",
      "o  271 86.6256103515625\n",
      "-  272 67.0092025756836\n",
      "o  273 97.15330963134765\n",
      "o  274 85.61900253295899\n",
      "-  275 65.58730468750001\n",
      "o  276 104.66570129394532\n",
      "-  277 69.38417434692383\n",
      "o  278 113.21460800170898\n",
      "o  279 75.94876098632812\n",
      "o  280 147.5626205444336\n",
      "o  281 85.69547882080079\n",
      "o  282 107.63983764648438\n",
      "o  283 78.77809448242188\n",
      "o  284 156.01065521240236\n",
      "o  285 114.46945495605475\n",
      "o  286 90.88005218505859\n",
      "-  287 61.109622955322266\n",
      "-  288 31.922071838378905\n",
      "o  289 115.78063201904298\n",
      "-  290 50.402313995361325\n",
      "-  291 32.28747062683105\n",
      "-  292 54.64524459838867\n",
      "-  293 63.21452026367188\n",
      "o  294 101.97721710205079\n",
      "-  295 65.14963989257812\n",
      "o  296 77.03066635131837\n",
      "o  297 73.10081176757812\n",
      "o  298 109.7497772216797\n",
      "-  299 45.286279296875\n",
      "o  300 74.06985702514649\n",
      "-  301 40.079895401000975\n",
      "o  302 91.1807746887207\n",
      "-  303 51.92698860168457\n",
      "o  304 71.535270690918\n",
      "-  305 58.178253173828125\n",
      "-  306 44.25846481323242\n",
      "-  307 59.525834274291995\n",
      "-  308 39.10042152404785\n",
      "o  309 86.8605239868164\n",
      "-  310 68.14012069702149\n",
      "-  311 67.66310806274414\n",
      "o  312 88.67132263183593\n",
      "o  313 75.09174346923828\n",
      "-  314 64.94989547729492\n",
      "o  315 85.60259475708008\n",
      "-  316 60.11776428222656\n",
      "o  317 87.65869674682618\n",
      "-  318 64.7389404296875\n",
      "-  319 59.84067497253418\n",
      "-  320 51.536716079711915\n",
      "-  321 58.515595245361325\n",
      "-  322 68.14350280761718\n",
      "-  323 52.798729324340826\n",
      "-  324 69.08274993896485\n",
      "-  325 65.86234283447266\n",
      "-  326 66.99866409301758\n",
      "o  327 73.75535278320312\n",
      "o  328 77.45660629272462\n",
      "-  329 48.389321899414064\n",
      "o  330 71.4996726989746\n",
      "o  331 100.15144348144533\n",
      "-  332 50.7330078125\n",
      "-  333 37.836809158325195\n",
      "o  334 70.72105102539062\n",
      "-  335 38.72295913696289\n",
      "-  336 56.188574981689456\n",
      "-  337 62.36208953857422\n",
      "-  338 33.41633491516114\n",
      "-  339 49.29067649841309\n",
      "-  340 38.35028610229492\n",
      "-  341 46.961944580078125\n",
      "-  342 60.00570526123047\n",
      "o  343 70.7595947265625\n",
      "-  344 56.87401161193848\n",
      "o  345 86.59329147338867\n",
      "-  346 35.843835067749026\n",
      "o  347 94.31845703125\n",
      "o  348 136.18255462646485\n",
      "o  349 120.39072036743164\n",
      "o  350 105.06656036376955\n",
      "o  351 139.34884643554688\n",
      "o  352 93.68482513427736\n",
      "-  353 49.30702095031738\n",
      "-  354 66.75363159179688\n",
      "o  355 96.036678314209\n",
      "o  356 120.56327133178713\n",
      "-  357 63.0905502319336\n",
      "o  358 88.90457229614259\n",
      "-  359 46.827324676513676\n",
      "o  360 107.02741622924805\n",
      "-  361 53.725032043457034\n",
      "-  362 67.30703506469727\n",
      "o  363 92.3881233215332\n",
      "o  364 91.61267547607422\n",
      "-  365 67.46184158325195\n",
      "o  366 71.63844528198243\n",
      "-  367 53.98367004394532\n",
      "-  368 52.6745346069336\n",
      "-  369 58.57854118347168\n",
      "o  370 78.4235954284668\n",
      "-  371 63.91029319763184\n",
      "-  372 63.52084884643556\n",
      "o  373 91.74913940429687\n",
      "o  374 89.50123825073243\n",
      "-  375 55.65824546813965\n",
      "o  376 107.00361175537111\n",
      "o  377 83.96654968261718\n",
      "-  378 64.56443252563477\n",
      "o  379 99.92225418090821\n",
      "o  380 81.48450012207032\n",
      "o  381 111.85153045654297\n",
      "o  382 99.86210327148437\n",
      "-  383 60.795556640625\n",
      "o  384 71.49984283447266\n",
      "o  385 96.2561622619629\n",
      "-  386 66.3796859741211\n",
      "-  387 47.6780387878418\n",
      "-  388 67.78574447631836\n",
      "o  389 79.20745315551758\n",
      "o  390 80.63827590942384\n",
      "-  391 59.90886001586914\n",
      "o  392 137.681803894043\n",
      "-  393 57.30531463623047\n",
      "o  394 95.02850494384768\n",
      "-  395 58.76599884033203\n",
      "o  396 105.18712310791015\n",
      "o  397 99.0927230834961\n",
      "-  398 51.89267196655273\n",
      "-  399 66.15278778076171\n",
      "-  400 63.274600982666016\n",
      "-  401 67.40228652954102\n",
      "-  402 60.52412414550781\n",
      "o  403 173.30379486083984\n",
      "-  404 53.26470375061035\n",
      "o  405 83.65429458618165\n",
      "o  406 74.34175186157226\n",
      "o  407 102.04511871337891\n",
      "o  408 81.85402984619141\n",
      "-  409 58.94397239685059\n",
      "-  410 59.3723072052002\n",
      "o  411 285.14344787597656\n",
      "o  412 83.54205322265625\n",
      "-  413 55.980200958251956\n",
      "o  414 73.85864715576173\n",
      "o  415 172.51748352050782\n",
      "-  416 56.34063873291016\n",
      "o  417 85.80471038818361\n",
      "-  418 42.24855842590332\n",
      "o  419 93.43911819458009\n",
      "o  420 98.78361358642579\n",
      "o  421 90.19236831665039\n",
      "o  422 116.77903594970704\n",
      "o  423 213.81100769042968\n",
      "-  424 54.26167678833008\n",
      "o  425 159.1106216430664\n",
      "o  426 89.57721633911133\n",
      "o  427 103.3531982421875\n",
      "o  428 77.12169647216797\n",
      "o  429 84.55830764770508\n",
      "o  430 210.21690673828124\n",
      "o  431 84.68767318725587\n",
      "o  432 118.10135803222657\n",
      "-  433 58.65715560913086\n",
      "-  434 42.255882263183594\n",
      "o  435 93.26693267822266\n",
      "-  436 45.22244529724121\n",
      "o  437 390.92738037109376\n",
      "-  438 69.42020721435547\n",
      "o  439 114.83844223022462\n",
      "o  440 74.7224609375\n",
      "o  441 231.8165283203125\n",
      "-  442 66.06287994384766\n",
      "o  443 128.23492584228515\n",
      "o  444 109.84205551147461\n",
      "o  445 77.05730133056642\n",
      "o  446 166.4813430786133\n",
      "-  447 51.7255355834961\n",
      "o  448 111.88228073120118\n",
      "o  449 90.08182144165039\n",
      "-  450 59.505363464355476\n",
      "o  451 99.43826599121094\n",
      "o  452 95.99447631835938\n",
      "o  453 95.8028175354004\n",
      "-  454 67.65446853637695\n",
      "o  455 85.47952728271486\n",
      "o  456 82.6483039855957\n",
      "-  457 49.148735809326176\n",
      "-  458 67.11124420166016\n",
      "o  459 119.08590850830078\n",
      "-  460 54.07683486938477\n",
      "-  461 45.82667846679688\n",
      "-  462 64.77311096191407\n",
      "o  463 73.77594375610354\n",
      "-  464 49.55817337036133\n",
      "o  465 89.5463119506836\n",
      "o  466 94.19101257324219\n",
      "o  467 209.2094253540039\n",
      "o  468 79.52039337158203\n",
      "o  469 72.65539855957032\n",
      "-  470 50.86391334533691\n",
      "o  471 156.40233612060547\n",
      "o  472 78.32908782958984\n",
      "-  473 50.84913368225098\n",
      "-  474 66.31254959106445\n",
      "o  475 174.35535278320313\n",
      "o  476 122.2495819091797\n",
      "o  477 88.33863983154298\n",
      "o  478 70.8172233581543\n",
      "o  479 171.06409149169923\n",
      "-  480 63.13645133972168\n",
      "o  481 134.14472351074218\n",
      "o  482 339.5282318115235\n",
      "o  483 84.42365264892578\n",
      "o  484 97.31028671264649\n",
      "o  485 96.58515396118165\n",
      "o  486 151.33826446533203\n",
      "-  487 66.91000671386719\n",
      "o  488 78.7481658935547\n",
      "-  489 44.61540756225586\n",
      "-  490 64.60675506591797\n",
      "-  491 67.83889694213867\n",
      "o  492 90.67480010986328\n",
      "-  493 66.4961555480957\n",
      "o  494 87.56573028564453\n",
      "-  495 68.65371932983399\n",
      "o  496 94.47189636230469\n",
      "o  497 105.97969284057618\n",
      "o  498 218.17442321777344\n",
      "o  499 270.75335998535155\n",
      "o  500 94.19526062011718\n",
      "o  501 352.06737060546874\n",
      "o  502 100.41559524536133\n",
      "o  503 93.47658309936523\n",
      "o  504 86.48698272705079\n",
      "o  505 74.25923767089844\n",
      "o  506 209.13665924072265\n",
      "o  507 70.4672004699707\n",
      "o  508 99.16637725830078\n",
      "-  509 48.3790168762207\n",
      "o  510 228.08040771484374\n",
      "-  511 59.250887680053715\n",
      "o  512 111.5212272644043\n",
      "== Loading TDT tank\n",
      "** Loading tank data from local (previusly cached)\n",
      "== Done\n",
      "== Trying to load events data\n",
      "Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-104852/events_photodiode.pkl\n",
      "== Done\n",
      "** MINIMAL_LOADING, therefore loading previuosly cached data\n",
      "=== CLEANING UP self.Dat (_cleanup_reloading_saved_state) ===== \n",
      "0 _behclass_alignsim_compute\n",
      "Running D._behclass_tokens_extract_datsegs\n",
      "0 _behclass_tokens_extract_datsegs\n",
      "stored in self.Dat[BehClass]\n",
      "- starting/ending len (grouping params):\n",
      "24\n",
      "24\n",
      "- starting/ending len (getting sequence):\n",
      "24\n",
      "24\n",
      "--- Removing nans\n",
      "start len: 24\n",
      "- num names for each col\n",
      "not removing nans, since columns=[]\n",
      "ADded new column: supervision_online\n",
      "Reassigned train/test, using key: probe\n",
      "and values:\n",
      "Train =  [0]\n",
      "Test =  [1]\n",
      " \n",
      "New distribution of train/test:\n",
      "train    24\n",
      "Name: monkey_train_or_test, dtype: int64\n",
      "Appended column: los_info\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_SEQUENCE_ALPHA]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "Appended self.Dat[superv_VISUALFB_METH]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_new\n",
      "[taskgroup_reassign_by_mapper], reassigned values in column: taskgroup\n",
      "GROUPING epoch\n",
      "GROUPING_LEVELS ['230615']\n",
      "FEATURE_NAMES ['hdoffline', 'num_strokes_beh', 'num_strokes_task', 'circ', 'dist']\n",
      "SCORE_COL_NAMES []\n",
      "appended col to self.Dat:\n",
      "date_epoch\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_concise\n",
      "Append column to self.Dat:  supervision_stage_semantic\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "... Generated these...\n",
      "self.BehTrialMapList [(1, 0)]\n",
      "self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27)}\n",
      "Generated self._MapperTrialcode2TrialToTrial!\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "Extracted successfully for session:  0\n",
      "session:  1\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230615*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514\n",
      "Beh Sessions hand netered (mapping: rec sess --> beh sess):  [2, 3]\n",
      "Beh Sessions that exist on this date:  {230615: [(2, 'priminvar5'), (3, 'priminvar5')]}\n",
      "------------------------------\n",
      "Loading this neural session: 1\n",
      "Loading these beh expts: ['priminvar5']\n",
      "Loading these beh sessions: [3]\n",
      "Using this beh_trial_map_list: [(1, 0)]\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230615*/**\n",
      "Found this many paths:\n",
      "2\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-104852\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514\n",
      "{'filename_components_hyphened': ['Diego', '230615', '105514'], 'basedirs': ['/mnt/Freiwald/ltian/recordings/Diego', '/mnt/Freiwald/ltian/recordings/Diego/230615'], 'basedirs_filenames': ['230615', 'Diego-230615-105514'], 'filename_final_ext': 'Diego-230615-105514', 'filename_final_noext': 'Diego-230615-105514'}\n",
      "FOund this path for spikes:  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514/spikes_tdt_quick-4\n",
      "== PATHS for this expt: \n",
      "raws  --  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514\n",
      "tank  --  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514/Diego-230615-105514\n",
      "spikes  --  /mnt/Freiwald/ltian/recordings/Diego/230615/Diego-230615-105514/spikes_tdt_quick-4\n",
      "final_dir_name  --  Diego-230615-105514\n",
      "time  --  105514\n",
      "pathbase_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514\n",
      "tank_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/data_tank.pkl\n",
      "spikes_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/data_spikes.pkl\n",
      "datall_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/data_datall.pkl\n",
      "events_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/events_photodiode.pkl\n",
      "mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/mapper_st2dat.pkl\n",
      "figs_local  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/figs\n",
      "metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego\n",
      "cached_dir  --  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/cached\n",
      "Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego/230615.yaml\n",
      "updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')\n",
      "[_sitesdirty_update] skipping! since did not find:  sites_error_spikes\n",
      "Printing whether spikes gotten (o) or not (-) because of spike peak to trough\n",
      "-  1 64.70497589111328\n",
      "o  2 70.79840850830078\n",
      "-  3 66.79705657958985\n",
      "-  4 58.867055892944336\n",
      "-  5 69.1676254272461\n",
      "o  6 71.37371368408203\n",
      "o  7 77.48218612670898\n",
      "-  8 59.850395584106444\n",
      "-  9 56.50086517333985\n",
      "-  10 67.07558746337891\n",
      "o  11 78.22493057250976\n",
      "o  12 76.13535614013672\n",
      "o  13 96.82398834228516\n",
      "o  14 78.78068161010742\n",
      "o  15 90.1428092956543\n",
      "-  16 66.51999053955079\n",
      "-  17 56.96490325927734\n",
      "-  18 63.00419273376465\n",
      "-  19 55.13617782592774\n",
      "-  20 60.69014549255371\n",
      "-  21 58.68099098205566\n",
      "-  22 65.09588928222657\n",
      "o  23 91.83484191894532\n",
      "-  24 53.36025924682618\n",
      "o  25 86.7066017150879\n",
      "o  26 428.91435241699224\n",
      "-  27 60.49651222229004\n",
      "-  28 48.782205200195314\n",
      "-  29 69.11810302734376\n",
      "-  30 60.03212471008302\n",
      "-  31 48.50477294921875\n",
      "o  32 76.95400619506837\n",
      "-  33 65.25360717773438\n",
      "-  34 60.27709541320801\n",
      "-  35 55.71850204467774\n",
      "-  36 64.59062576293945\n",
      "o  37 109.0134262084961\n",
      "-  38 62.57754058837891\n",
      "-  39 60.63677520751954\n",
      "-  40 58.625421142578126\n",
      "-  41 44.71713829040527\n",
      "-  42 57.44486694335937\n",
      "-  43 62.70497779846192\n",
      "o  44 81.18171615600586\n",
      "-  45 44.25368728637696\n",
      "o  46 461.84435729980476\n",
      "-  47 55.632479476928715\n",
      "o  48 328.9082794189453\n",
      "-  49 55.9321949005127\n",
      "-  50 52.5673324584961\n",
      "-  51 47.814792251586915\n",
      "-  52 53.363190460205075\n",
      "-  53 51.390673828125\n",
      "-  54 48.9815673828125\n",
      "-  55 56.29172554016113\n",
      "-  56 50.35234375\n",
      "-  57 65.8384895324707\n",
      "-  58 49.65246238708496\n",
      "-  59 46.454264068603514\n",
      "-  60 50.275785827636724\n",
      "-  61 68.36811447143555\n",
      "-  62 45.52688674926758\n",
      "-  63 46.540218353271484\n",
      "-  64 37.275069046020505\n",
      "-  65 65.06106414794922\n",
      "o  66 72.40497817993165\n",
      "o  67 142.51475677490234\n",
      "o  68 72.61145553588867\n",
      "o  69 158.82969818115234\n",
      "o  70 267.9433166503906\n",
      "o  71 88.68673553466797\n",
      "o  72 113.37611236572268\n",
      "o  73 105.93332061767579\n",
      "o  74 72.98647689819336\n",
      "o  75 106.41076049804688\n",
      "o  76 113.4280158996582\n",
      "o  77 252.97787017822267\n",
      "o  78 99.15912170410157\n",
      "-  79 60.31609840393067\n",
      "o  80 78.88012390136718\n",
      "o  81 112.90300674438477\n",
      "-  82 66.98342590332031\n",
      "o  83 240.3096466064453\n",
      "-  84 51.34337120056153\n",
      "o  85 134.57528076171877\n",
      "o  86 78.74923858642578\n",
      "o  87 148.1258071899414\n",
      "o  88 101.4878372192383\n",
      "o  89 149.68781585693358\n",
      "o  90 264.2841735839845\n",
      "-  91 69.11046295166015\n",
      "o  92 235.04952087402344\n",
      "o  93 121.40466079711916\n",
      "o  94 129.1829849243164\n",
      "o  95 784.5246459960938\n",
      "o  96 138.5107635498047\n",
      "o  97 126.8429931640625\n",
      "-  98 57.071405410766616\n",
      "-  99 64.59078826904297\n",
      "-  100 50.890737533569336\n",
      "o  101 103.80848541259766\n",
      "-  102 52.03642959594727\n",
      "o  103 206.35611114501953\n",
      "o  104 76.0744842529297\n",
      "o  105 232.75088348388672\n",
      "o  106 119.61953201293946\n",
      "o  107 227.6879638671875\n",
      "o  108 104.3611312866211\n",
      "o  109 130.3154541015625\n",
      "o  110 88.19529724121094\n",
      "o  111 120.95923919677735\n",
      "o  112 108.62521667480469\n",
      "o  113 81.49602279663087\n",
      "-  114 51.04822578430176\n",
      "o  115 131.70343322753908\n",
      "o  116 87.59645385742188\n",
      "-  117 37.581484985351565\n",
      "-  118 63.70029563903809\n",
      "-  119 64.55434570312501\n",
      "o  120 92.99441299438476\n",
      "o  121 72.67174530029297\n",
      "-  122 69.93772506713867\n",
      "o  123 107.3335029602051\n",
      "-  124 63.46861915588379\n",
      "o  125 144.16297454833983\n",
      "-  126 57.745450592041024\n",
      "o  127 95.95997314453125\n",
      "o  128 83.2071029663086\n",
      "o  129 76.97311630249024\n",
      "o  130 70.47487106323243\n",
      "o  131 228.80845642089844\n",
      "-  132 49.34553604125977\n",
      "-  133 49.224349975585945\n",
      "o  134 89.71570205688477\n",
      "o  135 74.33246002197265\n",
      "-  136 44.54200401306152\n",
      "o  137 83.29804992675781\n",
      "-  138 44.09211883544922\n",
      "-  139 47.05691299438477\n",
      "-  140 39.69609756469727\n",
      "-  141 29.67902030944824\n",
      "-  142 46.28593406677246\n",
      "-  143 44.91890106201172\n",
      "-  144 47.267055892944335\n",
      "-  145 65.54055786132812\n",
      "-  146 65.59669723510743\n",
      "o  147 92.55586776733398\n",
      "-  148 66.10108184814453\n",
      "o  149 75.21699981689453\n",
      "-  150 68.56681289672852\n",
      "o  151 84.30368423461914\n",
      "-  152 52.39178886413574\n",
      "o  153 70.63798141479492\n",
      "-  154 56.576791763305664\n",
      "o  155 87.51827545166016\n",
      "-  156 49.578903961181645\n",
      "-  157 57.59147338867188\n",
      "-  158 54.37297592163086\n",
      "-  159 66.70643005371096\n",
      "-  160 42.57966003417969\n",
      "-  161 59.83207168579102\n",
      "-  162 38.01113433837891\n",
      "-  163 34.9789737701416\n",
      "-  164 43.76262435913086\n",
      "-  165 51.819976806640625\n",
      "-  166 60.65597076416016\n",
      "-  167 46.388401412963866\n",
      "o  168 80.37323455810547\n",
      "-  169 44.393450546264646\n",
      "-  170 42.166702651977545\n",
      "-  171 43.8338119506836\n",
      "-  172 58.45182342529297\n",
      "-  173 35.88649940490723\n",
      "-  174 27.58219680786133\n",
      "-  175 39.77995185852051\n",
      "-  176 41.34300537109375\n",
      "-  177 54.82794952392578\n",
      "o  178 80.95177307128907\n",
      "-  179 55.209220504760744\n",
      "-  180 39.16178512573242\n",
      "-  181 37.820116424560545\n",
      "-  182 69.42584228515625\n",
      "-  183 39.11778182983399\n",
      "o  184 125.34897994995119\n",
      "-  185 37.140682983398435\n",
      "o  186 206.63740997314454\n",
      "-  187 30.621662902832032\n",
      "-  188 45.092096328735366\n",
      "-  189 40.979317855834964\n",
      "-  190 42.784140014648436\n",
      "-  191 53.50264129638672\n",
      "-  192 49.72571029663086\n",
      "o  225 95.59719848632812\n",
      "-  226 59.88669700622561\n",
      "o  227 85.94067153930665\n",
      "o  228 70.6693962097168\n",
      "o  229 71.10961303710937\n",
      "-  230 62.70861587524414\n",
      "o  231 176.30182495117188\n",
      "o  232 70.69194259643555\n",
      "o  233 75.35380325317384\n",
      "-  234 65.08861083984375\n",
      "o  235 87.00098114013673\n",
      "-  236 63.75924835205078\n",
      "o  237 78.44031219482422\n",
      "-  238 38.46920509338379\n",
      "o  239 77.86506805419923\n",
      "-  240 54.49495620727539\n",
      "-  241 54.941179275512695\n",
      "o  242 82.98463897705078\n",
      "o  243 89.30200958251953\n",
      "-  244 60.95017852783203\n",
      "o  245 78.11139450073242\n",
      "o  246 135.60389251708986\n",
      "o  247 74.67323455810548\n",
      "o  248 95.01296844482422\n",
      "-  249 69.91086883544922\n",
      "o  250 95.8973846435547\n",
      "o  251 70.1897590637207\n",
      "o  252 128.67677154541016\n",
      "o  253 131.00688934326172\n",
      "-  254 66.45284652709961\n",
      "-  255 60.4341064453125\n",
      "-  256 60.334450912475596\n",
      "o  257 77.00186462402345\n",
      "o  258 224.9828628540039\n",
      "o  259 188.4564437866211\n",
      "o  260 134.5898910522461\n",
      "o  261 89.50992736816407\n",
      "o  262 210.17927246093754\n",
      "o  263 104.4097297668457\n",
      "o  264 111.83044509887695\n",
      "o  265 114.10407028198242\n",
      "o  266 119.58732604980469\n",
      "o  267 89.25304260253907\n",
      "o  268 110.3693115234375\n",
      "o  269 160.20132141113282\n",
      "o  270 90.47927856445314\n",
      "o  271 70.56466369628907\n",
      "-  272 65.04246978759765\n",
      "o  273 97.70690917968751\n",
      "o  274 81.79069747924805\n",
      "o  275 73.69986648559572\n",
      "o  276 104.95803833007812\n",
      "o  277 70.38788528442383\n",
      "o  278 102.5410499572754\n",
      "o  279 73.60018005371094\n",
      "o  280 158.19833374023438\n",
      "o  281 81.61553802490235\n",
      "o  282 108.25359497070312\n",
      "o  283 83.02629699707032\n",
      "o  284 154.99036254882813\n",
      "o  285 180.2840454101563\n",
      "o  286 89.63203430175781\n",
      "-  287 59.53878593444824\n",
      "-  288 31.32132797241211\n",
      "o  289 105.20557022094727\n",
      "-  290 48.01884307861328\n",
      "-  291 31.599430656433107\n",
      "-  292 54.99943389892579\n",
      "-  293 63.681725311279294\n",
      "o  294 100.60935287475587\n",
      "o  295 71.42557907104492\n",
      "o  296 71.99364471435547\n",
      "o  297 70.50077362060547\n",
      "o  298 105.74660720825196\n",
      "-  299 45.000240325927734\n",
      "o  300 72.50397720336915\n",
      "-  301 39.80850830078125\n",
      "o  302 89.41687774658203\n",
      "-  303 51.82257766723633\n",
      "-  304 61.4344223022461\n",
      "-  305 63.41684112548828\n",
      "-  306 43.322823715209964\n",
      "-  307 61.37192344665528\n",
      "-  308 38.85002479553223\n",
      "o  309 83.11310348510743\n",
      "-  310 64.86150817871093\n",
      "o  311 71.93751296997071\n",
      "o  312 89.8837028503418\n",
      "o  313 85.98859252929688\n",
      "-  314 63.830637359619146\n",
      "o  315 80.97839813232422\n",
      "-  316 61.03964805603027\n",
      "o  317 89.27656784057618\n",
      "-  318 58.119012451171876\n",
      "-  319 59.39525032043457\n",
      "-  320 52.206643676757814\n",
      "-  321 58.67764396667481\n",
      "-  322 59.96737747192383\n",
      "-  323 62.94246902465823\n",
      "-  324 67.96900482177735\n",
      "o  325 70.48006134033203\n",
      "-  326 67.92576065063477\n",
      "o  327 72.88161010742188\n",
      "o  328 77.43824615478516\n",
      "-  329 54.608464050292966\n",
      "o  330 74.00506134033203\n",
      "o  331 99.18228149414062\n",
      "-  332 57.58678131103516\n",
      "-  333 40.314863204956055\n",
      "o  334 83.6329231262207\n",
      "-  335 38.53331336975098\n",
      "-  336 63.285478973388685\n",
      "-  337 52.18590240478515\n",
      "-  338 33.33166084289551\n",
      "-  339 46.73866157531739\n",
      "-  340 39.367326736450195\n",
      "-  341 49.51810302734375\n",
      "-  342 58.604120635986334\n",
      "o  343 78.682275390625\n",
      "-  344 57.69080657958985\n",
      "o  345 82.1061767578125\n",
      "-  346 35.35327262878418\n",
      "o  347 112.04069671630859\n",
      "o  348 123.21668167114258\n",
      "o  349 106.972705078125\n",
      "o  350 95.65330810546875\n",
      "o  351 168.75977478027343\n",
      "o  352 81.23733978271486\n",
      "-  353 50.301891326904304\n",
      "-  354 64.85276412963867\n",
      "o  355 89.72198486328125\n",
      "o  356 131.63162384033203\n",
      "-  357 59.61788558959961\n",
      "o  358 89.24421691894531\n",
      "-  359 45.629787826538085\n",
      "o  360 119.14530715942384\n",
      "-  361 56.38354644775391\n",
      "-  362 67.11667098999024\n",
      "o  363 85.71769409179689\n",
      "o  364 114.9013427734375\n",
      "-  365 65.8236457824707\n",
      "-  366 65.19835815429688\n",
      "-  367 50.61802978515625\n",
      "-  368 53.70093765258789\n",
      "-  369 59.368184661865236\n",
      "o  370 85.20991668701173\n",
      "o  371 95.83373489379883\n",
      "-  372 55.76255836486817\n",
      "o  373 94.33166427612305\n",
      "o  374 115.36933746337891\n",
      "-  375 56.6016471862793\n",
      "o  376 102.43492431640625\n",
      "o  377 81.8212905883789\n",
      "-  378 61.97549171447754\n",
      "o  379 111.90651702880861\n",
      "o  380 81.330810546875\n",
      "o  381 141.4147644042969\n",
      "o  382 101.43147583007813\n",
      "-  383 63.360931396484375\n",
      "o  384 70.93215866088867\n",
      "o  385 95.12571029663086\n",
      "-  386 64.76531448364258\n",
      "-  387 48.971477890014654\n",
      "-  388 69.62140045166016\n",
      "o  389 76.52764511108398\n",
      "o  390 93.46201019287109\n",
      "-  391 58.68976860046387\n",
      "o  392 155.53216400146485\n",
      "-  393 56.29294548034668\n",
      "o  394 92.97535552978516\n",
      "-  395 62.03368072509766\n",
      "o  396 102.47354125976562\n",
      "o  397 96.76592483520508\n",
      "-  398 51.8848316192627\n",
      "-  399 64.83347091674804\n",
      "o  400 70.05347518920898\n",
      "-  401 58.89890670776367\n",
      "-  402 59.86304473876953\n",
      "o  403 159.6084823608399\n",
      "-  404 51.307942962646486\n",
      "o  405 81.9513580322266\n",
      "o  406 79.66191787719727\n",
      "o  407 99.43415298461915\n",
      "o  408 80.87478332519531\n",
      "-  409 58.97012329101563\n",
      "-  410 58.75976638793946\n",
      "o  411 272.6666625976562\n",
      "o  412 76.06977157592773\n",
      "-  413 55.47145652770996\n",
      "o  414 96.47435150146484\n",
      "o  415 168.54577941894533\n",
      "-  416 56.70339431762695\n",
      "o  417 88.01181182861329\n",
      "-  418 42.21710395812988\n",
      "o  419 90.23485260009765\n",
      "o  420 94.19541931152344\n",
      "o  421 83.50453186035156\n",
      "o  422 110.23514099121094\n",
      "o  423 179.2789047241211\n",
      "-  424 53.94992294311524\n",
      "o  425 164.7197250366211\n",
      "o  426 108.37545776367188\n",
      "o  427 99.38599395751953\n",
      "-  428 59.008596420288086\n",
      "o  429 86.87159576416016\n",
      "o  430 171.91833190917973\n",
      "o  431 80.41638336181641\n",
      "o  432 102.76162948608399\n",
      "-  433 58.8278549194336\n",
      "-  434 42.55015754699707\n",
      "o  435 93.13616180419922\n",
      "-  436 45.23281555175782\n",
      "o  437 287.62281799316406\n",
      "o  438 71.5517074584961\n",
      "o  439 122.56441955566407\n",
      "o  440 76.60895690917968\n",
      "o  441 250.89266052246094\n",
      "-  442 66.89662246704101\n",
      "o  443 130.68404235839844\n",
      "o  444 114.41794815063477\n",
      "o  445 75.65799560546876\n",
      "o  446 149.63072204589844\n",
      "-  447 50.72295074462891\n",
      "o  448 95.0913803100586\n",
      "o  449 77.8703987121582\n",
      "-  450 60.96868629455567\n",
      "o  451 97.52481155395508\n",
      "o  452 99.85901794433595\n",
      "o  453 94.55700607299805\n",
      "o  454 70.76553573608399\n",
      "o  455 73.66389846801759\n",
      "o  456 78.83583297729493\n",
      "-  457 48.88233757019043\n",
      "-  458 60.77020835876465\n",
      "o  459 107.4057716369629\n",
      "-  460 54.8642936706543\n",
      "-  461 44.55413742065431\n",
      "o  462 70.19103393554687\n",
      "o  463 164.23838806152344\n",
      "-  464 49.5395191192627\n",
      "o  465 93.12625427246094\n",
      "o  466 102.91813201904297\n",
      "o  467 197.12177124023438\n",
      "o  468 76.50212631225585\n",
      "o  469 88.2828727722168\n",
      "-  470 50.81877822875977\n",
      "o  471 190.84933471679688\n",
      "o  472 74.04007339477539\n",
      "-  473 50.76114044189453\n",
      "-  474 64.33766098022461\n",
      "o  475 169.03682250976564\n",
      "o  476 108.13667449951178\n",
      "o  477 88.25552291870117\n",
      "-  478 64.6879867553711\n",
      "o  479 148.9693572998047\n",
      "-  480 64.40134582519532\n",
      "o  481 124.64790191650391\n",
      "o  482 304.18549804687507\n",
      "o  483 83.38544006347657\n",
      "o  484 97.8193145751953\n",
      "o  485 94.35020904541015\n",
      "o  486 143.14940948486327\n",
      "-  487 65.31767883300782\n",
      "o  488 73.18040237426759\n",
      "-  489 43.64071617126465\n",
      "-  490 63.92857398986817\n",
      "-  491 66.51629638671876\n",
      "o  492 88.14693679809571\n",
      "-  493 67.37301330566407\n",
      "o  494 102.2636947631836\n",
      "o  495 124.40481414794922\n",
      "o  496 112.6118034362793\n",
      "o  497 89.3576774597168\n",
      "o  498 210.30703735351562\n",
      "o  499 229.44557189941406\n",
      "o  500 107.3958953857422\n",
      "o  501 289.0433807373047\n",
      "o  502 102.00417022705079\n",
      "o  503 89.81569061279296\n",
      "o  504 99.1506561279297\n",
      "o  505 85.2096908569336\n",
      "o  506 204.98834228515625\n",
      "-  507 68.5327018737793\n",
      "o  508 98.68197326660156\n",
      "-  509 48.19075050354004\n",
      "o  510 218.03623657226564\n",
      "-  511 60.44444732666015\n",
      "o  512 110.29388122558593\n",
      "== Loading TDT tank\n",
      "** Loading tank data from local (previusly cached)\n",
      "== Done\n",
      "== Trying to load events data\n",
      "Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Diego/230615/Diego-230615-105514/events_photodiode.pkl\n",
      "== Done\n",
      "** MINIMAL_LOADING, therefore loading previuosly cached data\n",
      "=== CLEANING UP self.Dat (_cleanup_reloading_saved_state) ===== \n",
      "0 _behclass_alignsim_compute\n",
      "200 _behclass_alignsim_compute\n",
      "400 _behclass_alignsim_compute\n",
      "Running D._behclass_tokens_extract_datsegs\n",
      "0 _behclass_tokens_extract_datsegs\n",
      "200 _behclass_tokens_extract_datsegs\n",
      "400 _behclass_tokens_extract_datsegs\n",
      "stored in self.Dat[BehClass]\n",
      "- starting/ending len (grouping params):\n",
      "423\n",
      "423\n",
      "- starting/ending len (getting sequence):\n",
      "423\n",
      "423\n",
      "--- Removing nans\n",
      "start len: 423\n",
      "- num names for each col\n",
      "not removing nans, since columns=[]\n",
      "ADded new column: supervision_online\n",
      "Reassigned train/test, using key: probe\n",
      "and values:\n",
      "Train =  [0]\n",
      "Test =  [1]\n",
      " \n",
      "New distribution of train/test:\n",
      "train    423\n",
      "Name: monkey_train_or_test, dtype: int64\n",
      "Appended column: los_info\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_SEQUENCE_ALPHA]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "Appended self.Dat[superv_VISUALFB_METH]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_new\n",
      "[taskgroup_reassign_by_mapper], reassigned values in column: taskgroup\n",
      "GROUPING epoch\n",
      "GROUPING_LEVELS ['230615']\n",
      "FEATURE_NAMES ['hdoffline', 'num_strokes_beh', 'num_strokes_task', 'circ', 'dist']\n",
      "SCORE_COL_NAMES []\n",
      "appended col to self.Dat:\n",
      "date_epoch\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_concise\n",
      "Append column to self.Dat:  supervision_stage_semantic\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "... Generated these...\n",
      "self.BehTrialMapList [(1, 0)]\n",
      "self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27), 27: (0, 28), 28: (0, 29), 29: (0, 30), 30: (0, 31), 31: (0, 32), 32: (0, 33), 33: (0, 34), 34: (0, 35), 35: (0, 36), 36: (0, 37), 37: (0, 38), 38: (0, 39), 39: (0, 40), 40: (0, 41), 41: (0, 42), 42: (0, 43), 43: (0, 44), 44: (0, 45), 45: (0, 46), 46: (0, 47), 47: (0, 48), 48: (0, 49), 49: (0, 50), 50: (0, 51), 51: (0, 52), 52: (0, 53), 53: (0, 54), 54: (0, 55), 55: (0, 56), 56: (0, 57), 57: (0, 58), 58: (0, 59), 59: (0, 60), 60: (0, 61), 61: (0, 62), 62: (0, 63), 63: (0, 64), 64: (0, 65), 65: (0, 66), 66: (0, 67), 67: (0, 68), 68: (0, 69), 69: (0, 70), 70: (0, 71), 71: (0, 72), 72: (0, 73), 73: (0, 74), 74: (0, 75), 75: (0, 76), 76: (0, 77), 77: (0, 78), 78: (0, 79), 79: (0, 80), 80: (0, 81), 81: (0, 82), 82: (0, 83), 83: (0, 84), 84: (0, 85), 85: (0, 86), 86: (0, 87), 87: (0, 88), 88: (0, 89), 89: (0, 90), 90: (0, 91), 91: (0, 92), 92: (0, 93), 93: (0, 94), 94: (0, 95), 95: (0, 96), 96: (0, 97), 97: (0, 98), 98: (0, 99), 99: (0, 100), 100: (0, 101), 101: (0, 102), 102: (0, 103), 103: (0, 104), 104: (0, 105), 105: (0, 106), 106: (0, 107), 107: (0, 108), 108: (0, 109), 109: (0, 110), 110: (0, 111), 111: (0, 112), 112: (0, 113), 113: (0, 114), 114: (0, 115), 115: (0, 116), 116: (0, 117), 117: (0, 118), 118: (0, 119), 119: (0, 120), 120: (0, 121), 121: (0, 122), 122: (0, 123), 123: (0, 124), 124: (0, 125), 125: (0, 126), 126: (0, 127), 127: (0, 128), 128: (0, 129), 129: (0, 130), 130: (0, 131), 131: (0, 132), 132: (0, 133), 133: (0, 134), 134: (0, 135), 135: (0, 136), 136: (0, 137), 137: (0, 138), 138: (0, 139), 139: (0, 140), 140: (0, 141), 141: (0, 142), 142: (0, 143), 143: (0, 144), 144: (0, 145), 145: (0, 146), 146: (0, 147), 147: (0, 148), 148: (0, 149), 149: (0, 150), 150: (0, 151), 151: (0, 152), 152: (0, 153), 153: (0, 154), 154: (0, 155), 155: (0, 156), 156: (0, 157), 157: (0, 158), 158: (0, 159), 159: (0, 160), 160: (0, 161), 161: (0, 162), 162: (0, 163), 163: (0, 164), 164: (0, 165), 165: (0, 166), 166: (0, 167), 167: (0, 168), 168: (0, 169), 169: (0, 170), 170: (0, 171), 171: (0, 172), 172: (0, 173), 173: (0, 174), 174: (0, 175), 175: (0, 176), 176: (0, 177), 177: (0, 178), 178: (0, 179), 179: (0, 180), 180: (0, 181), 181: (0, 182), 182: (0, 183), 183: (0, 184), 184: (0, 185), 185: (0, 186), 186: (0, 187), 187: (0, 188), 188: (0, 189), 189: (0, 190), 190: (0, 191), 191: (0, 192), 192: (0, 193), 193: (0, 194), 194: (0, 195), 195: (0, 196), 196: (0, 197), 197: (0, 198), 198: (0, 199), 199: (0, 200), 200: (0, 201), 201: (0, 202), 202: (0, 203), 203: (0, 204), 204: (0, 205), 205: (0, 206), 206: (0, 207), 207: (0, 208), 208: (0, 209), 209: (0, 210), 210: (0, 211), 211: (0, 212), 212: (0, 213), 213: (0, 214), 214: (0, 215), 215: (0, 216), 216: (0, 217), 217: (0, 218), 218: (0, 219), 219: (0, 220), 220: (0, 221), 221: (0, 222), 222: (0, 223), 223: (0, 224), 224: (0, 225), 225: (0, 226), 226: (0, 227), 227: (0, 228), 228: (0, 229), 229: (0, 230), 230: (0, 231), 231: (0, 232), 232: (0, 233), 233: (0, 234), 234: (0, 235), 235: (0, 236), 236: (0, 237), 237: (0, 238), 238: (0, 239), 239: (0, 240), 240: (0, 241), 241: (0, 242), 242: (0, 243), 243: (0, 244), 244: (0, 245), 245: (0, 246), 246: (0, 247), 247: (0, 248), 248: (0, 249), 249: (0, 250), 250: (0, 251), 251: (0, 252), 252: (0, 253), 253: (0, 254), 254: (0, 255), 255: (0, 256), 256: (0, 257), 257: (0, 258), 258: (0, 259), 259: (0, 260), 260: (0, 261), 261: (0, 262), 262: (0, 263), 263: (0, 264), 264: (0, 265), 265: (0, 266), 266: (0, 267), 267: (0, 268), 268: (0, 269), 269: (0, 270), 270: (0, 271), 271: (0, 272), 272: (0, 273), 273: (0, 274), 274: (0, 275), 275: (0, 276), 276: (0, 277), 277: (0, 278), 278: (0, 279), 279: (0, 280), 280: (0, 281), 281: (0, 282), 282: (0, 283), 283: (0, 284), 284: (0, 285), 285: (0, 286), 286: (0, 287), 287: (0, 288), 288: (0, 289), 289: (0, 290), 290: (0, 291), 291: (0, 292), 292: (0, 293), 293: (0, 294), 294: (0, 295), 295: (0, 296), 296: (0, 297), 297: (0, 298), 298: (0, 299), 299: (0, 300), 300: (0, 301), 301: (0, 302), 302: (0, 303), 303: (0, 304), 304: (0, 305), 305: (0, 306), 306: (0, 307), 307: (0, 308), 308: (0, 309), 309: (0, 310), 310: (0, 311), 311: (0, 312), 312: (0, 313), 313: (0, 314), 314: (0, 315), 315: (0, 316), 316: (0, 317), 317: (0, 318), 318: (0, 319), 319: (0, 320), 320: (0, 321), 321: (0, 322), 322: (0, 323), 323: (0, 324), 324: (0, 325), 325: (0, 326), 326: (0, 327), 327: (0, 328), 328: (0, 329), 329: (0, 330), 330: (0, 331), 331: (0, 332), 332: (0, 333), 333: (0, 334), 334: (0, 335), 335: (0, 336), 336: (0, 337), 337: (0, 338), 338: (0, 339), 339: (0, 340), 340: (0, 341), 341: (0, 342), 342: (0, 343), 343: (0, 344), 344: (0, 345), 345: (0, 346), 346: (0, 347), 347: (0, 348), 348: (0, 349), 349: (0, 350), 350: (0, 351), 351: (0, 352), 352: (0, 353), 353: (0, 354), 354: (0, 355), 355: (0, 356), 356: (0, 357), 357: (0, 358), 358: (0, 359), 359: (0, 360), 360: (0, 361), 361: (0, 362), 362: (0, 363), 363: (0, 364), 364: (0, 365), 365: (0, 366), 366: (0, 367), 367: (0, 368), 368: (0, 369), 369: (0, 370), 370: (0, 371), 371: (0, 372), 372: (0, 373), 373: (0, 374), 374: (0, 375), 375: (0, 376), 376: (0, 377), 377: (0, 378), 378: (0, 379), 379: (0, 380), 380: (0, 381), 381: (0, 382), 382: (0, 383), 383: (0, 384), 384: (0, 385), 385: (0, 386), 386: (0, 387), 387: (0, 388), 388: (0, 389), 389: (0, 390), 390: (0, 391), 391: (0, 392), 392: (0, 393), 393: (0, 394), 394: (0, 395), 395: (0, 396), 396: (0, 397), 397: (0, 398), 398: (0, 399), 399: (0, 400), 400: (0, 401), 401: (0, 402), 402: (0, 403), 403: (0, 404), 404: (0, 405), 405: (0, 406), 406: (0, 407), 407: (0, 408), 408: (0, 409), 409: (0, 410), 410: (0, 411), 411: (0, 412), 412: (0, 413), 413: (0, 414), 414: (0, 415), 415: (0, 416), 416: (0, 417), 417: (0, 418), 418: (0, 419), 419: (0, 420), 420: (0, 421), 421: (0, 422), 422: (0, 423), 423: (0, 424), 424: (0, 425), 425: (0, 426), 426: (0, 427), 427: (0, 428), 428: (0, 429), 429: (0, 430), 430: (0, 431), 431: (0, 432), 432: (0, 433), 433: (0, 434), 434: (0, 435), 435: (0, 436), 436: (0, 437), 437: (0, 438), 438: (0, 439), 439: (0, 440), 440: (0, 441), 441: (0, 442), 442: (0, 443), 443: (0, 444), 444: (0, 445), 445: (0, 446), 446: (0, 447), 447: (0, 448), 448: (0, 449), 449: (0, 450), 450: (0, 451), 451: (0, 452), 452: (0, 453), 453: (0, 454), 454: (0, 455), 455: (0, 456), 456: (0, 457), 457: (0, 458), 458: (0, 459), 459: (0, 460), 460: (0, 461), 461: (0, 462), 462: (0, 463), 463: (0, 464), 464: (0, 465), 465: (0, 466), 466: (0, 467), 467: (0, 468), 468: (0, 469), 469: (0, 470), 470: (0, 471), 471: (0, 472), 472: (0, 473), 473: (0, 474), 474: (0, 475), 475: (0, 476), 476: (0, 477), 477: (0, 478), 478: (0, 479), 479: (0, 480), 480: (0, 481), 481: (0, 482), 482: (0, 483), 483: (0, 484), 484: (0, 485), 485: (0, 486), 486: (0, 487), 487: (0, 488), 488: (0, 489), 489: (0, 490), 490: (0, 491), 491: (0, 492), 492: (0, 493), 493: (0, 494), 494: (0, 495), 495: (0, 496), 496: (0, 497), 497: (0, 498), 498: (0, 499), 499: (0, 500), 500: (0, 501), 501: (0, 502), 502: (0, 503), 503: (0, 504), 504: (0, 505), 505: (0, 506), 506: (0, 507), 507: (0, 508), 508: (0, 509), 509: (0, 510), 510: (0, 511), 511: (0, 512), 512: (0, 513), 513: (0, 514), 514: (0, 515), 515: (0, 516), 516: (0, 517), 517: (0, 518), 518: (0, 519), 519: (0, 520), 520: (0, 521), 521: (0, 522), 522: (0, 523), 523: (0, 524), 524: (0, 525), 525: (0, 526), 526: (0, 527), 527: (0, 528), 528: (0, 529), 529: (0, 530), 530: (0, 531), 531: (0, 532), 532: (0, 533), 533: (0, 534), 534: (0, 535), 535: (0, 536), 536: (0, 537), 537: (0, 538), 538: (0, 539), 539: (0, 540), 540: (0, 541), 541: (0, 542), 542: (0, 543), 543: (0, 544), 544: (0, 545), 545: (0, 546), 546: (0, 547), 547: (0, 548), 548: (0, 549), 549: (0, 550), 550: (0, 551), 551: (0, 552), 552: (0, 553), 553: (0, 554), 554: (0, 555), 555: (0, 556), 556: (0, 557), 557: (0, 558), 558: (0, 559), 559: (0, 560), 560: (0, 561), 561: (0, 562), 562: (0, 563), 563: (0, 564), 564: (0, 565), 565: (0, 566), 566: (0, 567), 567: (0, 568), 568: (0, 569), 569: (0, 570), 570: (0, 571), 571: (0, 572), 572: (0, 573), 573: (0, 574), 574: (0, 575), 575: (0, 576), 576: (0, 577), 577: (0, 578), 578: (0, 579), 579: (0, 580), 580: (0, 581), 581: (0, 582), 582: (0, 583), 583: (0, 584), 584: (0, 585), 585: (0, 586), 586: (0, 587), 587: (0, 588), 588: (0, 589), 589: (0, 590), 590: (0, 591), 591: (0, 592), 592: (0, 593), 593: (0, 594), 594: (0, 595), 595: (0, 596), 596: (0, 597), 597: (0, 598), 598: (0, 599), 599: (0, 600), 600: (0, 601), 601: (0, 602), 602: (0, 603), 603: (0, 604), 604: (0, 605), 605: (0, 606), 606: (0, 607), 607: (0, 608), 608: (0, 609), 609: (0, 610), 610: (0, 611), 611: (0, 612), 612: (0, 613), 613: (0, 614), 614: (0, 615), 615: (0, 616), 616: (0, 617), 617: (0, 618), 618: (0, 619), 619: (0, 620), 620: (0, 621), 621: (0, 622), 622: (0, 623), 623: (0, 624), 624: (0, 625), 625: (0, 626), 626: (0, 627), 627: (0, 628), 628: (0, 629), 629: (0, 630), 630: (0, 631), 631: (0, 632), 632: (0, 633), 633: (0, 634), 634: (0, 635), 635: (0, 636), 636: (0, 637), 637: (0, 638), 638: (0, 639), 639: (0, 640), 640: (0, 641), 641: (0, 642), 642: (0, 643), 643: (0, 644), 644: (0, 645), 645: (0, 646), 646: (0, 647), 647: (0, 648), 648: (0, 649), 649: (0, 650), 650: (0, 651), 651: (0, 652), 652: (0, 653), 653: (0, 654), 654: (0, 655), 655: (0, 656), 656: (0, 657), 657: (0, 658), 658: (0, 659), 659: (0, 660), 660: (0, 661), 661: (0, 662), 662: (0, 663), 663: (0, 664), 664: (0, 665), 665: (0, 666), 666: (0, 667), 667: (0, 668), 668: (0, 669), 669: (0, 670), 670: (0, 671), 671: (0, 672), 672: (0, 673), 673: (0, 674), 674: (0, 675), 675: (0, 676), 676: (0, 677), 677: (0, 678), 678: (0, 679), 679: (0, 680), 680: (0, 681), 681: (0, 682), 682: (0, 683), 683: (0, 684), 684: (0, 685), 685: (0, 686), 686: (0, 687), 687: (0, 688), 688: (0, 689), 689: (0, 690), 690: (0, 691), 691: (0, 692), 692: (0, 693), 693: (0, 694), 694: (0, 695), 695: (0, 696), 696: (0, 697), 697: (0, 698), 698: (0, 699), 699: (0, 700), 700: (0, 701), 701: (0, 702), 702: (0, 703), 703: (0, 704), 704: (0, 705), 705: (0, 706), 706: (0, 707), 707: (0, 708), 708: (0, 709), 709: (0, 710), 710: (0, 711), 711: (0, 712), 712: (0, 713), 713: (0, 714), 714: (0, 715), 715: (0, 716), 716: (0, 717), 717: (0, 718), 718: (0, 719), 719: (0, 720), 720: (0, 721), 721: (0, 722), 722: (0, 723), 723: (0, 724), 724: (0, 725), 725: (0, 726), 726: (0, 727), 727: (0, 728), 728: (0, 729), 729: (0, 730), 730: (0, 731), 731: (0, 732), 732: (0, 733), 733: (0, 734), 734: (0, 735), 735: (0, 736), 736: (0, 737), 737: (0, 738), 738: (0, 739), 739: (0, 740), 740: (0, 741), 741: (0, 742), 742: (0, 743), 743: (0, 744), 744: (0, 745), 745: (0, 746), 746: (0, 747), 747: (0, 748), 748: (0, 749), 749: (0, 750), 750: (0, 751), 751: (0, 752), 752: (0, 753), 753: (0, 754), 754: (0, 755), 755: (0, 756), 756: (0, 757), 757: (0, 758), 758: (0, 759), 759: (0, 760), 760: (0, 761), 761: (0, 762), 762: (0, 763), 763: (0, 764), 764: (0, 765), 765: (0, 766), 766: (0, 767), 767: (0, 768), 768: (0, 769), 769: (0, 770), 770: (0, 771), 771: (0, 772), 772: (0, 773), 773: (0, 774), 774: (0, 775), 775: (0, 776), 776: (0, 777), 777: (0, 778), 778: (0, 779), 779: (0, 780), 780: (0, 781), 781: (0, 782), 782: (0, 783), 783: (0, 784), 784: (0, 785), 785: (0, 786), 786: (0, 787), 787: (0, 788), 788: (0, 789), 789: (0, 790), 790: (0, 791), 791: (0, 792), 792: (0, 793), 793: (0, 794), 794: (0, 795), 795: (0, 796), 796: (0, 797), 797: (0, 798), 798: (0, 799), 799: (0, 800), 800: (0, 801), 801: (0, 802), 802: (0, 803), 803: (0, 804), 804: (0, 805), 805: (0, 806), 806: (0, 807), 807: (0, 808), 808: (0, 809), 809: (0, 810), 810: (0, 811), 811: (0, 812), 812: (0, 813), 813: (0, 814), 814: (0, 815), 815: (0, 816), 816: (0, 817), 817: (0, 818), 818: (0, 819), 819: (0, 820), 820: (0, 821), 821: (0, 822), 822: (0, 823), 823: (0, 824), 824: (0, 825), 825: (0, 826), 826: (0, 827), 827: (0, 828), 828: (0, 829), 829: (0, 830), 830: (0, 831), 831: (0, 832), 832: (0, 833), 833: (0, 834), 834: (0, 835), 835: (0, 836), 836: (0, 837), 837: (0, 838), 838: (0, 839), 839: (0, 840), 840: (0, 841), 841: (0, 842), 842: (0, 843), 843: (0, 844), 844: (0, 845), 845: (0, 846), 846: (0, 847), 847: (0, 848), 848: (0, 849), 849: (0, 850), 850: (0, 851), 851: (0, 852), 852: (0, 853), 853: (0, 854), 854: (0, 855), 855: (0, 856), 856: (0, 857), 857: (0, 858), 858: (0, 859), 859: (0, 860), 860: (0, 861), 861: (0, 862), 862: (0, 863), 863: (0, 864), 864: (0, 865), 865: (0, 866), 866: (0, 867), 867: (0, 868), 868: (0, 869), 869: (0, 870), 870: (0, 871), 871: (0, 872), 872: (0, 873), 873: (0, 874), 874: (0, 875), 875: (0, 876), 876: (0, 877), 877: (0, 878), 878: (0, 879), 879: (0, 880), 880: (0, 881), 881: (0, 882), 882: (0, 883), 883: (0, 884), 884: (0, 885), 885: (0, 886), 886: (0, 887), 887: (0, 888), 888: (0, 889), 889: (0, 890), 890: (0, 891), 891: (0, 892), 892: (0, 893), 893: (0, 894), 894: (0, 895), 895: (0, 896), 896: (0, 897), 897: (0, 898), 898: (0, 899), 899: (0, 900), 900: (0, 901), 901: (0, 902), 902: (0, 903), 903: (0, 904), 904: (0, 905), 905: (0, 906), 906: (0, 907), 907: (0, 908), 908: (0, 909), 909: (0, 910), 910: (0, 911), 911: (0, 912), 912: (0, 913), 913: (0, 914), 914: (0, 915), 915: (0, 916), 916: (0, 917), 917: (0, 918), 918: (0, 919), 919: (0, 920), 920: (0, 921), 921: (0, 922), 922: (0, 923), 923: (0, 924), 924: (0, 925), 925: (0, 926), 926: (0, 927), 927: (0, 928), 928: (0, 929), 929: (0, 930), 930: (0, 931), 931: (0, 932), 932: (0, 933), 933: (0, 934)}\n",
      "Generated self._MapperTrialcode2TrialToTrial!\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "Extracted successfully for session:  1\n",
      "Generated index mappers!\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/DfScalar.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/fr_sm_times.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/DS.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/Params.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/ParamsGlobals.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/Sites.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_0/Trials.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/DfScalar.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/fr_sm_times.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/DS.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/Params.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/ParamsGlobals.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/Sites.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230615-sess_1/Trials.pkl\n",
      "This many vals across loaded session\n",
      "0 : 72779\n",
      "1 : 1182711\n",
      "tests passed\n",
      "Assigning to SP.Params this item:\n",
      "{'which_level': 'trial', '_list_events': ['fixcue', 'fix_touch', 'rulecue2', 'samp', 'go_cue', 'first_raise', 'on_strokeidx_0', 'off_stroke_last', 'doneb', 'post', 'reward_all'], 'list_events_uniqnames': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_features_extraction': [], 'list_features_get_conjunction': [], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65], 'map_var_to_othervars': None, 'strokes_only_keep_single': False, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'map_var_to_levels': None}\n",
      "tests passed\n",
      "Assigning to SP.ParamsGlobals this item:\n",
      "{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.65, 'POST_DUR_CALC': 0.65, 'list_events': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65]}\n",
      "Done!, new len of dataset 447\n",
      "Dataset preprocess, these params:\n",
      "{'DO_CHARSEQ_VER': None, 'EXTRACT_EPOCHSETS': False, 'EXTRACT_EPOCHSETS_trial_label': None, 'EXTRACT_EPOCHSETS_n_max_epochs': None, 'EXTRACT_EPOCHSETS_merge_sets': None, 'taskgroup_reassign_simple_neural': False, 'preprocess_steps_append': ['one_to_one_beh_task_strokes', 'remove_online_abort', 'beh_strokes_at_least_one'], 'remove_aborts': False, 'list_superv_keep': None, 'list_superv_keep_full': None, 'DO_SCORE_SEQUENCE_VER': None, 'list_epoch_merge': [], 'epoch_merge_key': None, 'DO_EXTRACT_EPOCHKIND': False, 'datasetstrokes_extract_to_prune_trial': 'singleprim', 'datasetstrokes_extract_to_prune_stroke': None}\n",
      "Appended columns gridsize!\n",
      "Starting length of D.Dat: 447\n",
      "--BEFORE REMOVE; existing supervision_stage_concise:\n",
      "off|0||1111|0    447\n",
      "Name: supervision_stage_concise, dtype: int64\n",
      "############ TAKING ONLY NO SUPERVISION TRIALS\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['no_supervision']\n",
      "-- Len of D, before applying this param: no_supervision, ... 447\n",
      "after: 447\n",
      "Dataset final len: 447\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['one_to_one_beh_task_strokes', 'remove_online_abort', 'beh_strokes_at_least_one']\n",
      "-- Len of D, before applying this param: one_to_one_beh_task_strokes, ... 447\n",
      "after: 441\n",
      "-- Len of D, before applying this param: remove_online_abort, ... 441\n",
      "after: 422\n",
      "-- Len of D, before applying this param: beh_strokes_at_least_one, ... 422\n",
      "after: 422\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['beh_strokes_at_least_one', 'one_to_one_beh_task_strokes_allow_unfinished', 'no_supervision', 'remove_online_abort']\n",
      "-- Len of D, before applying this param: beh_strokes_at_least_one, ... 422\n",
      "after: 422\n",
      "-- Len of D, before applying this param: one_to_one_beh_task_strokes_allow_unfinished, ... 422\n",
      "after: 422\n",
      "-- Len of D, before applying this param: no_supervision, ... 422\n",
      "after: 422\n",
      "-- Len of D, before applying this param: remove_online_abort, ... 422\n",
      "after: 422\n",
      "-- Len of D, before applying this param: frac_touched_min, ... 422\n",
      "after: 422\n",
      "-- Len of D, before applying this param: ft_decim_min, ... 422\n",
      "after: 422\n",
      "-- Len of D, before applying this param: shortness_min, ... 422\n",
      "after: 422\n",
      "Removing these trials: \n",
      "[]\n",
      "self.Dat starting legnth:  422\n",
      "Modified self.Dat, keeping only the inputted inds\n",
      "self.Dat final legnth:  422\n",
      "Success! all gridloc identical!\n",
      "These are the x and y mappings, gridloc:loc\n",
      "x... {0: -1.6, 1: 1.7}\n",
      "y... {1: 1.7, 0: -1.6}\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "This many strokes extracted:  422\n",
      "Appended epoch to self.Dat\n",
      "Appended character to self.Dat\n",
      "DONE!\n",
      "Added column: dist_beh_task_strok\n",
      "clean_preprocess_data...\n",
      "len of DS.Dat = 422, before running... stroke_too_short\n",
      "Doing...: stroke_too_short\n",
      "New len:  422\n",
      "len of DS.Dat = 422, before running... beh_task_dist_too_large\n",
      "Doing...: beh_task_dist_too_large\n",
      "New len:  422\n",
      "len of DS.Dat = 422, before running... stroke_too_quick\n",
      "Doing...: stroke_too_quick\n",
      "New len:  422\n",
      "clean_preprocess_data...\n",
      "len of DS.Dat = 422, before running... remove_if_multiple_behstrokes_per_taskstroke\n",
      "This many cases with >1 beh stroke needed to completed a task stroke:  0\n",
      "New len:  422\n",
      "Num nan/total, for angle_overall\n",
      "422 / 422\n",
      "Num nan/total, for num_strokes_beh\n",
      "0 / 422\n",
      "Num nan/total, for num_strokes_task\n",
      "0 / 422\n",
      "Num nan/total, for circ\n",
      "0 / 422\n",
      "Num nan/total, for dist\n",
      "0 / 422\n",
      "Added these features:\n",
      "['FEAT_angle_overall', 'FEAT_num_strokes_beh', 'FEAT_num_strokes_task', 'FEAT_circ', 'FEAT_dist']\n",
      ".. Appended new column 'char_seq', version: task_matlab\n",
      "Starting len dfscalar:  1165220\n",
      "Ending len dfscalar:  1108625\n",
      "Appending...  seqc_0_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_shape\n",
      "Appending...  seqc_0_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_loc\n",
      "Appending...  aborted\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "aborted\n",
      "Appending...  task_kind\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "task_kind\n",
      "Appending...  gridsize\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "gridsize\n",
      "Appending...  FEAT_num_strokes_task\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "FEAT_num_strokes_task\n",
      "Appending...  FEAT_num_strokes_beh\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "FEAT_num_strokes_beh\n",
      "Appending...  character\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "character\n",
      "Appending...  probe\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "probe\n",
      "Appending...  supervision_stage_concise\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "supervision_stage_concise\n",
      "Appending...  epoch_orig\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "epoch_orig\n",
      "Appending...  epoch\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "epoch\n",
      "Appending...  taskgroup\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskgroup\n",
      "Appending...  origin\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "origin\n",
      "Appending...  donepos\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "donepos\n",
      "Appending...  seqc_1_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_shape\n",
      "Appending...  seqc_1_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_loc\n",
      "Appending...  seqc_nstrokes_beh\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_nstrokes_beh\n",
      "Appending...  seqc_nstrokes_task\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_nstrokes_task\n",
      " -- Lumping shapes (renaming) in SP.DfScalar, for:  seqc_0_shape\n",
      " -- Lumping shapes (renaming) in SP.DfScalar, for:  seqc_0_shape\n",
      "Colected 422 out of 422 datapts.\n",
      "NOTE: missed datapts are likely because of removed outliers\n",
      "03_samp M1 (0.1, 0.3)\n",
      "Sites for this bregion  M1\n",
      "[2, 7, 11, 12, 13, 14, 15, 23, 25, 32, 37, 44, 46, 48]\n",
      "03_samp  --  M1  --  (0.1, 0.3)  -- (data shape:) (14, 422, 20)\n",
      "03_samp PMv (0.1, 0.3)\n",
      "Sites for this bregion  PMv\n",
      "[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 120, 121, 123, 125, 127, 128]\n",
      "03_samp  --  PMv  --  (0.1, 0.3)  -- (data shape:) (48, 422, 20)\n",
      "03_samp PMd (0.1, 0.3)\n",
      "Sites for this bregion  PMd\n",
      "[129, 130, 131, 134, 137, 147, 149, 151, 153, 155, 168, 178, 184, 186]\n",
      "03_samp  --  PMd  --  (0.1, 0.3)  -- (data shape:) (14, 422, 20)\n",
      "03_samp dlPFC (0.1, 0.3)\n",
      "Sites for this bregion  dlPFC\n",
      "[225, 227, 229, 231, 233, 235, 237, 239, 242, 243, 245, 246, 248, 250, 252, 253]\n",
      "03_samp  --  dlPFC  --  (0.1, 0.3)  -- (data shape:) (16, 422, 20)\n",
      "03_samp vlPFC (0.1, 0.3)\n",
      "Sites for this bregion  vlPFC\n",
      "[257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 289, 294, 296, 297, 298, 300, 302, 309, 312, 313, 315, 317]\n",
      "03_samp  --  vlPFC  --  (0.1, 0.3)  -- (data shape:) (39, 422, 20)\n",
      "03_samp FP (0.1, 0.3)\n",
      "Sites for this bregion  FP\n",
      "[327, 328, 330, 331, 334, 343, 345, 347, 348, 349, 350, 351, 352, 355, 356, 358, 363, 364, 370, 373, 374, 376, 377, 379, 380, 381, 382, 384]\n",
      "03_samp  --  FP  --  (0.1, 0.3)  -- (data shape:) (28, 422, 20)\n",
      "03_samp SMA (0.1, 0.3)\n",
      "Sites for this bregion  SMA\n",
      "[385, 389, 390, 392, 394, 396, 397, 403, 405, 406, 407, 408, 411, 412, 414, 415, 417, 419, 420, 421, 422, 423, 425, 426, 427, 429, 430, 431, 432, 435, 437, 439, 440, 441, 443, 444, 445, 446, 448]\n",
      "03_samp  --  SMA  --  (0.1, 0.3)  -- (data shape:) (39, 422, 20)\n",
      "03_samp preSMA (0.1, 0.3)\n",
      "Sites for this bregion  preSMA\n",
      "[449, 451, 452, 453, 455, 456, 459, 463, 465, 466, 467, 468, 469, 471, 472, 475, 476, 477, 479, 481, 482, 483, 484, 485, 486, 488, 492, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 510, 512]\n",
      "03_samp  --  preSMA  --  (0.1, 0.3)  -- (data shape:) (42, 422, 20)\n",
      "03_samp M1 (-0.3, 0.3)\n",
      "Sites for this bregion  M1\n",
      "[2, 7, 11, 12, 13, 14, 15, 23, 25, 32, 37, 44, 46, 48]\n",
      "03_samp  --  M1  --  (-0.3, 0.3)  -- (data shape:) (14, 422, 60)\n",
      "03_samp PMv (-0.3, 0.3)\n",
      "Sites for this bregion  PMv\n",
      "[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 120, 121, 123, 125, 127, 128]\n",
      "03_samp  --  PMv  --  (-0.3, 0.3)  -- (data shape:) (48, 422, 60)\n",
      "03_samp PMd (-0.3, 0.3)\n",
      "Sites for this bregion  PMd\n",
      "[129, 130, 131, 134, 137, 147, 149, 151, 153, 155, 168, 178, 184, 186]\n",
      "03_samp  --  PMd  --  (-0.3, 0.3)  -- (data shape:) (14, 422, 60)\n",
      "03_samp dlPFC (-0.3, 0.3)\n",
      "Sites for this bregion  dlPFC\n",
      "[225, 227, 229, 231, 233, 235, 237, 239, 242, 243, 245, 246, 248, 250, 252, 253]\n",
      "03_samp  --  dlPFC  --  (-0.3, 0.3)  -- (data shape:) (16, 422, 60)\n",
      "03_samp vlPFC (-0.3, 0.3)\n",
      "Sites for this bregion  vlPFC\n",
      "[257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 289, 294, 296, 297, 298, 300, 302, 309, 312, 313, 315, 317]\n",
      "03_samp  --  vlPFC  --  (-0.3, 0.3)  -- (data shape:) (39, 422, 60)\n",
      "03_samp FP (-0.3, 0.3)\n",
      "Sites for this bregion  FP\n",
      "[327, 328, 330, 331, 334, 343, 345, 347, 348, 349, 350, 351, 352, 355, 356, 358, 363, 364, 370, 373, 374, 376, 377, 379, 380, 381, 382, 384]\n",
      "03_samp  --  FP  --  (-0.3, 0.3)  -- (data shape:) (28, 422, 60)\n",
      "03_samp SMA (-0.3, 0.3)\n",
      "Sites for this bregion  SMA\n",
      "[385, 389, 390, 392, 394, 396, 397, 403, 405, 406, 407, 408, 411, 412, 414, 415, 417, 419, 420, 421, 422, 423, 425, 426, 427, 429, 430, 431, 432, 435, 437, 439, 440, 441, 443, 444, 445, 446, 448]\n",
      "03_samp  --  SMA  --  (-0.3, 0.3)  -- (data shape:) (39, 422, 60)\n",
      "03_samp preSMA (-0.3, 0.3)\n",
      "Sites for this bregion  preSMA\n",
      "[449, 451, 452, 453, 455, 456, 459, 463, 465, 466, 467, 468, 469, 471, 472, 475, 476, 477, 479, 481, 482, 483, 484, 485, 486, 488, 492, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 510, 512]\n",
      "03_samp  --  preSMA  --  (-0.3, 0.3)  -- (data shape:) (42, 422, 60)\n",
      "Colected 422 out of 422 datapts.\n",
      "NOTE: missed datapts are likely because of removed outliers\n",
      "06_on_strokeidx_0 M1 (0.1, 0.3)\n",
      "Sites for this bregion  M1\n",
      "[2, 7, 11, 12, 13, 14, 15, 23, 25, 32, 37, 44, 46, 48]\n",
      "06_on_strokeidx_0  --  M1  --  (0.1, 0.3)  -- (data shape:) (14, 422, 20)\n",
      "06_on_strokeidx_0 PMv (0.1, 0.3)\n",
      "Sites for this bregion  PMv\n",
      "[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 120, 121, 123, 125, 127, 128]\n",
      "06_on_strokeidx_0  --  PMv  --  (0.1, 0.3)  -- (data shape:) (48, 422, 20)\n",
      "06_on_strokeidx_0 PMd (0.1, 0.3)\n",
      "Sites for this bregion  PMd\n",
      "[129, 130, 131, 134, 137, 147, 149, 151, 153, 155, 168, 178, 184, 186]\n",
      "06_on_strokeidx_0  --  PMd  --  (0.1, 0.3)  -- (data shape:) (14, 422, 20)\n",
      "06_on_strokeidx_0 dlPFC (0.1, 0.3)\n",
      "Sites for this bregion  dlPFC\n",
      "[225, 227, 229, 231, 233, 235, 237, 239, 242, 243, 245, 246, 248, 250, 252, 253]\n",
      "06_on_strokeidx_0  --  dlPFC  --  (0.1, 0.3)  -- (data shape:) (16, 422, 20)\n",
      "06_on_strokeidx_0 vlPFC (0.1, 0.3)\n",
      "Sites for this bregion  vlPFC\n",
      "[257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 289, 294, 296, 297, 298, 300, 302, 309, 312, 313, 315, 317]\n",
      "06_on_strokeidx_0  --  vlPFC  --  (0.1, 0.3)  -- (data shape:) (39, 422, 20)\n",
      "06_on_strokeidx_0 FP (0.1, 0.3)\n",
      "Sites for this bregion  FP\n",
      "[327, 328, 330, 331, 334, 343, 345, 347, 348, 349, 350, 351, 352, 355, 356, 358, 363, 364, 370, 373, 374, 376, 377, 379, 380, 381, 382, 384]\n",
      "06_on_strokeidx_0  --  FP  --  (0.1, 0.3)  -- (data shape:) (28, 422, 20)\n",
      "06_on_strokeidx_0 SMA (0.1, 0.3)\n",
      "Sites for this bregion  SMA\n",
      "[385, 389, 390, 392, 394, 396, 397, 403, 405, 406, 407, 408, 411, 412, 414, 415, 417, 419, 420, 421, 422, 423, 425, 426, 427, 429, 430, 431, 432, 435, 437, 439, 440, 441, 443, 444, 445, 446, 448]\n",
      "06_on_strokeidx_0  --  SMA  --  (0.1, 0.3)  -- (data shape:) (39, 422, 20)\n",
      "06_on_strokeidx_0 preSMA (0.1, 0.3)\n",
      "Sites for this bregion  preSMA\n",
      "[449, 451, 452, 453, 455, 456, 459, 463, 465, 466, 467, 468, 469, 471, 472, 475, 476, 477, 479, 481, 482, 483, 484, 485, 486, 488, 492, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 510, 512]\n",
      "06_on_strokeidx_0  --  preSMA  --  (0.1, 0.3)  -- (data shape:) (42, 422, 20)\n",
      "06_on_strokeidx_0 M1 (-0.3, 0.3)\n",
      "Sites for this bregion  M1\n",
      "[2, 7, 11, 12, 13, 14, 15, 23, 25, 32, 37, 44, 46, 48]\n",
      "06_on_strokeidx_0  --  M1  --  (-0.3, 0.3)  -- (data shape:) (14, 422, 60)\n",
      "06_on_strokeidx_0 PMv (-0.3, 0.3)\n",
      "Sites for this bregion  PMv\n",
      "[66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 83, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 120, 121, 123, 125, 127, 128]\n",
      "06_on_strokeidx_0  --  PMv  --  (-0.3, 0.3)  -- (data shape:) (48, 422, 60)\n",
      "06_on_strokeidx_0 PMd (-0.3, 0.3)\n",
      "Sites for this bregion  PMd\n",
      "[129, 130, 131, 134, 137, 147, 149, 151, 153, 155, 168, 178, 184, 186]\n",
      "06_on_strokeidx_0  --  PMd  --  (-0.3, 0.3)  -- (data shape:) (14, 422, 60)\n",
      "06_on_strokeidx_0 dlPFC (-0.3, 0.3)\n",
      "Sites for this bregion  dlPFC\n",
      "[225, 227, 229, 231, 233, 235, 237, 239, 242, 243, 245, 246, 248, 250, 252, 253]\n",
      "06_on_strokeidx_0  --  dlPFC  --  (-0.3, 0.3)  -- (data shape:) (16, 422, 60)\n",
      "06_on_strokeidx_0 vlPFC (-0.3, 0.3)\n",
      "Sites for this bregion  vlPFC\n",
      "[257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 274, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 289, 294, 296, 297, 298, 300, 302, 309, 312, 313, 315, 317]\n",
      "06_on_strokeidx_0  --  vlPFC  --  (-0.3, 0.3)  -- (data shape:) (39, 422, 60)\n",
      "06_on_strokeidx_0 FP (-0.3, 0.3)\n",
      "Sites for this bregion  FP\n",
      "[327, 328, 330, 331, 334, 343, 345, 347, 348, 349, 350, 351, 352, 355, 356, 358, 363, 364, 370, 373, 374, 376, 377, 379, 380, 381, 382, 384]\n",
      "06_on_strokeidx_0  --  FP  --  (-0.3, 0.3)  -- (data shape:) (28, 422, 60)\n",
      "06_on_strokeidx_0 SMA (-0.3, 0.3)\n",
      "Sites for this bregion  SMA\n",
      "[385, 389, 390, 392, 394, 396, 397, 403, 405, 406, 407, 408, 411, 412, 414, 415, 417, 419, 420, 421, 422, 423, 425, 426, 427, 429, 430, 431, 432, 435, 437, 439, 440, 441, 443, 444, 445, 446, 448]\n",
      "06_on_strokeidx_0  --  SMA  --  (-0.3, 0.3)  -- (data shape:) (39, 422, 60)\n",
      "06_on_strokeidx_0 preSMA (-0.3, 0.3)\n",
      "Sites for this bregion  preSMA\n",
      "[449, 451, 452, 453, 455, 456, 459, 463, 465, 466, 467, 468, 469, 471, 472, 475, 476, 477, 479, 481, 482, 483, 484, 485, 486, 488, 492, 494, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 510, 512]\n",
      "06_on_strokeidx_0  --  preSMA  --  (-0.3, 0.3)  -- (data shape:) (42, 422, 60)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                bin_by_time_dur = bin_by_time_dur, \n",
    "                                bin_by_time_slide = bin_by_time_slide)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "653547aa7482c726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_statespace_2d_overlaying_all_othervar\n",
    "keep_all_margs = True\n",
    "PLOT = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d431ce30f60c726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    effect_vars = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "    q_params[\"effect_vars\"] = effect_vars\n",
    "else:\n",
    "    effect_vars = q_params[\"effect_vars\"]\n",
    "        \n",
    "\n",
    "##### savedir\n",
    "from pythonlib.tools.listtools import stringify_list\n",
    "a = stringify_list(list_time_windows, return_as_str=True)\n",
    "b = stringify_list(events_keep, return_as_str=True)\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSES}/{animal}-{date}/{question}/{a}--{b}--{keep_all_margs}\"\n",
    "\n",
    "RES = []\n",
    "for i, row in DFallpa.iterrows():\n",
    "\n",
    "    pa = row[\"pa\"]\n",
    "    br = row[\"bregion\"]\n",
    "    wl = row[\"which_level\"]\n",
    "    ev = row[\"event\"]\n",
    "    tw = row[\"twind\"]\n",
    "\n",
    "    savedir = f\"{SAVEDIR}/each_pa/{wl}-{ev}-{br}-{tw}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    print(\" *** Saving to:\", savedir)\n",
    "\n",
    "    # Clean up PA\n",
    "    from neuralmonkey.analyses.rsa import preprocess_rsa_prepare_popanal_wrapper\n",
    "    pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "\n",
    "    ########### SKIPPING ANALYSIS AND EXITING!!\n",
    "    from pythonlib.tools.expttools import writeDictToTxt, writeDictToYaml\n",
    "    path = f\"{savedir}/res_check_effectvars.txt\"\n",
    "    writeDictToTxt(res_check_effectvars, path)\n",
    "    path = f\"{savedir}/res_check_tasksets.txt\"\n",
    "    writeDictToTxt(res_check_tasksets, path)\n",
    "\n",
    "    # Preprocess\n",
    "    R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca = preprocess_pa_to_frtensor(pa, effect_vars, keep_all_margs=keep_all_margs)\n",
    "\n",
    "    params_dpca[\"data_shape-trial_N_features_time\"] = trialR.shape\n",
    "    writeDictToYaml(params_dpca, f\"{savedir}/params.yaml\")\n",
    "\n",
    "    print(\"---- OUTPUT\")\n",
    "    print(R.shape)\n",
    "    print(trialR.shape)\n",
    "    print(map_var_to_lev)\n",
    "    print(map_grp_to_idx)\n",
    "    print(params_dpca)\n",
    "\n",
    "    #### Fit model\n",
    "    from dPCA import dPCA\n",
    "\n",
    "    # We then instantiate a dPCA model where the two parameter axis are labeled by 's' (stimulus) and 't' (time) respectively. We set regularizer to 'auto' to optimize the regularization parameter when we fit the data.\n",
    "    labels = params_dpca[\"labels\"]\n",
    "    join = params_dpca[\"join\"]\n",
    "    n_components = params_dpca[\"n_components\"]\n",
    "    dpca = dPCA.dPCA(labels=labels, regularizer='auto', join=join, n_components=n_components)\n",
    "    dpca.protect = ['t']\n",
    "\n",
    "    # Now fit the data (R) using the model we just instatiated. Note that we only need trial-to-trial data when we want to optimize over the regularization parameter.\n",
    "    Z = dpca.fit_transform(R,trialR)\n",
    "\n",
    "    if PLOT:\n",
    "        plot_all_results_single(dpca, Z, effect_vars, params_dpca, savedir)\n",
    "\n",
    "    RES.append({\n",
    "        \"bregion\":br,\n",
    "        \"which_level\":wl,\n",
    "        \"event\":ev,\n",
    "        \"twind\":tw,\n",
    "        \"explained_var\":dpca.explained_variance_ratio_,\n",
    "        \"marginalizations\":list(dpca.marginalizations.keys()),\n",
    "        \"params_dpca\":params_dpca,\n",
    "        \"map_var_to_lev\":map_var_to_lev,\n",
    "        \"map_grp_to_idx\":map_grp_to_idx,\n",
    "    })\n",
    "    \n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_mult\n",
    "DFRES = pd.DataFrame(RES)\n",
    "plot_all_results_mult(DFRES, SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc46cc695323b096"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare two datasets train vs. test (each across all areas)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d5b84e60c76582"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "import sys\n",
    "\n",
    "SAVEDIR_ANALYSES = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/dPCA\"\n",
    "\n",
    "############### PARAMS\n",
    "animal = \"Diego\"\n",
    "date = 230615\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "bin_by_time_dur = 0.05\n",
    "bin_by_time_slide = 0.025\n",
    "\n",
    "if True:\n",
    "    # Train/test using different time windows. The PA related stuff is same as standard approach.\n",
    "    question = \"SP_shape_loc\"\n",
    "    slice_agg_slices = None\n",
    "    slice_agg_vars_to_split = None\n",
    "    slice_agg_concat_dim = None\n",
    "    \n",
    "    # list_time_windows = [(0.2, 0.6), (-0.3, 0.3)]\n",
    "    list_time_windows = [(0.1, 0.3), (-0.3, 0.3)] # Immediate visual response\n",
    "    events_keep = [\"03_samp\", \"06_on_strokeidx_0\"]\n",
    "    print(list_time_windows)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac314ddd08abdb52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                bin_by_time_dur = bin_by_time_dur, \n",
    "                                bin_by_time_slide = bin_by_time_slide)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ee07806bedea051"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "effect_vars = q_params[\"effect_vars\"]\n",
    "print(\"Effect vars:\", effect_vars)\n",
    "keep_all_margs = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dc3854756b47daa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_single\n",
    "list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "\n",
    "for keep_all_margs in [True, False]:\n",
    "    wl = \"trial\"\n",
    "    ev_train = events_keep[0]\n",
    "    tw_train = list_time_windows[0]\n",
    "    \n",
    "    ev_test = events_keep[1]\n",
    "    tw_test = list_time_windows[1]\n",
    "    \n",
    "    from pythonlib.tools.expttools import writeDictToTxt\n",
    "    SAVEDIR = f\"{SAVEDIR_ANALYSES}/{animal}-{date}/{question}/train_test_diff_evtw/TRAIN-{ev_train}-{tw_train}-TEST-{ev_test}-{tw_test}-keepallmarg_{keep_all_margs}\"\n",
    "    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "    params = {}\n",
    "    params[\"wl\"] = wl\n",
    "    params[\"ev_train\"] = ev_train\n",
    "    params[\"tw_train\"] = tw_train\n",
    "    params[\"ev_test\"] = ev_test\n",
    "    params[\"tw_test\"] = tw_test\n",
    "    writeDictToTxt(params, f\"{SAVEDIR}/params.txt\")\n",
    "    \n",
    "    ## Fit PCs and evaluate test data.\n",
    "    RES_TRAIN = []\n",
    "    RES_TEST = []\n",
    "    for br in list_br:\n",
    "        a = DFallpa[\"event\"] == ev_train\n",
    "        b = DFallpa[\"twind\"] == tw_train\n",
    "        c = DFallpa[\"bregion\"] == br\n",
    "        \n",
    "        tmp =DFallpa[(a & b & c)]\n",
    "        assert len(tmp)==1\n",
    "        pa_train = tmp[\"pa\"].values[0]\n",
    "    \n",
    "        a = DFallpa[\"event\"] == ev_test\n",
    "        b = DFallpa[\"twind\"] == tw_test\n",
    "        tmp =DFallpa[(a & b & c)]\n",
    "        assert len(tmp)==1\n",
    "        pa_test = tmp[\"pa\"].values[0]\n",
    "    \n",
    "        # Train\n",
    "        from dPCA import dPCA\n",
    "        R, trialR, map_var_to_lev, map_grp_to_idx, params_dpca = preprocess_pa_to_frtensor(pa_train, effect_vars, keep_all_margs=keep_all_margs)\n",
    "        \n",
    "        labels = params_dpca[\"labels\"]\n",
    "        join = params_dpca[\"join\"]\n",
    "        n_components = params_dpca[\"n_components\"]\n",
    "        dpca = dPCA.dPCA(labels=labels, regularizer='auto', join=join, n_components=n_components)\n",
    "        dpca.protect = ['t']\n",
    "        \n",
    "        # Now fit the data (R) using the model we just instatiated. Note that we only need trial-to-trial data when we want to optimize over the regularization parameter.\n",
    "        Z = dpca.fit_transform(R,trialR)\n",
    "            \n",
    "        ############ PLOT TRAINING DATA\n",
    "        savedir = f\"{SAVEDIR}/each_bregion/{br}/figures_summary_TRAIN\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        # Test, by projecting new data onto this space\n",
    "        R, trialR, _, _, params_dpca = preprocess_pa_to_frtensor(pa_train, effect_vars, keep_all_margs=keep_all_margs)\n",
    "        Ztrain = dpca.transform(R)\n",
    "        plot_all_results_single(dpca, Ztrain, effect_vars, params_dpca, savedir)    \n",
    "        plt.close(\"all\")\n",
    "    \n",
    "        RES_TRAIN.append({\n",
    "            \"bregion\":br,\n",
    "            \"which_level\":wl,\n",
    "            \"event\":ev_train,\n",
    "            \"twind\":tw_train,\n",
    "            \"explained_var\":dpca.explained_variance_ratio_,\n",
    "            \"marginalizations\":list(dpca.marginalizations.keys()),\n",
    "            \"params_dpca\":params_dpca,\n",
    "            \"map_var_to_lev\":map_var_to_lev,\n",
    "            \"map_grp_to_idx\":map_grp_to_idx,\n",
    "            \"Z\":Ztrain\n",
    "        })\n",
    "    \n",
    "        ############ PLOT TESTING DATA\n",
    "        savedir = f\"{SAVEDIR}/each_bregion/{br}/figures_summary_TEST\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        # Test, by projecting new data onto this space\n",
    "        R, trialR, _, _, params_dpca = preprocess_pa_to_frtensor(pa_test, effect_vars, keep_all_margs=keep_all_margs)\n",
    "        Ztest = dpca.transform(R)\n",
    "        plot_all_results_single(dpca, Ztest, effect_vars, params_dpca, savedir)    \n",
    "        plt.close(\"all\")\n",
    "                \n",
    "        RES_TEST.append({\n",
    "            \"bregion\":br,\n",
    "            \"which_level\":wl,\n",
    "            \"event\":ev_test,\n",
    "            \"twind\":tw_test,\n",
    "            \"explained_var\":dpca.explained_variance_ratio_,\n",
    "            \"marginalizations\":list(dpca.marginalizations.keys()),\n",
    "            \"params_dpca\":params_dpca,\n",
    "            \"map_var_to_lev\":map_var_to_lev,\n",
    "            \"map_grp_to_idx\":map_grp_to_idx,\n",
    "            \"Z\":Ztest\n",
    "        })\n",
    "        \n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        #### PLOT SUMMARY for test and train.\n",
    "        from neuralmonkey.scripts.analy_dpca_script_quick import plot_all_results_mult\n",
    "        \n",
    "        DFRES_TRAIN = pd.DataFrame(RES_TRAIN)\n",
    "        savedir = f\"{SAVEDIR}/train\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        plot_all_results_mult(DFRES_TRAIN, savedir)\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        DFRES_TEST = pd.DataFrame(RES_TEST)\n",
    "        savedir = f\"{SAVEDIR}/test\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        plot_all_results_mult(DFRES_TEST, savedir)\n",
    "        plt.close(\"all\")\n",
    "                \n",
    "                \n",
    "    ##### SUMMARY: Compare similarity of activity of test data, projected onto old subspace\n",
    "    # PARAMS\n",
    "    # - Time bins, indices into z[:,:, :, idx]\n",
    "    tbin_train = -1\n",
    "    tbin_test = 0\n",
    "    version_distance = \"_pearson_raw\"\n",
    "    PLOT_EACH_HEATMAP=False\n",
    "    \n",
    "    # RUN\n",
    "    savedir = f\"{SAVEDIR}/RSA\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    from neuralmonkey.scripts.analy_dpca_script_quick import plothelper_get_variables\n",
    "    \n",
    "    ts, margs, map_var_to_idx, map_margstr_to_var, map_var_lev_to_pcol, map_var_to_lev = plothelper_get_variables(Z, effect_vars, params_dpca)\n",
    "    \n",
    "    # RSA analysis.\n",
    "    map_grp_to_idx = params_dpca[\"map_grp_to_idx\"]\n",
    "    map_var_to_idx = params_dpca[\"map_var_to_idx\"]\n",
    "    margs = params_dpca[\"margs\"]\n",
    "    map_traintest_to_Z = {\n",
    "        \"train\":Ztrain,\n",
    "        \"test\":Ztest\n",
    "    }\n",
    "    SCORES = []\n",
    "    for marg in margs:\n",
    "        map_bregion_Ztrain = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TRAIN.iterrows()}\n",
    "        map_bregion_Ztest = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TEST.iterrows()} \n",
    "        \n",
    "        list_br = list(map_bregion_Ztrain.keys())\n",
    "        for br in list_br:    \n",
    "            ######## Collect z across all variables, and train/test.\n",
    "            tmp = []\n",
    "            for train_test in [\"train\", \"test\"]:\n",
    "                if train_test==\"train\":\n",
    "                    tbin = tbin_train\n",
    "                    Zthis = map_bregion_Ztrain[br]\n",
    "                elif train_test==\"test\":\n",
    "                    tbin = tbin_test                \n",
    "                    Zthis = map_bregion_Ztest[br]\n",
    "                else:\n",
    "                    assert False\n",
    "                    \n",
    "                # Collect data across effect vars\n",
    "                for grp, idx in map_grp_to_idx.items():\n",
    "                    dat = {}\n",
    "                    dat[\"bregion\"] = br\n",
    "                    for var, i in map_var_to_idx.items():\n",
    "                        dat[var] = grp[i]    \n",
    "                    dat[\"train_test\"] = train_test\n",
    "                    dat[\"z\"] = Zthis[:, idx[0], idx[1], tbin]\n",
    "                    tmp.append(dat)        \n",
    "            dftmp = pd.DataFrame(tmp)\n",
    "        \n",
    "        \n",
    "            ########### SCORE AND PLOT\n",
    "            # Convert to Cluster\n",
    "            from pythonlib.cluster.clustclass import Clusters\n",
    "            X = np.stack(dftmp[\"z\"]) # (ndat, ndims)\n",
    "            label_vars = effect_vars + [\"train_test\"]\n",
    "            labels = dftmp.loc[:, label_vars].values.tolist()\n",
    "            labels = [tuple(l) for l in labels]\n",
    "            labels_cols = list(range(X.shape[1]))\n",
    "            Clraw = Clusters(X, labels, labels_cols, \"rsa\", {\"label_vars\":label_vars})\n",
    "            Clsim = Clraw.distsimmat_convert(version_distance)\n",
    "                \n",
    "            if PLOT_EACH_HEATMAP:\n",
    "                # plot \n",
    "                fig, ax = Clraw.rsa_plot_heatmap(sort_order=(0,1,2), diverge=True)\n",
    "                savefig(fig, f\"{savedir}/rawheat-marg_{marg}-{br}.pdf\")\n",
    "                \n",
    "                fig, ax = Clsim.rsa_plot_heatmap(sort_order=(0,1,2))\n",
    "                savefig(fig, f\"{savedir}/rawdiff_pearson-marg_{marg}-{br}.pdf\")\n",
    "            \n",
    "            # compute score\n",
    "            res = []\n",
    "            for var_effect in effect_vars:\n",
    "                vars_all = label_vars\n",
    "                var_other = [v for v in effect_vars if not v==var_effect][0]\n",
    "                vars_test_invariance_over_dict = {\n",
    "                    \"same\":[var_other],\n",
    "                    \"diff\":[\"train_test\"]\n",
    "                }\n",
    "                out = Clsim.rsa_distmat_score_same_diff(var_effect, vars_all, vars_test_invariance_over_dict, PLOT=False)\n",
    "                \n",
    "                SCORES.append({\n",
    "                    \"EffS_CtxS\":out[\"EffS_CtxS\"],\n",
    "                    \"EffD_CtxS\":out[\"EffD_CtxS\"],\n",
    "                    \"EffS_CtxD\":out[\"EffS_CtxD\"],\n",
    "                    \"EffD_CtxD\":out[\"EffD_CtxD\"],\n",
    "                    \"bregion\":br,\n",
    "                    \"var_effect\":var_effect,                \n",
    "                    \"var_other\":var_other,\n",
    "                    \"vars_all\":vars_all,\n",
    "                    \"marginalization\":marg,\n",
    "                })\n",
    "                \n",
    "    ######### PLOT\n",
    "    dfscores = pd.DataFrame(SCORES)\n",
    "    \n",
    "    list_marg = dfscores[\"marginalization\"].unique()\n",
    "    import seaborn as sns\n",
    "    \n",
    "    ####\n",
    "    for yvar in [\n",
    "        \"EffS_CtxS\",\n",
    "        \"EffD_CtxS\",\n",
    "        \"EffS_CtxD\",\n",
    "        \"EffD_CtxD\"]:\n",
    "        \n",
    "        ncols =2\n",
    "        nrows = int(np.ceil(len(list_marg)/ncols))\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(12,nrows*5), sharey=True)\n",
    "        \n",
    "        for marg, ax in zip(list_marg, axes.flatten()):\n",
    "            dfthis = dfscores[dfscores[\"marginalization\"]==marg]\n",
    "            sns.barplot(data=dfthis, x=\"bregion\", y=yvar, ax=ax, hue=\"var_effect\")\n",
    "            ax.set_title(marg)\n",
    "        \n",
    "        print(\"Plotting for:\", yvar)\n",
    "        savefig(fig, f\"{savedir}/corr-test_vs_train-projected_on_train_space-{version_distance}-yvar_{yvar}.pdf\")  \n",
    "        plt.close(\"all\")\n",
    "    \n",
    "    #### Plot summary of scores.   \n",
    "    dfscores[\"score_CtxD\"] = dfscores[\"EffS_CtxD\"] - dfscores[\"EffD_CtxD\"]\n",
    "    fig = sns.catplot(data=dfscores, x=\"bregion\", y=\"score_CtxD\", hue=\"var_effect\", col=\"marginalization\", col_wrap=2, kind=\"bar\")\n",
    "    savefig(fig, f\"{savedir}/score_EffS_CtxD_minus_EffD_CtxD.pdf\")\n",
    "    \n",
    "    # --\n",
    "    from pythonlib.tools.pandastools import unpivot\n",
    "    value_vars = [\n",
    "                \"EffS_CtxS\",\n",
    "                \"EffD_CtxS\",\n",
    "                \"EffS_CtxD\",\n",
    "                \"EffD_CtxD\"]\n",
    "    \n",
    "    dfscores_long = unpivot(dfscores, [\"bregion\", \"var_effect\", \"var_other\", \"marginalization\", \"vars_all\"], value_vars, \"score_kind\", \"score\")\n",
    "    dfscores_long\n",
    "    score_kinds_keep = [\"EffD_CtxS\", \"EffS_CtxD\", \"EffD_CtxD\"]\n",
    "    dfscores_long = dfscores_long[dfscores_long[\"score_kind\"].isin(score_kinds_keep)].reset_index(drop=True)\n",
    "    fig = sns.catplot(data=dfscores_long, x=\"bregion\", y=\"score\", hue=\"score_kind\", col=\"var_effect\", row=\"marginalization\", kind=\"bar\")\n",
    "    savefig(fig, f\"{savedir}/all_effects.pdf\")\n",
    "    plt.close(\"all\")\n",
    "    \n",
    "    \n",
    "##### A single plot overlaying train and test, where one time bin for train is concatted to one for test.\n",
    "# [OLDER, beducase this is difficult to compare represntational structure of train vs. for test.\n",
    "\n",
    "# Construct a new fake Z, which makes Ztrain and Ztest be the two \"timepoints\"\n",
    "def _concat_ztrain_ztest(ztrain, ztest, tbin_train, tbin_test):\n",
    "    \"\"\" Given two matrics of (a,b,c,n) and (a,b,c,m), takes one \n",
    "    time slice each, to output (a,b,c,2)\n",
    "    \"\"\"\n",
    "    # tbin_train = -1\n",
    "    # tbin_test = 0\n",
    "    zfake = np.stack((ztrain[:,:,:,tbin_train], ztest[:,:,:,tbin_test]), axis=3)\n",
    "    return zfake\n",
    "\n",
    "savedir = f\"{SAVEDIR}/state_space_combined\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "# Note which time bins\n",
    "from pythonlib.tools.expttools import writeDictToTxt\n",
    "prms = {\n",
    "    \"tbin_train\":tbin_train,\n",
    "    \"tbin_test\":tbin_test,\n",
    "}\n",
    "writeDictToTxt(prms, f\"{savedir}/prms_tbins.txt\")\n",
    "\n",
    "SAME_AXES_ACROSS_MARGS = False\n",
    "\n",
    "for marg in margs:\n",
    "    map_bregion_Ztrain = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TRAIN.iterrows()}\n",
    "    map_bregion_Ztest = {row[\"bregion\"]:row[\"Z\"][marg] for _, row in DFRES_TEST.iterrows()}     \n",
    "    for br in list_br:\n",
    "        ztrain = map_bregion_Ztrain[br]\n",
    "        ztest = map_bregion_Ztest[br]\n",
    "        var = map_margstr_to_var[marg]\n",
    "        \n",
    "        # get combined Z\n",
    "        Z = _concat_ztrain_ztest(ztrain, ztest, tbin_train, tbin_test)\n",
    "        Z = {marg:Z} # HACK\n",
    "        \n",
    "        # indvar_to_color_by = 0\n",
    "        for var_color_by in effect_vars:\n",
    "            if SAME_AXES_ACROSS_MARGS:\n",
    "                val_minmax = [zmin, zmax]\n",
    "            else:\n",
    "                val_minmax = None\n",
    "            fig, axes = plot_statespace_2d_overlaying_all_othervar(Z, marg, var_color_by, params_dpca, val_minmax, return_axes=True)\n",
    "            # for ax in axes.flatten():\n",
    "            #     ax.set_title()\n",
    "\n",
    "            savefig(fig, f\"{savedir}/2D-marg_{marg}-br_{br}-color_by_{var_color_by}-share_axes_{SAME_AXES_ACROSS_MARGS}.pdf\")\n",
    "            plt.close(\"all\")\n",
    "    \n",
    "\n",
    "#### BETTER PLOTS, seaprate plots for train and test.\n",
    "# First, collect all trajectories into dataframe, split up where each row is a single condition, with z shape (ndim, ntrials, ntimes), where ntrials is 1\n",
    "\n",
    "res = []\n",
    "for train_test in [\"train\", \"test\"]:\n",
    "    if train_test==\"train\":\n",
    "        DFRES_THIS = DFRES_TRAIN\n",
    "    elif train_test==\"test\":\n",
    "        DFRES_THIS = DFRES_TEST\n",
    "    else:\n",
    "        assert False\n",
    "    for _, row in DFRES_THIS.iterrows():\n",
    "        for marg, Zthis in row[\"Z\"].items():\n",
    "            # Collect data across effect vars\n",
    "            for grp, idx in map_grp_to_idx.items():    \n",
    "                z = Zthis[:, idx[0], idx[1], :] # (ndim, ntime)\n",
    "                z = z.reshape(z.shape[0], 1, z.shape[1])\n",
    "                \n",
    "                # Collect data\n",
    "                dat = {}\n",
    "                dat[\"train_test\"] = train_test\n",
    "                dat[\"marg\"] = marg\n",
    "                for var, i in map_var_to_idx.items():\n",
    "                    dat[var] = grp[i]    \n",
    "                dat[\"bregion\"] = row[\"bregion\"]\n",
    "                dat[\"which_level\"] = row[\"which_level\"]\n",
    "                dat[\"event\"] = row[\"event\"]\n",
    "                dat[\"twind\"] = row[\"twind\"]\n",
    "                dat[\"explained_var\"] = row[\"explained_var\"]\n",
    "                dat[\"params_dpca\"] = row[\"params_dpca\"]\n",
    "                dat[\"times\"] = row[\"params_dpca\"][\"times\"]\n",
    "                dat[\"z\"] = z\n",
    "                res.append(dat)               \n",
    "    \n",
    "DFRES_TRAINTEST = pd.DataFrame(res)\n",
    "\n",
    "### GOOD PLOT separately train vs. test, each doing both scalar and traj,\n",
    "# plotted on same space.\n",
    "list_tbin_test = [0, -1], # each does separate set of plots...\n",
    "### PLot params\n",
    "times_to_mark = [0]\n",
    "times_to_mark_markers = [\"d\"]\n",
    "# text_plot_pt1 = \"on\"\n",
    "text_plot_pt1 = None\n",
    "markersize=7\n",
    "marker=\"P\"\n",
    "time_bin_size = 0.05\n",
    "\n",
    "for tbin_test in list_tbin_test:\n",
    "\n",
    "    # Taske a single scalar time bin, differently for train and test\n",
    "    list_z_scalar = []\n",
    "    for i, row in DFRES_TRAINTEST.iterrows():\n",
    "        z = row[\"z\"] # (ndims, ntrials, ntimes)\n",
    "        if row[\"train_test\"]==\"train\":\n",
    "            tbin = tbin_train\n",
    "        elif row[\"train_test\"]==\"test\":\n",
    "            tbin = tbin_test\n",
    "        else:\n",
    "            assert False\n",
    "        list_z_scalar.append(z[:,:,tbin][:,:,None]) # (ndims, ntrials, 1)\n",
    "        \n",
    "    DFRES_TRAINTEST[\"z_scalar\"] = list_z_scalar\n",
    "        \n",
    "    \n",
    "    # SCALAR PLOTS: train vs. test\n",
    "    var_subplots = \"train_test\"\n",
    "    var_color_by = \"seqc_0_shape\"\n",
    "    # for traj_or_scalar in [\"traj\", \"scalar\"]:\n",
    "    for traj_or_scalar in [\"scalar\"]:\n",
    "        for marg in margs:\n",
    "            for bregion in list_br:\n",
    "                    \n",
    "                if bregion==\"vlPFC\":\n",
    "                    # Pick out specific df\n",
    "                    b = DFRES_TRAINTEST[\"marg\"]==marg\n",
    "                    c = DFRES_TRAINTEST[\"bregion\"]==bregion\n",
    "                    dfthis = DFRES_TRAINTEST[b & c]\n",
    "                    \n",
    "                    \n",
    "                    from neuralmonkey.analyses.state_space_good import trajgood_plot_colorby_splotby\n",
    "                    fig, axes = trajgood_plot_colorby_splotby(dfthis, var_color_by, var_subplots, dims=(0,1),\n",
    "                                                      traj_or_scalar=traj_or_scalar, mean_over_trials=True,\n",
    "                                                      times_to_mark=times_to_mark,\n",
    "                                                       times_to_mark_markers=times_to_mark_markers,\n",
    "                                                       time_bin_size=time_bin_size,\n",
    "                                                       markersize=markersize, marker=marker,\n",
    "                                                       text_plot_pt1=text_plot_pt1,\n",
    "                                                       alpha=0.2)\n",
    "                    \n",
    "                    savefig(fig, f\"{savedir}/2D_train_vs_test-{traj_or_scalar}-marg_{marg}-{bregion}-var_{var_color_by}-ovar_{var_subplots}-tbin_test_{tbin_test}.pdf\")\n",
    "                    plt.close(\"all\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7ff7d80d6e8f2ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute significance, based on decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7b719342bdae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 1st mixing component looks merely like noise. But to be sure, we can run a significance analysis:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bd3ae945d6e45fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_masks = dpca.significance_analysis(R,  trialR, n_shuffles=10, n_splits=10, n_consecutive=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e8bb918ae5ee6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_masks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b4231af5b2375d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can highlight the significant parts of the demixed components with a black bar underneath. Note that there is no significant analysis time, since there are no classes to compute the significance over."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c541a15e079a99f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_masks[\"st\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14d6a0758de5ba31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Z[\"s\"].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74e0e06dac7e97d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time = arange(T)\n",
    "\n",
    "figure(figsize=(16,7))\n",
    "subplot(131)\n",
    "\n",
    "for s in range(S):\n",
    "    plot(time,Z['t'][0,s])\n",
    "\n",
    "title('1st time component')\n",
    "    \n",
    "subplot(132)\n",
    "\n",
    "for s in range(S):\n",
    "    plot(time,Z['s'][0,s])\n",
    "\n",
    "imshow(significance_masks['s'][0][None,:],extent=[0,250,amin(Z['s'])-1,amin(Z['s'])-0.5],aspect='auto',cmap='gray_r',vmin=0,vmax=1)\n",
    "ylim([amin(Z['s'])-1,amax(Z['s'])+1])\n",
    "\n",
    "title('1st stimulus component')\n",
    "    \n",
    "subplot(133)\n",
    "\n",
    "for s in range(S):\n",
    "    plot(time,Z['st'][0,s])\n",
    "\n",
    "dZ = amax(Z['st'])-amin(Z['st'])\n",
    "imshow(significance_masks['st'][0][None,:],extent=[0,250,amin(Z['st'])-dZ/10.,amin(Z['st'])-dZ/5.],aspect='auto',cmap='gray_r',vmin=0,vmax=1)\n",
    "ylim([amin(Z['st'])-dZ/10.,amax(Z['st'])+dZ/10.])\n",
    "    \n",
    "title('1st mixing component')\n",
    "show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc6d4754706d43a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
