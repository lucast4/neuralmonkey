{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" Loads data saved in /rsa\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from neuralmonkey.classes.session import load_mult_session_helper\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T01:15:17.974100065Z",
     "start_time": "2024-01-10T01:15:17.896626749Z"
    }
   },
   "id": "93d48dd175d3e9e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MULT RES PLOTS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa1dfb1d5ec391a5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pythonlib.tools.pandastools import grouping_print_n_samples, append_col_with_grp_index\n",
    "from pythonlib.tools.plottools import rotate_x_labels, rotate_y_labels, savefig\n",
    "from pythonlib.tools.snstools import rotateLabel\n",
    "from pythonlib.tools.expttools import writeDictToYaml\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T01:15:18.683890572Z",
     "start_time": "2024-01-10T01:15:18.615367739Z"
    }
   },
   "id": "e88b95f92fd902ca"
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "list_which_level = [\"stroke\", \"stroke_off\"]\n",
    "# version_distance = \"pearson\"\n",
    "\n",
    "def load_mult_data_helper(animal, DATE, version_distance):\n",
    "    \"\"\" Load all data across (i) timw widopws (ii) which levels\n",
    "    \"\"\"\n",
    "    SAVEDIR = \"/gorilla1/analyses/recordings/main/RSA\"\n",
    "    SAVEDIR_MULT = f\"{SAVEDIR}/{animal}/MULT/{DATE}/{version_distance}\"\n",
    "    import os\n",
    "    os.makedirs(SAVEDIR_MULT, exist_ok=True)\n",
    "    \n",
    "    params = {\n",
    "        \"DATE\":DATE,\n",
    "        \"animal\":animal,\n",
    "        \"list_which_level\":list_which_level,\n",
    "        \"version_distance\":version_distance,\n",
    "        \"SAVEDIR\":SAVEDIR\n",
    "    }\n",
    "    path = f\"{SAVEDIR_MULT}/params.yaml\"\n",
    "    writeDictToYaml(params, path)\n",
    "    \n",
    "    ####### LOAD    \n",
    "    RES = []\n",
    "    for which_level in list_which_level:\n",
    "        print(\"Getting: \", which_level)\n",
    "        savedir = f\"{SAVEDIR}/{animal}/{DATE}/{which_level}/{version_distance}\"\n",
    "        path = f\"{savedir}/resthis.pkl\"\n",
    "        print(\"Loading res from: \", path)\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                res = pickle.load(f)\n",
    "            RES.append(res)\n",
    "        except Exception as err:\n",
    "            print(path)\n",
    "            print(\"Couldnt load this data! *******************\", version_distance, animal, DATE, which_level)\n",
    "        RES.append(res)\n",
    "        \n",
    "    # bregions\n",
    "    from neuralmonkey.neuralplots.brainschematic import REGIONS_IN_ORDER\n",
    "    \n",
    "    return RES, SAVEDIR_MULT, params, REGIONS_IN_ORDER"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T04:00:17.441759184Z",
     "start_time": "2024-01-10T04:00:17.371024307Z"
    }
   },
   "id": "2f894f8e08848b00"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2eebc8e5b31a896a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m     21\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(SAVEDIR_MULT, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     23\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDATE\u001B[39m\u001B[38;5;124m\"\u001B[39m:DATE,\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manimal\u001B[39m\u001B[38;5;124m\"\u001B[39m:animal,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSAVEDIR\u001B[39m\u001B[38;5;124m\"\u001B[39m:SAVEDIR\n\u001B[1;32m     29\u001B[0m }\n\u001B[1;32m     30\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSAVEDIR_MULT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/params.yaml\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Either run above and create RES, or load here\n",
    "# DATE = 230623\n",
    "# animal = \"Pancho\"\n",
    "\n",
    "for animal, DATE in [\n",
    "    (\"Diego\", \"230628\"),\n",
    "    (\"Diego\", \"230630\"),\n",
    "    (\"Pancho\", \"230623\"),\n",
    "    (\"Pancho\", \"230626\"),\n",
    "    ]:\n",
    "    for version_distance in [\"pearson\", \"euclidian\"]:\n",
    "    \n",
    "        # DATE = 230630\n",
    "        # animal = \"Diego\"\n",
    "    \n",
    "        RES, SAVEDIR_MULT, params, REGIONS_IN_ORDER = load_mult_data_helper(animal, DATE, version_distance)\n",
    "\n",
    "        # COLLEC \n",
    "        # Compare to theoetical simmat\n",
    "        list_df = []\n",
    "        for res in RES:\n",
    "            which_level = res[\"which_level\"] \n",
    "            version_distance = res[\"version_distance\"] \n",
    "            df = res[\"DFRES_THEOR\"]\n",
    "            df[\"which_level\"] = which_level\n",
    "            df[\"version_distance\"] = version_distance\n",
    "            list_df.append(df)\n",
    "            \n",
    "        DFMULT_THEOR = pd.concat(list_df).reset_index(drop=True)\n",
    "        \n",
    "        DFMULT_THEOR\n",
    "        \n",
    "        # a = DFMULT_THEOR[\"bregion\"]==\"vlPFC_p\"\n",
    "        # b = DFMULT_THEOR[\"var\"]==\"shape_oriented\"\n",
    "        # c = DFMULT_THEOR[\"which_level\"]==\"stroke\"\n",
    "        # d = DFMULT_THEOR[\"twind\"]==(-0.1, 0.1)\n",
    "        # \n",
    "        # DFMULT_THEOR[(a) & (b) & (c) & (d)]\n",
    "        \n",
    "        # grouping_print_n_samples(DFMULT_THEOR, [\"bregion\", \"var\", \"which_level\", \"twind\"])\n",
    "        \n",
    "        ##### Preprocess\n",
    "        DFMULT_THEOR[\"twind_str\"] = [\"_to_\".join([str(tt) for tt in t]) for t in DFMULT_THEOR[\"twind\"].tolist()]\n",
    "        DFMULT_THEOR = append_col_with_grp_index(DFMULT_THEOR, [\"which_level\", \"twind_str\"], \"wl_tw\", strings_compact=True)\n",
    "        EFFECT_VARS = DFMULT_THEOR[\"var\"].unique().tolist()\n",
    "        list_bregion = DFMULT_THEOR[\"bregion\"].unique().tolist()\n",
    "        \n",
    "        ##### Plots\n",
    "        savedir = f\"{SAVEDIR_MULT}/overview\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        from pythonlib.tools.pandastools import append_col_with_grp_index, convert_to_2d_dataframe\n",
    "        \n",
    "        for yvar in [\"cc\", \"mr_coeff\"]:\n",
    "            \n",
    "            # (1) Pointplot, showing all results, but hard to read.\n",
    "            fig = sns.catplot(data=DFMULT_THEOR, x=\"twind\", y=yvar, col=\"bregion\", hue=\"var\", kind=\"point\", row=\"which_level\")\n",
    "            rotateLabel(fig)\n",
    "            savefig(fig, f\"{savedir}/pointplot-{yvar}-bregions.pdf\")\n",
    "            plt.close(\"all\")\n",
    "        \n",
    "            # IN PROGRESS - subtracting global mean within each level of (effect var). Decided it wasnt needed\n",
    "            # conjucntion of twind and which_level\n",
    "            if False:\n",
    "                from pythonlib.tools.pandastools import datamod_normalize_row_after_grouping\n",
    "                # datamod_normalize_row_after_grouping(DFMULT_THEOR, \"bregion\", \n",
    "                grp = [\"bregion\"]\n",
    "                DFMULT_THEOR.groupby(grp).transform(lambda x: (x - x.mean()) / x.std())\n",
    "                \n",
    "                # Normalize by subtracting mean effect within each bregion, to allow comparison of each bregion's \"signature\"\n",
    "                from pythonlib.tools.pandastools import aggregGeneral\n",
    "                aggregGeneral(DFMULT_THEOR, [\"var\" , \"\"])\n",
    "            \n",
    "            if False:\n",
    "                # IN PROGRESS - ONE VECTOR FOR EACH BREGION (across var, which_level, and time window).\n",
    "                # new variable, conjunction of var and time window\n",
    "                %matplotlib inline\n",
    "                from pythonlib.tools.pandastools import append_col_with_grp_index, convert_to_2d_dataframe\n",
    "                DFMULT_THEOR = append_col_with_grp_index(DFMULT_THEOR, [\"var\", \"wl_tw\"], \"var_wl_tw\", strings_compact=True)\n",
    "                \n",
    "                # Heatmap\n",
    "                ncols = 3\n",
    "                W = 4\n",
    "                H = 4\n",
    "                nrows = int(np.ceil(len(EFFECT_VARS)/ncols))\n",
    "                dfthis = DFMULT_THEOR\n",
    "                for norm_method in [None, \"row_sub\", \"col_sub\"]:\n",
    "                    _, fig, _, _ = convert_to_2d_dataframe(dfthis, \"bregion\", \"var_wl_tw\", True, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False,\n",
    "                                            norm_method=norm_method)\n",
    "                    savefig(fig, f\"{savedir}/heatmap-\")\n",
    "                    \n",
    "            # (2) Heatmaps, easier to parse (Concatting all which_levels)\n",
    "            W = 4\n",
    "            H = 4\n",
    "            ncols = 3\n",
    "            for norm_method in [None, \"all_sub\", \"row_sub\", \"col_sub\"]:\n",
    "                # Heatmap - one subplot for each var\n",
    "                nrows = int(np.ceil(len(EFFECT_VARS)/ncols))\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "                for var, ax in zip(EFFECT_VARS, axes.flatten()):\n",
    "                    print(var)\n",
    "                    dfthis = DFMULT_THEOR[DFMULT_THEOR[\"var\"]==var]\n",
    "                    convert_to_2d_dataframe(dfthis, \"bregion\", \"wl_tw\", True, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False,\n",
    "                                            norm_method=norm_method, ax=ax)\n",
    "                    ax.set_title(var)\n",
    "                savefig(fig, f\"{savedir}/heatmap-subplot_by_var-norm_{norm_method}-ccvar_{yvar}.pdf\")\n",
    "        \n",
    "            \n",
    "                # Heatmap - one subplot for each bregion\n",
    "                nrows = int(np.ceil(len(list_bregion)/ncols))\n",
    "                fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "                for bregion, ax in zip(list_bregion, axes.flatten()):\n",
    "                    print(norm_method, bregion)\n",
    "                    dfthis = DFMULT_THEOR[DFMULT_THEOR[\"bregion\"]==bregion]\n",
    "                    convert_to_2d_dataframe(dfthis, \"var\", \"wl_tw\", True, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False,\n",
    "                                            norm_method=norm_method, ax=ax)\n",
    "                    ax.set_title(bregion)\n",
    "                savefig(fig, f\"{savedir}/heatmap_concat-subplot_by_bregion-norm_{norm_method}-ccvar_{yvar}.pdf\")\n",
    "                \n",
    "                plt.close(\"all\")\n",
    "                \n",
    "            # (3) Heatmaps, easier to parse (Separate plots for each which_levels)\n",
    "            W = 4\n",
    "            H = 4\n",
    "            ncols = 3\n",
    "            for which_level in list_which_level:\n",
    "                for norm_method in [None, \"all_sub\", \"row_sub\", \"col_sub\"]:\n",
    "                    # Heatmap - one subplot for each var\n",
    "                    nrows = int(np.ceil(len(EFFECT_VARS)/ncols))\n",
    "                    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "                    for var, ax in zip(EFFECT_VARS, axes.flatten()):\n",
    "                        print(var)\n",
    "                        dfthis = DFMULT_THEOR[(DFMULT_THEOR[\"var\"]==var) & (DFMULT_THEOR[\"which_level\"]==which_level)]\n",
    "                        convert_to_2d_dataframe(dfthis, \"bregion\", \"twind\", True, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False,\n",
    "                                                norm_method=norm_method, ax=ax)\n",
    "                        ax.set_title(var)\n",
    "                    savefig(fig, f\"{savedir}/heatmap_whichlevel_{which_level}-subplot_by_var-norm_{norm_method}-ccvar_{yvar}.pdf\")\n",
    "            \n",
    "                \n",
    "                    # Heatmap - one subplot for each bregion\n",
    "                    nrows = int(np.ceil(len(list_bregion)/ncols))\n",
    "                    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "                    for bregion, ax in zip(list_bregion, axes.flatten()):\n",
    "                        print(norm_method, bregion)\n",
    "                        dfthis = DFMULT_THEOR[(DFMULT_THEOR[\"bregion\"]==bregion) & (DFMULT_THEOR[\"which_level\"]==which_level)]\n",
    "                        convert_to_2d_dataframe(dfthis, \"var\", \"twind\", True, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False,\n",
    "                                                norm_method=norm_method, ax=ax)\n",
    "                        ax.set_title(bregion)\n",
    "                    savefig(fig, f\"{savedir}/heatmap_whichlevel_{which_level}-subplot_by_bregion-norm_{norm_method}-ccvar_{yvar}.pdf\")\n",
    "                    \n",
    "                    plt.close(\"all\")\n",
    "                    \n",
    "        ##### Kernels - each bregions \"signature\" by concatting across all times and var\n",
    "        savedir = f\"{SAVEDIR_MULT}/kernels\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        \n",
    "        def kernel_convert_from_combined_to_split(kernel_combined):\n",
    "            \"\"\" Convert from list of tuple (event string, weight) to\n",
    "            two lists; events and weights.\n",
    "            \"\"\"\n",
    "            kernel = [k[0] for k in kernel_combined]\n",
    "            weights = [k[1] for k in kernel_combined]\n",
    "            \n",
    "            return kernel, weights\n",
    "            \n",
    "        def kernel_extract_which_var(kernel_name):\n",
    "            \n",
    "            map_name_to_var = {\n",
    "                \"reach_prev\":\"gap_from_prev_angle_binned\",\n",
    "                \"reach_next\":\"gap_to_next_angle_binned\",\n",
    "                \"stkidx_stroke\":\"stroke_index_semantic\",\n",
    "                \"shape\":\"shape_oriented\",\n",
    "                \"loc_future\":\"CTXT_loc_next\",\n",
    "                \"shape_future\":\"CTXT_shape_next\",\n",
    "                \"stkidx_entire\":\"stroke_index_semantic\"}\n",
    "            \n",
    "            if kernel_name not in map_name_to_var.keys():\n",
    "                # The name is the var\n",
    "                return kernel_name\n",
    "            else:\n",
    "                return map_name_to_var[kernel_name]\n",
    "            \n",
    "        def _check_twind_center_within(twind, tmin, tmax):\n",
    "            \"\"\" Return True if the center of the window (twind) \n",
    "            is greater than tmin and les sthan tmax\"\"\"\n",
    "            cen = np.mean(twind)\n",
    "            return cen>tmin and cen<tmax\n",
    "        \n",
    "        # Construct kernels for each (i.e., events in order and weights in order)\n",
    "        list_twind = DFMULT_THEOR[\"twind\"].unique().tolist()\n",
    "        list_which_level = DFMULT_THEOR[\"which_level\"].unique().tolist()\n",
    "        \n",
    "        kernel_reach_prev = [] \n",
    "        kernel_reach_next = [] \n",
    "        kernel_gridloc = [] \n",
    "        kernel_shape = [] \n",
    "        kernel_future_loc = []\n",
    "        kernel_future_shape = []\n",
    "        kernel_stkidx_during = []\n",
    "        kernel_stkidx_entire = []\n",
    "        for wl in list_which_level:\n",
    "            for twind in list_twind:\n",
    "                ev = f\"{wl}|{twind[0]}_to_{twind[1]}\"\n",
    "        \n",
    "                # Reach motor\n",
    "                if wl==\"stroke\" and _check_twind_center_within(twind, -0.5, -0.1):\n",
    "                    # then window ends before align time\n",
    "                    k = 1\n",
    "                else:\n",
    "                    k = 0\n",
    "                kernel_reach_prev.append((ev, k))\n",
    "                    \n",
    "                # After offset\n",
    "                if wl==\"stroke_off\" and _check_twind_center_within(twind, 0, 0.5):\n",
    "                    # then window ends before align time\n",
    "                    k = 1\n",
    "                else:\n",
    "                    k = 0\n",
    "                kernel_reach_next.append((ev, k))\n",
    "                \n",
    "                # Location (abstract, current stroke)\n",
    "                if wl==\"stroke\" and _check_twind_center_within(twind, -0.1, 0.3):\n",
    "                    # then window ends before align time\n",
    "                    k = 1        \n",
    "                # elif wl==\"stroke_off\" and _check_twind_center_within(twind, -0.3, 0):\n",
    "                #     k = 1\n",
    "                else:\n",
    "                    k = 0\n",
    "                kernel_gridloc.append((ev, k))\n",
    "                kernel_stkidx_during.append((ev, k))\n",
    "                kernel_shape.append((ev, k))\n",
    "                \n",
    "                # Next stroke (loc)\n",
    "                if wl==\"stroke\" and _check_twind_center_within(twind, -0.5, 0.3):\n",
    "                    # then window ends before align time\n",
    "                    k = 1        \n",
    "                else:\n",
    "                    k = 0\n",
    "                kernel_future_loc.append((ev, k))\n",
    "                kernel_future_shape.append((ev, k))\n",
    "                \n",
    "                # Stroke index (entire)\n",
    "                k=1\n",
    "                kernel_stkidx_entire.append((ev, k))\n",
    "        \n",
    "        KERNELS = {\n",
    "            \"reach_prev\":kernel_reach_prev,\n",
    "            \"reach_next\":kernel_reach_next,\n",
    "            \"gridloc\":kernel_gridloc,\n",
    "            \"stkidx_stroke\":kernel_stkidx_during,\n",
    "            \"shape\":kernel_shape,\n",
    "            \"loc_future\":kernel_future_loc,\n",
    "            \"shape_future\":kernel_future_shape,\n",
    "            \"stkidx_entire\":kernel_stkidx_entire,\n",
    "        }\n",
    "        \n",
    "        # Plot kernel templates\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        for i, (name, kernel) in enumerate(KERNELS.items()):\n",
    "            k, w= kernel_convert_from_combined_to_split(kernel)\n",
    "            \n",
    "            var = kernel_extract_which_var(name)\n",
    "            \n",
    "            ax.scatter(k, np.ones(len(k))*i, c=[1-x for x in w], alpha=0.5, label=name)\n",
    "            ax.text(0, i, var, color=\"r\", alpha=0.5)\n",
    "            \n",
    "        ax.set_yticks(list(range(len(KERNELS.keys()))), labels=KERNELS.keys())\n",
    "        ax.set_ylabel(\"kernel name\")\n",
    "        ax.set_xlabel(\"time window\")\n",
    "        rotate_x_labels(ax, 90)\n",
    "        rotate_y_labels(ax, 0)\n",
    "        ax.set_title(\"Kernels (dark dot = 1); red: var it operates on\")            \n",
    "        savefig(fig, f\"{savedir}/kernels_weights.pdf\")\n",
    "        \n",
    "        \n",
    "        yvar = \"cc\"\n",
    "        for yvar in [\"cc\", \"mr_coeff\"]:\n",
    "            # PREPROCESS - # for each variable, get 2d df (bregion x twinds)\n",
    "            from neuralmonkey.analyses.event_temporal_modulation import kernel_compute, _kernel_compute_scores\n",
    "            DictBregionToDf2d = {}\n",
    "            for bregion in list_bregion:\n",
    "                dfthis = DFMULT_THEOR[DFMULT_THEOR[\"bregion\"]==bregion]\n",
    "                dftmp, fig, ax, rgba_values = convert_to_2d_dataframe(dfthis, \"var\", \"wl_tw\", False, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False)\n",
    "                DictBregionToDf2d[bregion] = dftmp\n",
    "            DictVarToDf2d = {}\n",
    "            for var in EFFECT_VARS:\n",
    "                dfthis = DFMULT_THEOR[DFMULT_THEOR[\"var\"]==var]\n",
    "                dftmp, fig, ax, rgba_values = convert_to_2d_dataframe(dfthis, \"bregion\", \"wl_tw\", False, \"mean\", yvar, annotate_heatmap=False, dosort_colnames=False)\n",
    "                DictVarToDf2d[var] = dftmp\n",
    "            \n",
    "            ##### Score data using kernesl\n",
    "            res = []\n",
    "                \n",
    "            for i, (name, kernel) in enumerate(KERNELS.items()):\n",
    "                k, w= kernel_convert_from_combined_to_split(kernel)\n",
    "                var = kernel_extract_which_var(name)\n",
    "                dfthis = DictVarToDf2d[var]\n",
    "                \n",
    "                # apply kernel\n",
    "                scores = _kernel_compute_scores(dfthis, k, w)\n",
    "                \n",
    "                # Distribute scores across bregions\n",
    "                assert list_bregion==dfthis.index.tolist()\n",
    "                \n",
    "                # Collect\n",
    "                for s, br in zip(scores, list_bregion):\n",
    "                    \n",
    "                    res.append({\n",
    "                        \"bregion\":br,\n",
    "                        \"score\":s,\n",
    "                        \"kernel_name\":name,\n",
    "                        \"kernel_events\":k,\n",
    "                        \"kernel_weights\":w,\n",
    "                        \"var\":var\n",
    "                    })\n",
    "                \n",
    "            dfres_kernels = pd.DataFrame(res)  \n",
    "            dfres_kernels\n",
    "            \n",
    "            # Second-order kernels, whose dimensions are the first-order kernels\n",
    "            # NOTE: First-order kernels operate over time windows (for a specific var)\n",
    "            # --> Therefore, second-order kernels are 2d-kernels, operating first over time, then over variables.\n",
    "            # NOTE: many first order kernels are perfectly fine to keep as second-order..\n",
    "            \n",
    "            # first, get reshaped df (bregion, first order kernel name)\n",
    "            # dfres_kernels_2d = convert_to_2d_dataframe(dfres_kernels, \"bregion\", \"kernel_name\", False, \"mean\", \"score\", dosort_colnames=False, list_cat_1=REGIONS_IN_ORDER)[0]\n",
    "            dfres_kernels_2d = convert_to_2d_dataframe(dfres_kernels, \"bregion\", \"kernel_name\", False, \"mean\", \"score\", dosort_colnames=False)[0]\n",
    "            \n",
    "            k = (\"reach_next\", \"reach_prev\")\n",
    "            w = (1,1)\n",
    "            name = \"reachdir\"\n",
    "            scores = _kernel_compute_scores(dfres_kernels_2d, k, w)\n",
    "            dfres_kernels_2d[name] = scores\n",
    "            \n",
    "            k = (\"gridloc\", \"reachdir\")\n",
    "            w = (1,-1)\n",
    "            name = \"gridloc_abstract\"\n",
    "            scores = _kernel_compute_scores(dfres_kernels_2d, k, w)\n",
    "            dfres_kernels_2d[name] = scores\n",
    "            \n",
    "            \n",
    "            from pythonlib.tools.snstools import heatmap\n",
    "            fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "            for ax, norm_method in zip(axes.flatten(), [None, \"all_sub\", \"col_sub\", \"row_sub\"]):\n",
    "                heatmap(dfres_kernels_2d, ax, False, (None, None), norm_method=norm_method)\n",
    "                # convert_to_2d_dataframe(dfres_kernels, \"bregion\", \"kernel_name\", True, \"mean\", \"score\", annotate_heatmap=False, norm_method=\"col_sub\", dosort_colnames=False, list_cat_1=REGIONS_IN_ORDER)\n",
    "                ax.set_title(f\"norm_{norm_method}\")\n",
    "            savefig(fig, f\"{savedir}/heatmap-kernel_scores-ccvar_{yvar}.pdf\")\n",
    "            \n",
    "            plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T01:15:27.527904988Z",
     "start_time": "2024-01-10T01:15:27.404759236Z"
    }
   },
   "id": "641a087b671281fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Pick out specific pairs to contrast (plot)\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "plot_45scatter_means_flexible_grouping(dfres_kernels_2d, \"kernel_name\", \"gridloc_abstract\", \"reachdir\", None, \"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf60c57488941726"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# And then PCA.\n",
    "\n",
    "# kernels, i.e., a mask of length (var x which_level x twind)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "753df122f8c8e492"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Testing out, method for directly pitting two variables against each other"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27c630a5e5bc0339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1) Prune at level of comparisons: keep only comparisons that are opposed across the vars.\n",
    "# -- What is the principle behind this? IT is more about the sign of the effect than the magnitude?\n",
    "# -- More generally, is about getting subset of comparisons which are not correlated across the two vars.\n",
    "# -- This seems weird?\n",
    "# 2) Prune at level of var-levels: keep only levels (of both vars) that have at least 1 case of other var.\n",
    "# -- This would get the \"same beh\" control in epochs.\n",
    "# 3) \"Kernels\" using either the raw results, or the processed ones here (pairwise between variables).\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8754a66b2ea4008f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# - pick specific pair of variables. and plot across (bregion, timewindows). \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bfe7d774fb488b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pull out speciifc dataset (bregion x time)\n",
    "which_level = \"stroke\"\n",
    "version_distance = \"pearson\"\n",
    "PLOT = False\n",
    "\n",
    "savedir = f\"{SAVEDIR}/{animal}/{DATE}/{which_level}/{version_distance}\"\n",
    "path = f\"{savedir}/resthis.pkl\"\n",
    "print(\"Loading res from: \", path)\n",
    "try:\n",
    "    with open(path, \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "except Exception as err:\n",
    "    print(path)\n",
    "    print(\"Couldnt load this data! *******************\", version_distance, animal, DATE, which_level)\n",
    "\n",
    "## EXTRACT\n",
    "DictBregionTwindClraw = res[\"DictBregionTwindClraw\"]\n",
    "DictBregionTwindClsim = res[\"DictBregionTwindClsim\"]\n",
    "DictBregionTwindCPA = res[\"DictBregionTwindPA\"]\n",
    "\n",
    "res = []\n",
    "for twind in list_twind:\n",
    "    for bregion in list_bregion:\n",
    "        # bregion = \"PMd_p\"\n",
    "        key = (bregion, twind)\n",
    "        Clraw = DictBregionTwindClraw[key]\n",
    "        Clsim = DictBregionTwindClsim[key]\n",
    "        PA = DictBregionTwindCPA[key]\n",
    "        \n",
    "        ##### 1. Given two variables, how similar are their theoretical sim mats?\n",
    "        if PLOT:\n",
    "            list_var1 =[]\n",
    "            list_var2 = []\n",
    "            list_cc = []\n",
    "            for i, var1 in enumerate(EFFECT_VARS):\n",
    "                for j, var2 in enumerate(EFFECT_VARS):\n",
    "                    if j>i:\n",
    "                        Cltheor1, _ = Clsim.rsa_distmat_construct_theoretical(var1, False)\n",
    "                        Cltheor2, _ = Clsim.rsa_distmat_construct_theoretical(var2, False)\n",
    "            \n",
    "                        vec1 = Cltheor1.dataextract_upper_triangular_flattened()\n",
    "                        vec2 = Cltheor2.dataextract_upper_triangular_flattened()\n",
    "                        c = np.corrcoef(vec1, vec2)[0,1]\n",
    "                        \n",
    "                        print(var1, \" -- \", var2, \" -- \", c)\n",
    "                        list_var1.append(var1)\n",
    "                        list_var2.append(var2)\n",
    "                        list_cc.append(c)\n",
    "            \n",
    "            dfthis = pd.DataFrame({\"var1\":list_var1, \"var2\":list_var2, \"cc\":list_cc})\n",
    "            convert_to_2d_dataframe(dfthis, \"var1\", \"var2\", True, \"mean\", \"cc\", annotate_heatmap=True, dosort_colnames=False)\n",
    "    \n",
    "        # from neuralmonkey.analyses.state_space_good import rsa_convert_PA_to_Cl\n",
    "        # rsa_convert_PA_to_Cl(DictBregionTwindPA.values[0])\n",
    "        \n",
    "        ##### Compare pair of vars\n",
    "        # Go thru all pairs of vars\n",
    "        PLOT = False\n",
    "        for ivar, var in enumerate(EFFECT_VARS):\n",
    "            for jvar, var_other in enumerate(EFFECT_VARS):\n",
    "                if not var==var_other:\n",
    "                # if jvar>ivar:\n",
    "                    # var = \"gridloc\"\n",
    "                    # var_other = \"gap_from_prev_angle_binned\"\n",
    "                    # var = \"shape_oriented\"\n",
    "                    # var_other = \"gridloc\"\n",
    "                    \n",
    "                    # find pairs of datapts which make different predictions across the two variables.\n",
    "                    Cltheor1, _ = Clsim.rsa_distmat_construct_theoretical(var, False)\n",
    "                    Cltheor2, _ = Clsim.rsa_distmat_construct_theoretical(var_other, False)\n",
    "                    \n",
    "                    # Extract actual data and theor data\n",
    "                    vec_dat = Clsim.dataextract_upper_triangular_flattened()\n",
    "                    vec1 = Cltheor1.dataextract_upper_triangular_flattened()\n",
    "                    vec2 = Cltheor2.dataextract_upper_triangular_flattened()\n",
    "                    \n",
    "                    cc1 = np.corrcoef(vec_dat, vec1)[0,1]\n",
    "                    cc2 = np.corrcoef(vec_dat, vec2)[0,1]\n",
    "                    print(var, cc1)\n",
    "                    print(var_other, cc2)\n",
    "                    \n",
    "                    cc_kind = \"actual\"\n",
    "                    res.append({\n",
    "                        \"var\":var,\n",
    "                        \"var_other\":var_other,\n",
    "                        \"cc1\":cc1,\n",
    "                        \"cc2\":cc2,\n",
    "                        \"cc_kind\":cc_kind,\n",
    "                        \"bregion\":bregion,\n",
    "                        \"twind\":twind\n",
    "                    })\n",
    "                    # res.append({\n",
    "                    #     \"var_or_other\":\"var\",\n",
    "                    #     \"var\":var,\n",
    "                    #     \"cc\":cc1,\n",
    "                    #     \"cc_kind\":cc_kind,\n",
    "                    #     \"bregion\":bregion,\n",
    "                    #     \"twind\":twind\n",
    "                    # })\n",
    "                    # res.append({\n",
    "                    #     \"var_or_other\":\"other\",\n",
    "                    #     \"var\":var_other,\n",
    "                    #     \"cc\":cc2,\n",
    "                    #     \"cc_kind\":cc_kind,\n",
    "                    #     \"bregion\":bregion,\n",
    "                    #     \"twind\":twind\n",
    "                    # })\n",
    "                    \n",
    "                    ##### Method 1: include only data-pairs that make different predictions across the var\n",
    "                    \n",
    "                    # - normalize so that 0 is same and 1 is most different. This is becuase some, e.g, for\n",
    "                    # ordinal, have larger range.\n",
    "                    vec1_norm = vec1/np.max(vec1)\n",
    "                    vec2_norm = vec2/np.max(vec2)\n",
    "                    \n",
    "                    inds_mask = vec1_norm!=vec2_norm\n",
    "                    print(f\"Keeping this many comparisons: {sum(inds_mask)}, out of this many total: {len(inds_mask)}\")\n",
    "                    \n",
    "                    cc1 = np.corrcoef(vec_dat[inds_mask], vec1[inds_mask])[0,1]\n",
    "                    cc2 = np.corrcoef(vec_dat[inds_mask], vec2[inds_mask])[0,1]\n",
    "                    print(var, cc1)\n",
    "                    print(var_other, cc2)\n",
    "                    \n",
    "                    cc_kind = \"method1\"\n",
    "                    res.append({\n",
    "                        \"var\":var,\n",
    "                        \"var_other\":var_other,\n",
    "                        \"cc1\":cc1,\n",
    "                        \"cc2\":cc2,\n",
    "                        \"cc_kind\":cc_kind,\n",
    "                        \"bregion\":bregion,\n",
    "                        \"twind\":twind\n",
    "                    })\n",
    "                    \n",
    "                    ##### Method 2: Constrain just to cases that are \"same\" (i.e., dist = 0) in the other var. \n",
    "                    # NO: this is not good. this removes all comparisons of var across levels of other_var. This stops \n",
    "                    # testing of \"abstraction\" of var.\n",
    "                    if False:\n",
    "                        inds_mask = vec2==1\n",
    "                        print(f\"Keeping this many comparisons: {sum(inds_mask)}, out of this many total: {len(inds_mask)}\")\n",
    "                        \n",
    "                        cc1 = np.corrcoef(vec_dat[inds_mask], vec1[inds_mask])[0,1]\n",
    "                        cc2 = np.corrcoef(vec_dat[inds_mask], vec2[inds_mask])[0,1]\n",
    "                        print(cc1, cc2)\n",
    "                    \n",
    "                    ##### Method 3: Constrain just to levels of \"othervar\" that have at least 2 levels for var. \n",
    "                    # - reason: otherwise that level of othervar isn't contributing.\n",
    "                    from pythonlib.tools.pandastools import conjunction_vars_prune_to_balance, extract_with_levels_of_conjunction_vars\n",
    "                    \n",
    "                    # First, get subset of rows that are good.\n",
    "                    dflab = Clsim.rsa_labels_return_as_df()\n",
    "                    if PLOT:\n",
    "                        savedir=\"/tmp/tmp.png\"\n",
    "                    else:\n",
    "                        savedir = None\n",
    "                    dfout1, _ = extract_with_levels_of_conjunction_vars(dflab, var, [var_other], n_min=2, lenient_allow_data_if_has_n_levels=2, plot_counts_heatmap_savedir=savedir, DEBUG=False);\n",
    "                    dfout2, _ = extract_with_levels_of_conjunction_vars(dflab, var_other, [var], n_min=2, lenient_allow_data_if_has_n_levels=2, plot_counts_heatmap_savedir=savedir, DEBUG=False);\n",
    "                    inds_rows = [i for i in dfout1[\"row_index\"].tolist() if i in dfout2[\"row_index\"].tolist()]\n",
    "                    \n",
    "                    print(len(dflab), \" datapts, pruned to --> \", len(inds_rows))\n",
    "                    \n",
    "                    vec_dat = Clsim.dataextract_upper_triangular_flattened(True, inds_rows)\n",
    "                    vec1 = Cltheor1.dataextract_upper_triangular_flattened(True, inds_rows)\n",
    "                    vec2 = Cltheor2.dataextract_upper_triangular_flattened(True, inds_rows)\n",
    "                    \n",
    "                    cc1 = np.corrcoef(vec_dat, vec1)[0,1]\n",
    "                    cc2 = np.corrcoef(vec_dat, vec2)[0,1]\n",
    "                    print(var, cc1)\n",
    "                    print(var_other, cc2)\n",
    "                    \n",
    "                    cc_kind = \"method3\"\n",
    "                    res.append({\n",
    "                        \"var\":var,\n",
    "                        \"var_other\":var_other,\n",
    "                        \"cc1\":cc1,\n",
    "                        \"cc2\":cc2,\n",
    "                        \"cc_kind\":cc_kind,\n",
    "                        \"bregion\":bregion,\n",
    "                        \"twind\":twind\n",
    "                    })\n",
    "                    \n",
    "                    if False:\n",
    "                        # To get fully square matrix. Not worth it - drops too much data.\n",
    "                        dfthisout, dfcounts = conjunction_vars_prune_to_balance(dflab, var, var_other, True, prefer_to_drop_which=var);\n",
    "                    \n",
    "                    if PLOT:    \n",
    "                        # Plot correlation\n",
    "                        fig, ax = plt.subplots()\n",
    "                        # ax.plot(vec1[inds_mask], vec_dat[inds_mask], \"xk\", alpha=0.1)\n",
    "                        ax.plot(vec2[inds_mask], vec_dat[inds_mask], \"xk\", alpha=0.1)\n",
    "                        ax.set_xlabel(\"dist(theor)\")\n",
    "                        ax.set_ylabel(\"dist(dat)\")\n",
    "                        \n",
    "                    plt.close(\"all\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1522c1037320b7eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot summary\n",
    "dfres_cc_pairs = pd.DataFrame(res)\n",
    "\n",
    "# Heatmap (subplot for each bregion, each showing each pair of variables)\n",
    "cc_kind = \"method1\"\n",
    "\n",
    "if cc_kind==\"method1\":\n",
    "    diverge=True\n",
    "else:\n",
    "    diverge=False\n",
    "ncols = 3\n",
    "W = 4\n",
    "H = 4\n",
    "nrows = int(np.ceil(len(list_bregion)/ncols))\n",
    "\n",
    "# for norm_method in [\"all_sub\", None, \"row_sub\", \"col_sub\"]:\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "for bregion, ax in zip(list_bregion, axes.flatten()):\n",
    "    print(bregion)\n",
    "    a = dfres_cc_pairs[\"bregion\"]==bregion\n",
    "    b = dfres_cc_pairs[\"cc_kind\"]==cc_kind    \n",
    "    dfthis = dfres_cc_pairs[(a) & (b)]\n",
    "    convert_to_2d_dataframe(dfthis, \"var\", \"var_other\", True, \"mean\", \"cc1\", ax=ax, annotate_heatmap=False, diverge=diverge)\n",
    "    ax.set_title(bregion) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "797615dbb453c43f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Heatmap (subplot for each bregion, each showing each pair of variables)\n",
    "cc_kind = \"method3\"\n",
    "\n",
    "if cc_kind==\"method1\":\n",
    "    diverge=True\n",
    "else:\n",
    "    diverge=False\n",
    "ncols = 3\n",
    "W = 4\n",
    "H = 4\n",
    "nrows = int(np.ceil(len(list_bregion)/ncols))\n",
    "\n",
    "# for norm_method in [\"all_sub\", None, \"row_sub\", \"col_sub\"]:\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "for bregion, ax in zip(list_bregion, axes.flatten()):\n",
    "    print(bregion)\n",
    "    a = dfres_cc_pairs[\"bregion\"]==bregion\n",
    "    b = dfres_cc_pairs[\"cc_kind\"]==cc_kind    \n",
    "    dfthis = dfres_cc_pairs[(a) & (b)]\n",
    "    convert_to_2d_dataframe(dfthis, \"var\", \"var_other\", True, \"mean\", \"cc1\", ax=ax, annotate_heatmap=False, diverge=diverge)\n",
    "    ax.set_title(bregion) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570bc1f111904973"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Process results from method 1: for each var take mean over all other vars.\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "dfres_cc_pairs_AGG = aggregGeneral(dfres_cc_pairs, [\"var\", \"cc_kind\", \"bregion\", \"twind\"], [\"cc1\"], aggmethod=[\"mean\"])\n",
    "dfres_cc_pairs_AGG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b131b7eed795cdfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_cc_method = dfres_cc_pairs_AGG[\"cc_kind\"].unique().tolist()\n",
    "cc_kind = \"method3\"\n",
    "\n",
    "ncols = 3\n",
    "W = 4\n",
    "H = 4\n",
    "nrows = int(np.ceil(len(list_cc_method)/ncols))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*W, nrows*H))\n",
    "for cc_kind, ax in zip(list_cc_method, axes.flatten()):\n",
    "    if cc_kind==\"method1\":\n",
    "        diverge=True\n",
    "    else:\n",
    "        diverge=False\n",
    "    print(cc_kind)\n",
    "    # a = dfres_cc_pairs[\"bregion\"]==bregion\n",
    "    b = dfres_cc_pairs_AGG[\"cc_kind\"]==cc_kind    \n",
    "    dfthis = dfres_cc_pairs_AGG[(b)]\n",
    "    convert_to_2d_dataframe(dfthis, \"bregion\", \"var\", True, \"mean\", \"cc1\", ax=ax, annotate_heatmap=False, diverge=diverge, dosort_colnames=False)\n",
    "    ax.set_title(cc_kind) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fa816777e39092d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Summary plot across all data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c342ba27e01f522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0da2aec143af5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post-hoc plotting things realtied to sim mat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b265fcf8c35b1e05"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.state_space_good import _preprocess_rsa_scalar_population"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T01:16:00.696284697Z",
     "start_time": "2024-01-10T01:16:00.656766396Z"
    }
   },
   "id": "c175f7cd8ef1055d"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "RES[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae665d431d40b35"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "\n",
    "version_distance = \"pearson\"\n",
    "RES, SAVEDIR_MULT, params, list_bregion = load_mult_data_helper(animal, DATE, version_distance)\n",
    "\n",
    "resthis =[]\n",
    "resthis = []\n",
    "for res in RES:\n",
    "    for bregion in list_bregion:\n",
    "        for twind in res[\"list_time_windows\"]:\n",
    "            key = (bregion, twind)\n",
    "            PA = res[\"DictBregionTwindPA\"][key]\n",
    "            # Clraw = res[\"DictBregionTwindClraw\"][key]\n",
    "            # Clsim = res[\"DictBregionTwindClsim\"][key]\n",
    "            EFFECT_VARS = res[\"EFFECT_VARS\"]\n",
    "    \n",
    "            # Norm of activity for each level of the variable. Then average these norms over the levels.    \n",
    "            for var in EFFECT_VARS:\n",
    "                # Extract raw data\n",
    "                _, _, Clraw, _, PAagg = _preprocess_rsa_scalar_population(PA, [var], res[\"version_distance\"], \n",
    "                                                  res[\"subtract_mean_each_level_of_var\"],\n",
    "                                                  PLOT = False, sdir=sdir, PLOT_THEORETICAL_SIMMATS=False,\n",
    "                                                COMPUTE_SAME_DIFF_DIST = False, COMPUTE_VS_THEOR_MAT = False)\n",
    "                \n",
    "                norms = np.linalg.norm(Clraw.Xinput, axis=1)\n",
    "                # Then take avereage\n",
    "                np.mean(norms)\n",
    "                \n",
    "                # break it into levels of responses\n",
    "                for normthis, lev in zip(norms, Clraw.Labels):\n",
    "                    assert len(lev)==1, \"should be 1-tuple\"\n",
    "                    resthis.append({\n",
    "                        \"bregion\":bregion,\n",
    "                        \"twind\":twind,\n",
    "                        \"which_level\":res[\"which_level\"],\n",
    "                        \"var\":var,\n",
    "                        \"var_level\":lev[0],\n",
    "                        \"norm_pop_vec\":normthis\n",
    "                    })    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fdbc75a8cd9d4bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# correlation between these variables\n",
    "# TODO: extract values from within _preprocess_rsa_scalar_population to do this.\n",
    "\n",
    "# %matplotlib inline\n",
    "# Cltheor = Clsim.rsa_distmat_construct_theoretical(\"shape_oriented\")[0]\n",
    "# Cltheor.rsa_plot_heatmap()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f04cde01ad846c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute effect size of a given variable"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fc3bd2b6382409e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a3a9c5deb26e3324"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gorilla1/analyses/recordings/main/RSA/Pancho/MULT/230626/pearson/effects_each_var\n"
     ]
    }
   ],
   "source": [
    "savedir = f\"{SAVEDIR_MULT}/effects_each_var\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "print(savedir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T03:08:27.269995465Z",
     "start_time": "2024-01-10T03:08:26.992190352Z"
    }
   },
   "id": "20b7a1010e277cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "Clraw.rsa_labels_extract_var_levels()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c683fefd6cc2a979"
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "\n",
    "version_distance = \"pearson\"\n",
    "RES, SAVEDIR_MULT, params, list_bregion = load_mult_data_helper(animal, DATE, version_distance)\n",
    "\n",
    "resthis =[]\n",
    "resthis = []\n",
    "for res in RES:\n",
    "    for bregion in list_bregion:\n",
    "        for twind in res[\"list_time_windows\"]:\n",
    "            key = (bregion, twind)\n",
    "            print(\"Collecting data for:\", key)\n",
    "            PA = res[\"DictBregionTwindPA\"][key]\n",
    "            # Clraw = res[\"DictBregionTwindClraw\"][key]\n",
    "            # Clsim = res[\"DictBregionTwindClsim\"][key]\n",
    "            EFFECT_VARS = res[\"EFFECT_VARS\"]\n",
    "    \n",
    "            # Norm of activity for each level of the variable. Then average these norms over the levels.    \n",
    "            for var in EFFECT_VARS:\n",
    "                # condition this on each of the other vars\n",
    "                for var_other in EFFECT_VARS:\n",
    "                    if var==var_other:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract raw data\n",
    "                    subtract_mean_each_level_of_var = var_other\n",
    "                    _, _, Clraw, _, PAagg = _preprocess_rsa_scalar_population(PA, [var, var_other], res[\"version_distance\"], \n",
    "                                                      subtract_mean_each_level_of_var,\n",
    "                                                      PLOT = False, sdir=sdir, PLOT_THEORETICAL_SIMMATS=False,\n",
    "                                                    COMPUTE_SAME_DIFF_DIST = False, COMPUTE_VS_THEOR_MAT = False)\n",
    "                    \n",
    "                    # Norms is currently one val for each (var, var_other)\n",
    "                    norms = np.linalg.norm(Clraw.Xinput, axis=1)\n",
    "\n",
    "                    # get the mean for each lev of var, averaged over all othervar. \n",
    "                    # this is ok ebcuase alrraedy subtracted out effect of each ohervar above.\n",
    "                    list_lev = Clraw.rsa_labels_extract_var_levels()[var]\n",
    "                    for lev in list_lev:\n",
    "                        # get rows with this level\n",
    "                        inds_ = Clraw.rsa_index_rows_with_this_level(var, lev)\n",
    "                        norm_this = np.mean(norms[inds_])\n",
    "                        \n",
    "                        resthis.append({\n",
    "                            \"bregion\":bregion,\n",
    "                            \"twind\":twind,\n",
    "                            \"which_level\":res[\"which_level\"],\n",
    "                            \"var\":var,\n",
    "                            \"var_level\":lev,\n",
    "                            \"norm_pop_vec\":normthis\n",
    "                        })    \n",
    "                                            \n",
    "                    # # break it into levels of responses\n",
    "                    # for normthis, lev in zip(norms, Clraw.Labels):\n",
    "                    #     assert len(lev)==1, \"should be 1-tuple\"\n",
    "                    #     resthis.append({\n",
    "                    #         \"bregion\":bregion,\n",
    "                    #         \"twind\":twind,\n",
    "                    #         \"which_level\":res[\"which_level\"],\n",
    "                    #         \"var\":var,\n",
    "                    #         \"var_level\":lev[0],\n",
    "                    #         \"norm_pop_vec\":normthis\n",
    "                    #     })    \n",
    "\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index, plot_subplots_heatmap\n",
    "dfres_norm_levs = pd.DataFrame(resthis)\n",
    "dfres_norm_levs\n",
    "dfres_norm_levs = append_col_with_grp_index(dfres_norm_levs, [\"var_level\", \"which_level\", \"twind\"], \"vl_wl_tw\", strings_compact=True)\n",
    "dfres_norm_levs = append_col_with_grp_index(dfres_norm_levs, [\"which_level\", \"twind\"], \"wl_tw\", strings_compact=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbd40a22f485436"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for each level of othervar, slice out just those rows.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57f5a6aff2609332"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "list_lev_other = PA.Xlabels[\"trials\"][var_other].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63495f72323e39ca"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "Clraw.Labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "891a054278e63ef9"
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "var = \"gap_to_next_angle_binned\"\n",
    "var_other = \"stroke_index_semantic\"\n",
    "subtract_mean_each_level_of_var = var_other\n",
    "_, _, Clraw, _, PAagg = _preprocess_rsa_scalar_population(PA, [var, var_other], res[\"version_distance\"], \n",
    "                                  subtract_mean_each_level_of_var,\n",
    "                                  PLOT = False, sdir=sdir, PLOT_THEORETICAL_SIMMATS=False,\n",
    "                                COMPUTE_SAME_DIFF_DIST = False, COMPUTE_VS_THEOR_MAT = False)\n",
    "\n",
    "Clraw.rsa_plot_heatmap(diverge=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf6f6016578bceef"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "for lev_other in list_lev_other:\n",
    "    inds = Clraw.rsa_index_rows_with_this_level(var_other, lev_other)\n",
    "    print(lev_other, inds)\n",
    "    levs = Clraw.rsa_index_values_this_var(var, inds)\n",
    "    \n",
    "    # "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1f20989554d52"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "levs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ba54597bc42738"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Summarize all. datapt is (bregion, var)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53c6206d2e8b4a1a"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# Each subplot is a bregion... \n",
    "# for norm_method in [None, \"all_sub\", \"col_sub\", \"row_sub\"]:\n",
    "for norm_method in [None]:\n",
    "    for var in list_var:\n",
    "        dfthis = dfres_norm_levs[dfres_norm_levs[\"var\"]==var]\n",
    "        fig, axes = plot_subplots_heatmap(dfthis, \"var_level\", \"wl_tw\", \"norm_pop_vec\", \"bregion\", share_zlim=False, norm_method=None)\n",
    "        savefig(fig, f\"{savedir}/heatmaps_varlevels-subplot_bregion-var_{var}-norm_{norm_method}.pdf\")\n",
    "        plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e348fd4edf647fa9"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Each subplot a variable level. this doesnt work well since big diff across bregions in fr.\n",
    "    norm_method = \"row_sub\"\n",
    "    for var in list_var:\n",
    "        dfthis = dfres_norm_levs[dfres_norm_levs[\"var\"]==var]\n",
    "        fig, axes = plot_subplots_heatmap(dfthis, \"bregion\", \"wl_tw\", \"norm_pop_vec\", \"var_level\", share_zlim=True, norm_method=norm_method)\n",
    "        plt.close(\"all\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4562a8dbc6a8474f"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "# Average over levels\n",
    "### SINGLE SUMMARY:\n",
    "for norm_method in [None, \"all_sub\", \"col_sub\", \"row_sub\"]:\n",
    "    fig, axes = plot_subplots_heatmap(dfres_norm_levs, \"var\", \"wl_tw\", \"norm_pop_vec\", \"bregion\", share_zlim=True, \n",
    "                                      norm_method=norm_method)\n",
    "    savefig(fig, f\"{savedir}/heatmaps_meanOverLevs_varlevels-subplot_bregion-norm_{norm_method}.pdf\")\n",
    "    plt.close(\"all\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0858f687a0477c1"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "# for each variable, plot its activity for each level\n",
    "from pythonlib.tools.listtools import stringify_list\n",
    "from pythonlib.tools.snstools import rotateLabel\n",
    "list_var = EFFECT_VARS\n",
    "# S = 3\n",
    "# ncols = len(list_var)\n",
    "# nrows = int(np.ceil(len(list_var)/ncols))\n",
    "# fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*S, nrows*S), sharex=False, sharey=True)\n",
    "\n",
    "for var, ax in zip(list_var, axes.flatten()):\n",
    "    \n",
    "    dfthis = dfres_norm_levs[dfres_norm_levs[\"var\"]==var]\n",
    "    fig = sns.catplot(data=dfthis, x=\"wl_tw\", y=\"norm_pop_vec\", col=\"bregion\", hue=\"var_level\", kind=\"point\")\n",
    "    rotateLabel(fig)\n",
    "    savefig(fig, f\"{savedir}/pointplot-alllevels-{var}.pdf\")\n",
    "\n",
    "    # levs = dfthis[\"var_level\"].tolist()\n",
    "    # norms = dfthis[\"norm_pop_vec\"].values\n",
    "    # \n",
    "    # levs = stringify_list(levs)\n",
    "    # \n",
    "    # ax.plot(levs, norms, \"-o\", label=lev)\n",
    "    # ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "    # ax.legend()\n",
    "    # ax.set_title(var)\n",
    "    # rotate_x_labels(ax, 70)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f519df129c143d1"
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "\n",
    "fig = sns.catplot(data=dfres_norm_levs, x=\"wl_tw\", y=\"norm_pop_vec\", hue=\"var\", col=\"bregion\", kind=\"point\")\n",
    "rotateLabel(fig)\n",
    "savefig(fig, f\"{savedir}/pointplot-meanOverLevs.pdf\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43883c53b0ad0d5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute decodability of the variable"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9c5866757bd233f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Relates to how separate are the activity patterns.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27df84c348c08b64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
