{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Quick analysi of neural data, 6/22, for quick look for K99 ,\n",
    "mainly for making rasters, pulling out interesting units, etc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load neural and beh data (pending: cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import Session\n",
    "import matplotlib.pyplot as plt\n",
    "from neuralmonkey.scripts.load_and_save_locally import load_and_preprocess_single_session\n",
    "\n",
    "import neuralmonkey.utils.monkeylogic as mkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_preprocess_single_session(220703, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Loading these beh expts: ['primsingrid2']\n",
      "Loading these beh sessions: [1]\n",
      "Loading this neural session: 0\n",
      "Searching using this string:\n",
      "/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220713*/**\n",
      "Found this many paths:\n",
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "maybe you didn't mount server?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30683/1774154469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     SN = Session(DATE, beh_expt_list, beh_sess_list, beh_trial_map_list, sites_garbage=sites_garbage,\n\u001b[0;32m---> 41\u001b[0;31m                 rec_session = rec_session)\n\u001b[0m",
      "\u001b[0;32m/data1/code/python/neuralmonkey/neuralmonkey/classes/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datestr, beh_expt_list, beh_sess_list, beh_trial_map_list, sites_garbage, expt, animal, path_base, path_local, rec_session, do_all_copy_to_local, do_sanity_checks, dataset_beh_expt)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# self._initialize_params()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"== PATHS for this expt: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/code/python/neuralmonkey/neuralmonkey/classes/session.py\u001b[0m in \u001b[0;36m_initialize_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecPathBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_hierarchy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# assert len(paths)==1, 'not yhet coded for combining sessions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maybe you didn't mount server?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mpaththis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecSession\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: maybe you didn't mount server?"
     ]
    }
   ],
   "source": [
    "animal = \"Pancho\"\n",
    "expt = \"*\"\n",
    "# DATE = \"220608\"\n",
    "DATE = \"220713\"\n",
    "rec_session = 0 # assumes one-to-one mapping between neural and beh sessions.\n",
    "# dataset_beh_expt = \"charnovel3\"\n",
    "# dataset_beh_expt = \"charneuraltrain1\"\n",
    "dataset_beh_expt = \"priminvar\"\n",
    "\n",
    "if DATE==\"220532\":\n",
    "    # For K99, quick analy\n",
    "    # Beh (ml2)\n",
    "    beh_expt_list = [\"charneuraltrain1b\"]\n",
    "    beh_sess_list = [1]\n",
    "    beh_trial_map_list = [(1,0)]\n",
    "    sites_garbage = [20, 26, 36, 44, 46, 56, 67, 69, 78, 98, 100, \n",
    "                          1+256, 2+256, 10+256, 91+256, 127+256]    \n",
    "    SN = Session(DATE, beh_expt_list, beh_sess_list, beh_trial_map_list, sites_garbage=sites_garbage)\n",
    "else:\n",
    "    # General purpose\n",
    "    beh_session = rec_session+1 # 1-indexing.\n",
    "    sessdict = mkl.getSessionsList(animal, datelist=[DATE])\n",
    "    if False:\n",
    "        # get all sessions\n",
    "        beh_expt_list = [sess_expt[1] for sess_expt in sessdict[DATE]]\n",
    "        beh_sess_list = [sess_expt[0] for sess_expt in sessdict[DATE]]\n",
    "    else:\n",
    "        # Get the single session assued to map onto this neural.\n",
    "        beh_expt_list = [sess_expt[1] for sess_expt in sessdict[DATE] if sess_expt[0]==beh_session]\n",
    "        assert(len(beh_expt_list))==1, \"must be error, multiple sessions with same session num\"\n",
    "        beh_sess_list = [beh_session]\n",
    "\n",
    "    beh_trial_map_list = [(1, 0)]\n",
    "    sites_garbage = None\n",
    "\n",
    "    print(\"Loading these beh expts:\", beh_expt_list)\n",
    "    print(\"Loading these beh sessions:\",beh_sess_list)\n",
    "    print(\"Loading this neural session:\", rec_session)\n",
    "\n",
    "    SN = Session(DATE, beh_expt_list, beh_sess_list, beh_trial_map_list, sites_garbage=sites_garbage,\n",
    "                rec_session = rec_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Loading datall from local (previusly cached)\n",
      "** Loading _MapperSiteTrial2DatAllInd from local (previusly cached)\n",
      "Saving DatAll (raw and spikes) locally to:  /data3/recordings/Pancho/220608/Pancho-220608-143122/data_datall.pkl\n",
      "Saving _MapperSiteTrial2DatAllInd locally to:  /data3/recordings/Pancho/220608/Pancho-220608-143122/mapper_st2dat.pkl\n"
     ]
    }
   ],
   "source": [
    "# get all trials spikes (takes a while...)\n",
    "# SN.DatAll = None\n",
    "# SN._MapperSiteTrial2DatAllInd = {}\n",
    "\n",
    "SN.extract_raw_and_spikes_helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/analyses/recordings/NOTEBOOKS/quickanaly_220603/Pancho/220608/Pancho-220608-143122\n"
     ]
    }
   ],
   "source": [
    "SAVEDIR = f\"/data2/analyses/recordings/NOTEBOOKS/quickanaly_220603/{SN.Animal}/{SN.Date}/{SN.Paths['final_dir_name']}\"\n",
    "import os\n",
    "print(SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract raw data for a single trial\n",
    "\n",
    "NOTE: if want to consider all trial, ignore raw, too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialtdt = 5\n",
    "\n",
    "SN.extract_raw_and_spikes([2, 3], list(range(1, 257)), trialtdt, False)\n",
    "# SN.extract_raw_and_spikes([2, 3], list(range(1, 257)), trialtdt, False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.print_summarize_datall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "##### Plot all for this trial, including raw neural "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "SN.plot_specific_trial_overview(trialtdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single trial: Plot all rasters and beh and drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = f\"{SAVEDIR}/PLOTS/rasters_singletrial_allsites\"\n",
    "os.makedirs(sdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = 625\n",
    "sites = SN.sitegetter_all()\n",
    "# sites = SN.sitegetter_all([\"M1_m\", \"M1_l\"])\n",
    "fig, axes = SN.plot_raster_oneetrial_multsites(trial, sites, WIDTH=15, HEIGHT=14)\n",
    "fig.savefig(f\"{sdir}/trial_{trial}.pdf\")\n",
    "fig.savefig(f\"{sdir}/trial_{trial}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given list of sites (yaxis), modify plot to indicate the brain region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute and normalize FR (diff ways) for easier comparison across units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out a few trials\n",
    "list_trials_ml2 = [5, 10, 20, 50, 100, 110, 316, 327, 329,  622, 624, 626, 608, 612, 674]\n",
    "list_trials = [x-1 for x in list_trials_ml2]\n",
    "for t in list_trials:\n",
    "    SN.extract_raw_and_spikes([2,3], list(range(1,257)), t, get_raw=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out range of many trials\n",
    "\n",
    "list_trials_good = [315, 326, 328,  621, 623, 625, 607, 611, 673]\n",
    "list_trials_others = [t-1 for t in list_trials_good]\n",
    "list_trials_others2 = [t-2 for t in list_trials_good]\n",
    "list_trials = list_trials_good + list_trials_others + list_trials_others2\n",
    "print(list_trials)\n",
    "for t in list_trials:\n",
    "    SN.extract_raw_and_spikes([2,3], list(range(1,257)), t, get_raw=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even larger range of trials\n",
    "list_trials = range(300, 700)\n",
    "print(list_trials)\n",
    "for t in list_trials:\n",
    "    SN.extract_raw_and_spikes([2,3], list(range(1,257)), t, get_raw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One plot per trial and brain region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = f\"/data2/analyses/recordings/NOTEBOOKS/quickanaly_220603/{SN.Animal}/{SN.Date}/{SN.Animal}-{SN.Date}-{SN.Paths['time']}\"\n",
    "import os\n",
    "print(SAVEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = f\"{SAVEDIR}/PLOTS/rasters_trial_by_region_all\"\n",
    "os.makedirs(sdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion_mapper = SN.sitegetter_brainregion(\"mapper\", clean=True)\n",
    "for trial in list_trials:\n",
    "    for region, sites in bregion_mapper.items():\n",
    "        try:\n",
    "            # sites = SN.sitegetter_brainregion(\"FP_p\")\n",
    "            # sites = SN.sitegetter_brainregion(\"dlPFC_a\")\n",
    "            # sites = SN.sitegetter_brainregion(\"preSMA_a\")\n",
    "            fig, axes = SN.plot_raster_onetrial_multsites(trial, sites)\n",
    "\n",
    "            fig.savefig(f\"{sdir}/{region}-trial_{trial}.pdf\")\n",
    "        except Exception as err:\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One plot per trial and hand-picked units across regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One plot for a unit across trials,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SORT_TRIALS_BY = \"duration\"\n",
    "\n",
    "site = 177 # vlpfc\n",
    "# site = 312\n",
    "list_trials = [315, 326, 328,  338, 347, 362, 487, 554, 621, 623, 625, 607, 611, 673, 651, 658, 682, 692, 710]\n",
    "\n",
    "if SORT_TRIALS_BY==\"duration\":\n",
    "    # Sort trials by duration (go to done)\n",
    "    def calc_trial_dur(trial):\n",
    "        go = SN.behcode_extract_times(\"go\", trial, shorthand=True, first_instance_only=True)\n",
    "        done = SN.behcode_extract_times(\"done\", trial, shorthand=True, first_instance_only=True)\n",
    "        return done-go\n",
    "\n",
    "    list_durs = [calc_trial_dur(trial) for trial in list_trials]\n",
    "\n",
    "    tmp = [(t, d) for t, d in zip(list_trials, list_durs)]\n",
    "    tmp = sorted(tmp, key=lambda x: x[1])\n",
    "    list_trials = [x[0] for x in tmp]\n",
    "elif SORT_TRIALS_BY==\"chron\":\n",
    "    list_trials = sorted(list_trials)\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "# How to align them?\n",
    "alignto = 'go'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure extracted all spikes for these trials\n",
    "SN.extract_raw_and_spikes_helper(list_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sites = {\n",
    "    \"dlPFC_a\":[255, 243, 231],\n",
    "    \"dlPFC_p\":[222, 215, 198],\n",
    "    \"FP_a\":[97, 119],\n",
    "    \"FP_p\":[77, 92, 95, 96], \n",
    "    \"M1_m\":[31, 29, 13],\n",
    "    \"M1_l\":[39, 43, 45, 57, 63],\n",
    "    \"PMd_a\":[175, 191, 177, 179, 181, 172, 187],\n",
    "    \"PMv_l\":[66, 90, 72, 77],\n",
    "    \"preSMA_a\":[227, 256, 246, 253, 246],\n",
    "    \"preSMA_p\":[221, 212, 213, 193],\n",
    "    \"SMA_a\":[173, 179, 181],\n",
    "    \"SMA_p\":[151, 140],\n",
    "    \"vlPFC_a\":[33, 47, 56, 38, 56, 64],\n",
    "    \"vlPFC_p\":[5, 16, 22], \n",
    "}\n",
    "\n",
    "\n",
    "# convert to actual sites\n",
    "list_sites = []\n",
    "for region, chans in dict_sites.items():\n",
    "    for ch in chans:\n",
    "        list_sites.append(SN.sitegetter_brainregion_chan(region, ch))\n",
    "list_sites = sorted(list_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = f\"{SAVEDIR}/PLOTS/rasters_multtrials_by_site_handpicked_sortby_{SORT_TRIALS_BY}\"\n",
    "os.makedirs(sdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_alignto = [\"on\", \"fixcue\", \"fix\", \"samp\", \"go\", \"done\", \"fb_vs\", \"rew\",\n",
    "                (\"strokes\", 0, \"on\"), (\"strokes\", -1, \"off\"),\n",
    "                (\"strokes\", 0, \"off\")\n",
    "               ]\n",
    "# list_alignto = [(\"strokes\", -1, \"off\"),\n",
    "#                 (\"strokes\", 0, \"off\")\n",
    "#                ]\n",
    "saved_draw=False\n",
    "for site in list_sites:\n",
    "    for alignto in list_alignto:\n",
    "        fig1, _, fig2, _ = SN.plot_raster_multrials_onesite(list_trials, site=site, alignto=alignto)\n",
    "        \n",
    "        prefix = SN.sitegetter_summarytext(site)\n",
    "        fig1.savefig(f\"{sdir}/{prefix}-alignto_{alignto}-rasters.pdf\")\n",
    "        \n",
    "        if saved_draw==False:\n",
    "            saved_draw=True\n",
    "            fig2.savefig(f\"{sdir}/drawings.pdf\")\n",
    "            \n",
    "        plt.close(\"all\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, but ordering trials by the duiration (go to done)\n",
    "\n",
    "list_alignto = [\"on\", \"fixcue\", \"fix\", \"samp\", \"go\", \"done\", \"fb_vs\", \"rew\",\n",
    "                (\"strokes\", 0, \"on\"), (\"strokes\", -1, \"off\"),\n",
    "                (\"strokes\", 0, \"off\")\n",
    "               ]\n",
    "# list_alignto = [(\"strokes\", -1, \"off\"),\n",
    "#                 (\"strokes\", 0, \"off\")\n",
    "#                ]\n",
    "saved_draw=False\n",
    "for site in list_sites:\n",
    "    for alignto in list_alignto:\n",
    "        fig1, _, fig2, _ = SN.plot_raster_multrials_onesite(list_trials, site=site, alignto=alignto)\n",
    "        \n",
    "        prefix = SN.sitegetter_summarytext(site)\n",
    "        fig1.savefig(f\"{sdir}/{prefix}-alignto_{alignto}-rasters.pdf\")\n",
    "        \n",
    "        if saved_draw==False:\n",
    "            saved_draw=True\n",
    "            fig2.savefig(f\"{sdir}/drawings.pdf\")\n",
    "            \n",
    "        plt.close(\"all\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOOTHED FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian kernel smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.DatAll[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) bin spikes\n",
    "\n",
    "\n",
    "# 2) gaussian kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Elephant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for d in SN.DatAll:\n",
    "#         spike_times = datspikes_slice_single(d[\"rs\"], d[\"chan\"], d[\"time_range\"])\n",
    "    print(d[\"trial0\"])\n",
    "    spiketimes, time_dur, time_on, time_off = SN.datspikes_slice_single(d[\"rs\"], d[\"chan\"], trial0=d[\"trial0\"])\n",
    "    d[\"spike_times\"] = spiketimes\n",
    "    d[\"time_dur\"] = time_dur\n",
    "    d[\"time_on\"] = time_on\n",
    "    d[\"time_off\"] = time_off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.sitegetter_all(clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all for a single trial\n",
    "trial = 600\n",
    "fig, ax = plt.subplots(1,1)\n",
    "SN.plot_trial_timecourse_summary(ax, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on one trial for now, get all sites.\n",
    "\n",
    "trial = 600\n",
    "for site in SN.sitegetter_all(clean=False):\n",
    "    print(site)\n",
    "    SN.spiketrain_as_elephant(site, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.spiketrain_as_elephant_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all spike trains for a trial\n",
    "\n",
    "list_sites = SN.sitegetter_all(clean=False)\n",
    "list_spiketrain = []\n",
    "for site in list_sites:\n",
    "    SN.spiketrain_as_elephant(site, trial)\n",
    "    dat = SN.datall_slice_single_bysite(site,trial)\n",
    "    list_spiketrain.append(dat[\"spiketrain\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant.kernels import GaussianKernel\n",
    "frate = instantaneous_rate(list_spiketrain, sampling_period=0.01*s, kernel=GaussianKernel(0.1*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frate.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scratch (for smoothing spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "# # plotting the original spiketrain\n",
    "# plt.plot(st_train, [0]*len(st_train), 'r', marker=2, ms=25, markeredgewidth=2, lw=0, label='poisson spike times')\n",
    "\n",
    "# # mean firing rate\n",
    "# plt.hlines(mean_firing_rate(st_train), xmin=st_train.t_start, xmax=st_train.t_stop, linestyle='--', label='mean firing rate')\n",
    "\n",
    "# # time histogram\n",
    "# plt.bar(histthis.times, histthis.magnitude.flatten(), width=histthis.sampling_period, align='edge', alpha=0.3, label='time histogram (rate)')\n",
    "\n",
    "# instantaneous rate\n",
    "# plt.plot(frate.times, frate.rescale(histthis.dimensionality).magnitude.flatten(), label='instantaneous rate')\n",
    "plt.plot(frate.times, frate.magnitude[:,site], label='instantaneous rate')\n",
    "\n",
    "# axis labels and legend\n",
    "plt.xlim(frate.t_start, frate.t_stop)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram\n",
    "\n",
    "from elephant.statistics import time_histogram, instantaneous_rate,mean_firing_rate\n",
    "from quantities import s\n",
    "\n",
    "bindur = 0.1 * s\n",
    "histthis = time_histogram(list_spiketrain, bindur, output=\"rate\")\n",
    "\n",
    "print(type(histthis), f\"of shape {histthis.shape}: {histthis.shape[0]} samples, {histthis.shape[1]} channel\")\n",
    "print('sampling rate:', histthis.sampling_rate)\n",
    "print('times:', histthis.times)\n",
    "print('counts:', histthis.T[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant.kernels import GaussianKernel\n",
    "frate = instantaneous_rate(list_spiketrain, sampling_period=0.02*s, kernel=GaussianKernel(0.1*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frate.rescale(frate.dimensionality).magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 100\n",
    "frate.magnitude[:,site]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check spike waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = f\"{SAVEDIR}/PLOTS/spike_waveforms\"\n",
    "os.makedirs(sdir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.SitesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "SN.plot_spike_waveform(ax, waveforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = SN.load_spike_waveforms_(rs=2, chan=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Plot waveforms for all channels in a grid\n",
    "RS = 2\n",
    "CHAN = 218\n",
    "YLIM = [-250, 100]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "# 1) Waveforms\n",
    "ax = axes.flatten()[0]\n",
    "waveforms = SN.load_spike_waveforms_(rs=RS, chan=CHAN)\n",
    "SN.plot_spike_waveform(ax, waveforms)\n",
    "ax.set_ylim(YLIM)\n",
    "\n",
    "# 2) min and max voltages\n",
    "outdict = SN.spikewave_compute_stats(waveforms)\n",
    "ax = axes.flatten()[1]\n",
    "ax.hist(outdict[\"volt_max\"], bins=100);\n",
    "ax.hist(outdict[\"volt_min\"], bins=100);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.plot_spike_waveform_stats_multchans(XLIM=[-400, 200])\n",
    "SN.plot_spike_waveform_multchans(LIST_YLIM=[YLIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN.DatSpikeWaveforms = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect stats about each site, to autoamtically remove bad sites\n",
    "\n",
    "for site in SN.sitegetter_all(clean=False):\n",
    "    rs, chan = SN.convert_site_to_rschan(site)\n",
    "    wf = SN.load_spike_waveforms_(rs, chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE DOWN sites with weak/no activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_weak = [1,2,3,4,6, 8, 18, 19, 21, 24, 25, 26,\n",
    "              33, 35, 36, 38, 44, 46, 48, 50, 64, \n",
    "             65, 67, 68, 69, 71, 72, 75, 76, 77, 81, 85, 86, 87, \n",
    "              89, 92, 93, 95\n",
    "              100, 104, 105, 110, 111, 115, 120, 122, 127, 128,\n",
    "              129, 131, 133, 134, 137, 139, 147, 149, 152, 155, \n",
    "              156, 157, 159, 163, 166, 168, 174, 182, 185, 190, 191,\n",
    "              196, 199, 201, 202, 204, 206, 208, 212, 214, 222,\n",
    "              224, 230, 234, 238, 242, 251, \n",
    "              258, 266, 268, 284, 286, 289, 291, 292, 293, 294, 295, \n",
    "              297, 298, 305, 306, 307, 308, 311, 313, 315, 317, 318, 319, \n",
    "              322, 323, 324, 325, 326, 328, 329, 330, 336, \n",
    "              338, 341, 342, 343, 345, 350, 355, 357, 359, 361, 362,363, \n",
    "              364, 365,  355, 357, 359, 361, 362, 363, 364,366,\n",
    "              370, 372. 375, 376, 377, 378, 379, 381, 382, 383, 384,\n",
    "              391, 418, 424, 426, 427, 432, 440, 444, 446, 448,\n",
    "              476, 482, 507]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 220608\n",
    "# sites_garbage = [(2,12), (2,21), (2,34), (2,36), (2,50), (2,67), (2,68), (2,69), \n",
    "sites_garbage = [12, 21, 26, 34, 36, 46, 48, 50, 60, 65, 67, 68, 69, 70, 73, 77, 78, 82, 83, 84, 85, 87, 88, 89, \n",
    "                 91, 92, 93, 95, 96, 97, 100, 101, 109, 115, 122, 127, 129, 131, 133,  137, 139, 142, 143, \n",
    "                 145, 147, 149, 151, 152, 155, 157, 160, 168, 182, 183, 184, 190, 192, 196, 204, 206, 210, \n",
    "                 226, 234, 250, 262, 264, 266, 268, 287, 289, 290, 291, 292, 295, 298, 304, \n",
    "                 306, 307, 308, 309, 311, 313, 315, 318, 319, 321, 322, 323, 324, 325, 326, 330, 335, \n",
    "                 337, 338, 340, 341, 342, 348, 350, 355, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,  \n",
    "                 372, 376, 378, 379, 380, 382, 383, 384, \n",
    "                 411, 413, 419, 426, 428, 432, 440, 444, 445, 446, 448,\n",
    "                 462, 478, 482, 487, 507]\n",
    "SN.SitesGarbage = sites_garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.diff(sites_garbage)>0.), \"you made mistake entering chanels (assuming going in order)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveforms for garbage and good.\n",
    "\n",
    "# Good:\n",
    "SN.plot_spike_waveform_multchans(clean=True, LIST_YLIM=[YLIM])\n",
    "SN._plot_spike_waveform_multchans(SN.SitesGarbage, YLIM, True, \"garbage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Integrate with beh (monkeylogic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "from tools.utils import * \n",
    "from tools.plots import *\n",
    "# from tools.analy import *\n",
    "# from tools.calc import *\n",
    "# from tools.analyplot import *\n",
    "from tools.preprocess import *\n",
    "# from tools.dayanalysis import *\n",
    "# from analysis.line2 import *\n",
    "# from analysis.probedatTaskmodel import *\n",
    "# from pythonlib.drawmodel.analysis import *\n",
    "# from pythonlib.tools.stroketools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filedata\n",
    "# Load filedata, quick\n",
    "a = \"Pancho\"\n",
    "d = 220531\n",
    "e = \"charneuraltrain1b\"\n",
    "s = 1\n",
    "\n",
    "fd = loadSingleDataQuick(a, d, e, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tank = tdt.read_block(PATH_TANK, evtype = [\"epocs\", \"streams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # tdt.epoc_filter(data, 'SMa1', values=[9., 18.])\n",
    "    tdt.epoc_filter(data_tank, 'SMa1', t = [-0.1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RS4 starts at a later sample, so account for that\n",
    "RS4_SAMPLE_ADD = 1 # 1 means RS4 started 1 sample later.\n",
    "# TODO: add this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_behcode_times_sequence([9, 11, 16, 91, 92, 62, 73, 50], trial0+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subset of chans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label by brain region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized FR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For each unit, plot across N (hand picked trials) and save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list chans (chans to overlay)\n",
    "\n",
    "\n",
    "# ===  5/31\n",
    "# for plotting exmaples, \n",
    "list_good_sites = [\n",
    "    6, 7, 10, 12, 13]\n",
    "\n",
    "# Sites that should not even inlcude in raster because of noise\n",
    "# not because of low FR.\n",
    "list_sites_garbage = [20, 26, 36, 44, 46, 56, 67, 69, 78, 98, 100, \n",
    "                      1+256, 2+256, 10+256, 91+256, 127+256]    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SN.sitegetter_brainregion(\"M1_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sites_all = range(1, 512)\n",
    "list_sites_all = [s for s in list_sites_all if s not in list_sites_garbage]\n",
    "\n",
    "print(len(list_sites_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = f\"{SAVEDIR}/rasters_region_by_trials\"\n",
    "os.makedirs(sdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go thru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small set of trials, one plot for each site\n",
    "\n",
    "plot_trial_timecourse_summary("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 1 plot for each area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
