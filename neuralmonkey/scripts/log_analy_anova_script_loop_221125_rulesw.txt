Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221125*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909
session:  0
1
Beh Sessions that exist on this date:  {221125: [(1, 'dirshapecolor4b')]}
taking this beh session: 1
Loading these beh expts: ['dirshapecolor4b']
Loading these beh sessions: [1]
Loading this neural session: 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221125*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909
{'filename_components_hyphened': ['Pancho', '221125', '131909'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/221125'], 'basedirs_filenames': ['221125', 'Pancho-221125-131909'], 'filename_final_ext': 'Pancho-221125-131909', 'filename_final_noext': 'Pancho-221125-131909'}
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909/Pancho-221125-131909
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909/spikes_tdt_quick-4
final_dir_name  --  Pancho-221125-131909
time  --  131909
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/221125.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
o  1 110.70015106201174
-  2 69.10210723876953
-  3 68.57892150878907
o  4 134.8060302734375
o  5 76.63243865966797
o  6 75.66429061889649
o  7 134.07001647949218
-  8 63.589978027343754
o  9 251.26464233398437
o  10 98.07416534423828
o  11 132.20796508789064
-  12 69.47706832885743
o  13 140.5678466796875
-  14 53.41653900146484
o  15 169.58192443847656
o  16 81.9468475341797
o  17 77.63435974121094
o  18 104.25444946289063
o  19 96.09076919555665
-  20 42.508761215209965
o  21 120.13069839477541
o  22 214.58809509277347
-  23 45.68513412475586
o  24 74.81740951538086
o  25 73.57882080078126
o  26 580.3622314453125
o  27 102.81129302978516
o  28 71.16385803222656
o  29 255.68379974365234
o  30 98.35868911743165
o  31 156.63096923828127
o  32 70.74853210449218
o  33 79.14913330078124
o  34 101.5077491760254
-  35 41.81251068115235
o  36 166.95777893066406
-  37 60.5641658782959
o  38 75.84748992919923
o  39 129.9774383544922
o  40 160.30959930419922
o  41 88.66414031982421
o  42 97.5841682434082
o  43 123.66165466308594
o  44 71.22148742675782
o  45 251.99578857421875
-  46 51.70357437133789
o  47 150.3605758666992
-  48 43.63840522766113
o  49 100.45581359863282
o  50 76.19973373413086
o  51 93.96791076660156
-  52 68.32694778442382
o  53 725.7648376464844
o  54 80.09454956054688
o  55 163.62498168945314
-  56 60.89304084777832
o  57 246.76725006103518
-  58 67.1504409790039
o  59 89.30016860961913
o  60 90.24994888305665
o  61 85.6065544128418
o  62 80.9630599975586
o  63 248.70314025878906
o  64 117.72973937988284
o  65 81.29031219482422
o  66 104.23651123046875
o  67 102.87630615234376
o  68 94.12977600097656
o  69 73.97239151000977
-  70 52.247196960449216
o  71 71.68388290405274
o  72 156.85655364990237
o  73 72.7403678894043
o  74 72.37607498168946
o  75 75.03299865722657
o  76 108.15685424804688
-  77 63.31049156188965
o  78 85.38916625976563
-  79 60.5265251159668
o  80 151.6734130859375
-  81 58.64752883911133
o  82 92.2702621459961
-  83 64.20382156372071
-  84 49.89320945739746
-  85 62.70757141113281
-  86 69.28522186279297
-  87 46.61496124267578
o  88 88.66550521850586
-  89 56.200006103515626
o  90 93.35928039550781
o  91 70.53487548828124
-  92 48.67988395690918
-  93 64.94072723388672
o  94 74.74431838989258
o  95 73.21788711547852
o  96 88.53709030151367
o  97 172.1494155883789
o  98 80.27128829956055
o  99 103.43579330444337
o  100 84.92826538085937
-  101 64.34832000732422
o  102 204.72989959716796
-  103 64.93257522583008
o  104 83.87729568481446
o  105 83.48537979125977
o  106 83.08042373657227
-  107 45.005855560302734
-  108 60.08348083496094
-  109 49.631516265869145
-  110 61.08753929138184
-  111 61.222680282592776
o  112 96.45987701416016
o  113 136.00503387451172
o  114 155.4192077636719
-  115 58.703425598144534
o  116 114.09267578125001
-  117 53.382208251953124
o  118 120.8844596862793
o  119 70.86467590332032
-  120 51.14034919738769
o  121 110.1386215209961
o  122 73.78821640014648
o  123 99.32025375366212
o  124 88.34268341064453
-  125 67.90655364990235
o  126 98.08311309814454
o  127 87.79210968017578
-  128 46.213303375244145
o  129 80.4477149963379
o  130 289.26820068359376
-  131 65.32670516967774
o  132 819.5115783691406
o  133 137.80018310546876
-  134 69.14446792602538
o  135 189.4669891357422
o  136 128.86629638671877
o  137 83.71893463134765
o  138 153.4797790527344
-  139 60.685387802124026
o  140 133.85625305175782
o  141 123.9957618713379
o  142 386.5689727783203
o  143 104.61200790405273
o  144 103.5354507446289
-  145 61.44029655456543
-  146 60.825867080688475
-  147 42.71024017333985
o  148 94.45768280029297
-  149 64.81007156372071
o  150 85.91355590820312
-  151 51.04849281311035
o  152 203.08633575439453
-  153 56.24013557434082
-  154 67.64160079956055
-  155 53.86366691589355
o  156 91.44458923339845
o  157 190.86886901855473
o  158 293.89886169433595
-  159 60.72432861328126
o  160 75.19936828613281
o  161 72.82343368530273
o  162 75.97271575927735
o  163 71.50904769897461
o  164 92.22686614990235
o  165 105.13744430541993
-  166 54.359056854248045
o  167 189.88598632812503
o  168 82.7800750732422
o  169 72.67840270996093
o  170 100.07739562988283
o  171 176.82958068847657
-  172 69.11110839843751
o  173 122.57362670898438
-  174 66.73458938598633
-  175 69.29116287231446
-  176 66.31260681152344
o  177 105.72633056640625
o  178 97.1804412841797
o  179 231.0660415649414
-  180 64.6927276611328
o  181 115.43360214233398
-  182 63.6655387878418
-  183 63.07242240905762
o  184 71.6611312866211
-  185 30.025129508972167
o  186 101.83674392700196
o  187 73.15551071166992
-  188 66.69722518920898
o  189 96.55576248168946
-  190 63.822997665405275
-  191 62.907927703857425
o  192 75.94124450683594
o  193 444.61603088378905
-  194 64.75936813354492
o  195 87.14242782592774
-  196 54.753278732299805
o  197 71.49410400390626
o  198 70.30115432739258
o  199 82.90447311401368
o  200 99.0100570678711
o  201 74.44369354248047
o  202 86.78613433837891
o  203 114.15658493041994
o  204 86.46830215454102
o  205 95.02078170776367
o  206 77.23492584228516
-  207 62.50435562133789
-  208 67.80478210449219
-  209 47.63797988891602
o  210 74.29186935424805
-  211 66.39629058837892
-  212 65.57066650390625
o  213 85.98596878051758
o  214 71.8694221496582
o  215 73.07013931274415
o  216 76.3842155456543
o  217 78.67113571166993
o  218 82.35745468139649
o  219 80.12860641479492
-  220 67.54318542480469
o  221 70.09756317138672
o  222 75.26875457763671
-  223 68.41546173095703
o  224 72.815576171875
o  225 77.94846725463869
-  226 66.56689453125
o  227 111.31159057617188
-  228 62.3229362487793
o  229 100.01448822021484
-  230 62.92288818359375
-  231 64.48743896484375
-  232 35.76767845153809
o  233 89.0982879638672
o  234 72.62473678588867
-  235 65.9875862121582
-  236 55.423601150512695
o  237 88.51481246948242
-  238 64.58454208374023
-  239 52.516048431396484
o  240 80.83622665405274
-  241 69.73650360107422
-  242 67.46670455932617
o  243 72.11400756835937
o  244 84.56570892333984
-  245 59.42482223510742
o  246 91.67399673461914
-  247 64.31516494750977
o  248 106.05211029052735
-  249 63.4588508605957
-  250 55.51843948364259
o  251 75.55241012573242
o  252 91.28190841674805
-  253 55.949310302734375
-  254 48.791921615600586
-  255 68.47396392822266
o  256 85.85845184326172
o  257 200.06482543945313
o  258 101.1684326171875
o  259 104.27263565063477
o  260 190.90434722900392
o  261 127.20354080200195
-  262 64.95013732910157
o  263 90.55942535400392
o  264 116.98455657958985
o  265 140.23272705078125
o  266 86.88879089355468
o  267 108.41556930541994
-  268 50.83239898681641
o  269 102.51695556640627
o  270 126.93021392822266
o  271 107.690731048584
o  272 155.84538574218752
o  273 84.39642944335938
o  274 83.41875457763672
o  275 94.38887329101563
o  276 138.85443572998048
o  277 106.4805763244629
o  278 95.83465042114258
o  279 89.27790298461915
o  280 176.27704467773438
o  281 80.62609024047852
o  282 90.9362159729004
o  283 70.51200942993164
o  284 137.08782653808595
o  285 114.543514251709
o  286 176.3472091674805
-  287 64.72055511474609
o  288 87.20435180664063
-  289 68.95698394775391
o  290 89.14742279052736
-  291 67.36632232666015
-  292 43.063302993774414
-  293 66.05284881591797
o  294 106.24157409667968
-  295 59.675035858154295
o  296 81.39755325317383
-  297 64.2825309753418
-  298 57.883567810058594
-  299 50.72239303588867
-  300 62.311617660522465
o  301 78.42091522216796
o  302 97.81740570068361
-  303 65.78792877197266
o  304 100.05301361083986
-  305 53.67313003540039
-  306 65.58594589233398
-  307 66.2244354248047
-  308 53.994205474853516
-  309 67.49986190795899
o  310 76.99055480957031
-  311 56.48042335510254
o  312 78.82228546142578
-  313 51.70027542114258
o  314 79.99634246826172
-  315 56.83137855529785
o  316 76.20538864135742
-  317 50.64972610473633
o  318 96.25165328979493
-  319 61.51771621704102
o  320 71.70805435180664
-  321 60.39003143310548
-  322 52.76914291381836
-  323 44.8484375
-  324 57.14113235473633
-  325 46.450526809692384
-  326 66.59435729980468
-  327 61.62563400268555
o  328 79.24991455078126
-  329 66.12214889526368
o  330 77.80061111450196
-  331 61.890215682983396
-  332 60.04970169067383
o  333 90.61001358032227
o  334 76.7575309753418
-  335 47.93784446716309
-  336 57.87575263977051
-  337 54.54773712158203
-  338 52.23845176696777
-  339 58.527296447753905
-  340 35.47724304199219
-  341 58.51338348388672
-  342 61.36659240722656
o  343 80.62860260009765
o  344 82.15951461791992
-  345 61.55494918823242
o  346 96.99386749267579
-  347 62.290042877197266
-  348 61.776740646362306
o  349 101.39890899658204
-  350 62.50719375610352
o  351 87.04169082641602
o  352 111.17218017578125
o  353 71.80141754150391
o  354 71.32414093017579
-  355 59.94085922241211
-  356 63.872389221191405
-  357 38.606950759887695
-  358 57.830715560913085
-  359 50.4404296875
-  360 67.36530838012696
-  361 56.724374771118164
-  362 58.27755889892578
-  363 64.35935821533204
-  364 59.73833122253418
-  365 46.32350044250489
o  366 71.59865188598633
o  367 74.65164413452149
o  368 82.6249252319336
o  369 76.2406723022461
-  370 51.504225158691405
o  371 95.4997055053711
-  372 63.52554054260254
o  373 96.0804214477539
-  374 64.27255477905274
o  375 75.66929779052735
-  376 59.03804168701172
-  377 65.48707885742188
-  378 62.0864143371582
-  379 64.07446670532227
-  380 50.05590972900391
o  381 102.43929290771484
-  382 63.24454536437988
-  383 57.18334693908692
-  384 63.2460075378418
o  385 133.89635467529297
o  386 104.61388549804687
o  387 148.3811492919922
o  388 167.7071273803711
o  389 484.7919250488282
o  390 155.9050491333008
-  391 64.72459487915039
-  392 59.307297515869145
o  393 102.21038665771486
o  394 122.04725723266601
o  395 84.99583129882814
o  396 96.80938034057618
o  397 81.44146041870118
o  398 72.18138732910157
o  399 93.89944000244141
o  400 120.11569671630859
o  401 129.5833938598633
o  402 162.17811431884766
o  403 100.58709106445312
o  404 145.16400299072265
o  405 139.51884918212892
o  406 92.07083129882812
o  407 137.52503204345703
o  408 73.11993865966798
-  409 61.35277061462402
o  410 197.63924255371094
o  411 153.28421478271486
-  412 60.79781112670899
o  413 141.38671264648437
o  414 155.5705993652344
o  415 103.22595596313477
o  416 119.15671768188477
o  417 75.01794204711915
o  418 94.66784820556641
o  419 71.04573516845704
-  420 61.99414749145508
-  421 54.30499076843262
o  422 95.9200927734375
o  423 89.85424041748047
-  424 68.0490119934082
-  425 68.06893310546874
-  426 51.90628433227539
-  427 60.97346801757813
-  428 26.00811309814453
o  429 119.65631561279298
o  430 75.11852645874023
-  431 49.63441734313965
-  432 53.66374168395996
o  433 93.15924911499023
o  434 108.20101318359374
o  435 166.60639038085938
o  436 161.51554870605472
o  437 85.97569427490235
o  438 125.32010192871094
o  439 88.5990951538086
-  440 56.03309364318848
o  441 106.61256790161133
-  442 59.723408508300786
o  443 179.85931701660158
-  444 36.84068908691406
o  445 94.44808044433594
-  446 33.18281021118164
o  447 81.05898437500001
-  448 28.492034721374512
o  449 858.8096618652345
o  450 87.83609771728516
o  451 81.72127532958984
o  452 78.19880981445313
o  453 89.88974914550782
o  454 133.03699951171876
o  455 110.74587554931641
-  456 46.51711311340332
o  457 226.14704589843754
-  458 61.785817337036136
o  459 199.51001129150393
o  460 125.2533576965332
o  461 146.84071350097665
-  462 56.3821762084961
o  463 84.48873138427734
o  464 121.61434631347662
o  465 103.70204238891606
o  466 77.54516830444337
o  467 94.58667755126953
o  468 117.63696746826172
o  469 143.75538330078126
o  470 82.53487243652344
o  471 89.49221038818361
o  472 85.71981582641602
o  473 125.29704284667972
o  474 76.20619888305664
o  475 92.70186767578126
-  476 53.05752296447754
o  477 139.05584106445312
o  478 105.29784698486328
o  479 72.2844467163086
o  480 75.41327285766602
o  481 71.77266159057618
-  482 36.81549911499024
o  483 110.2579948425293
o  484 126.6875473022461
o  485 166.36297607421875
o  486 157.57456970214844
o  487 81.9480079650879
o  488 123.81530761718751
o  489 118.49560623168945
o  490 141.4908935546875
o  491 112.95933380126954
o  492 243.48755798339846
o  493 120.6584838867188
o  494 110.78821334838868
o  495 76.60453567504882
o  496 90.67294235229492
o  497 123.78264236450195
o  498 171.3817596435547
-  499 55.252449417114256
o  500 167.69637756347657
o  501 93.85818481445312
o  502 219.43219451904298
o  503 85.1436248779297
o  504 128.02153625488282
o  505 92.66090850830078
o  506 175.98504943847658
-  507 30.419240760803223
-  508 56.39633483886719
-  509 68.65046081542968
o  510 93.00208435058593
-  511 34.87723808288574
o  512 105.65067825317384
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/Trials.pkl
** SKIPPING EXTRACTION, since was able to load snippets, for: 
(animal, DATE, which_level, ANALY_VER, session)
Pancho 221125 trial rulesw 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221125*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909
session:  0
1
Beh Sessions that exist on this date:  {221125: [(1, 'dirshapecolor4b')]}
taking this beh session: 1
Loading these beh expts: ['dirshapecolor4b']
Loading these beh sessions: [1]
Loading this neural session: 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221125*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909
{'filename_components_hyphened': ['Pancho', '221125', '131909'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/221125'], 'basedirs_filenames': ['221125', 'Pancho-221125-131909'], 'filename_final_ext': 'Pancho-221125-131909', 'filename_final_noext': 'Pancho-221125-131909'}
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909/Pancho-221125-131909
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221125/Pancho-221125-131909/spikes_tdt_quick-4
final_dir_name  --  Pancho-221125-131909
time  --  131909
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/221125.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
o  1 110.70015106201174
-  2 69.10210723876953
-  3 68.57892150878907
o  4 134.8060302734375
o  5 76.63243865966797
o  6 75.66429061889649
o  7 134.07001647949218
-  8 63.589978027343754
o  9 251.26464233398437
o  10 98.07416534423828
o  11 132.20796508789064
-  12 69.47706832885743
o  13 140.5678466796875
-  14 53.41653900146484
o  15 169.58192443847656
o  16 81.9468475341797
o  17 77.63435974121094
o  18 104.25444946289063
o  19 96.09076919555665
-  20 42.508761215209965
o  21 120.13069839477541
o  22 214.58809509277347
-  23 45.68513412475586
o  24 74.81740951538086
o  25 73.57882080078126
o  26 580.3622314453125
o  27 102.81129302978516
o  28 71.16385803222656
o  29 255.68379974365234
o  30 98.35868911743165
o  31 156.63096923828127
o  32 70.74853210449218
o  33 79.14913330078124
o  34 101.5077491760254
-  35 41.81251068115235
o  36 166.95777893066406
-  37 60.5641658782959
o  38 75.84748992919923
o  39 129.9774383544922
o  40 160.30959930419922
o  41 88.66414031982421
o  42 97.5841682434082
o  43 123.66165466308594
o  44 71.22148742675782
o  45 251.99578857421875
-  46 51.70357437133789
o  47 150.3605758666992
-  48 43.63840522766113
o  49 100.45581359863282
o  50 76.19973373413086
o  51 93.96791076660156
-  52 68.32694778442382
o  53 725.7648376464844
o  54 80.09454956054688
o  55 163.62498168945314
-  56 60.89304084777832
o  57 246.76725006103518
-  58 67.1504409790039
o  59 89.30016860961913
o  60 90.24994888305665
o  61 85.6065544128418
o  62 80.9630599975586
o  63 248.70314025878906
o  64 117.72973937988284
o  65 81.29031219482422
o  66 104.23651123046875
o  67 102.87630615234376
o  68 94.12977600097656
o  69 73.97239151000977
-  70 52.247196960449216
o  71 71.68388290405274
o  72 156.85655364990237
o  73 72.7403678894043
o  74 72.37607498168946
o  75 75.03299865722657
o  76 108.15685424804688
-  77 63.31049156188965
o  78 85.38916625976563
-  79 60.5265251159668
o  80 151.6734130859375
-  81 58.64752883911133
o  82 92.2702621459961
-  83 64.20382156372071
-  84 49.89320945739746
-  85 62.70757141113281
-  86 69.28522186279297
-  87 46.61496124267578
o  88 88.66550521850586
-  89 56.200006103515626
o  90 93.35928039550781
o  91 70.53487548828124
-  92 48.67988395690918
-  93 64.94072723388672
o  94 74.74431838989258
o  95 73.21788711547852
o  96 88.53709030151367
o  97 172.1494155883789
o  98 80.27128829956055
o  99 103.43579330444337
o  100 84.92826538085937
-  101 64.34832000732422
o  102 204.72989959716796
-  103 64.93257522583008
o  104 83.87729568481446
o  105 83.48537979125977
o  106 83.08042373657227
-  107 45.005855560302734
-  108 60.08348083496094
-  109 49.631516265869145
-  110 61.08753929138184
-  111 61.222680282592776
o  112 96.45987701416016
o  113 136.00503387451172
o  114 155.4192077636719
-  115 58.703425598144534
o  116 114.09267578125001
-  117 53.382208251953124
o  118 120.8844596862793
o  119 70.86467590332032
-  120 51.14034919738769
o  121 110.1386215209961
o  122 73.78821640014648
o  123 99.32025375366212
o  124 88.34268341064453
-  125 67.90655364990235
o  126 98.08311309814454
o  127 87.79210968017578
-  128 46.213303375244145
o  129 80.4477149963379
o  130 289.26820068359376
-  131 65.32670516967774
o  132 819.5115783691406
o  133 137.80018310546876
-  134 69.14446792602538
o  135 189.4669891357422
o  136 128.86629638671877
o  137 83.71893463134765
o  138 153.4797790527344
-  139 60.685387802124026
o  140 133.85625305175782
o  141 123.9957618713379
o  142 386.5689727783203
o  143 104.61200790405273
o  144 103.5354507446289
-  145 61.44029655456543
-  146 60.825867080688475
-  147 42.71024017333985
o  148 94.45768280029297
-  149 64.81007156372071
o  150 85.91355590820312
-  151 51.04849281311035
o  152 203.08633575439453
-  153 56.24013557434082
-  154 67.64160079956055
-  155 53.86366691589355
o  156 91.44458923339845
o  157 190.86886901855473
o  158 293.89886169433595
-  159 60.72432861328126
o  160 75.19936828613281
o  161 72.82343368530273
o  162 75.97271575927735
o  163 71.50904769897461
o  164 92.22686614990235
o  165 105.13744430541993
-  166 54.359056854248045
o  167 189.88598632812503
o  168 82.7800750732422
o  169 72.67840270996093
o  170 100.07739562988283
o  171 176.82958068847657
-  172 69.11110839843751
o  173 122.57362670898438
-  174 66.73458938598633
-  175 69.29116287231446
-  176 66.31260681152344
o  177 105.72633056640625
o  178 97.1804412841797
o  179 231.0660415649414
-  180 64.6927276611328
o  181 115.43360214233398
-  182 63.6655387878418
-  183 63.07242240905762
o  184 71.6611312866211
-  185 30.025129508972167
o  186 101.83674392700196
o  187 73.15551071166992
-  188 66.69722518920898
o  189 96.55576248168946
-  190 63.822997665405275
-  191 62.907927703857425
o  192 75.94124450683594
o  193 444.61603088378905
-  194 64.75936813354492
o  195 87.14242782592774
-  196 54.753278732299805
o  197 71.49410400390626
o  198 70.30115432739258
o  199 82.90447311401368
o  200 99.0100570678711
o  201 74.44369354248047
o  202 86.78613433837891
o  203 114.15658493041994
o  204 86.46830215454102
o  205 95.02078170776367
o  206 77.23492584228516
-  207 62.50435562133789
-  208 67.80478210449219
-  209 47.63797988891602
o  210 74.29186935424805
-  211 66.39629058837892
-  212 65.57066650390625
o  213 85.98596878051758
o  214 71.8694221496582
o  215 73.07013931274415
o  216 76.3842155456543
o  217 78.67113571166993
o  218 82.35745468139649
o  219 80.12860641479492
-  220 67.54318542480469
o  221 70.09756317138672
o  222 75.26875457763671
-  223 68.41546173095703
o  224 72.815576171875
o  225 77.94846725463869
-  226 66.56689453125
o  227 111.31159057617188
-  228 62.3229362487793
o  229 100.01448822021484
-  230 62.92288818359375
-  231 64.48743896484375
-  232 35.76767845153809
o  233 89.0982879638672
o  234 72.62473678588867
-  235 65.9875862121582
-  236 55.423601150512695
o  237 88.51481246948242
-  238 64.58454208374023
-  239 52.516048431396484
o  240 80.83622665405274
-  241 69.73650360107422
-  242 67.46670455932617
o  243 72.11400756835937
o  244 84.56570892333984
-  245 59.42482223510742
o  246 91.67399673461914
-  247 64.31516494750977
o  248 106.05211029052735
-  249 63.4588508605957
-  250 55.51843948364259
o  251 75.55241012573242
o  252 91.28190841674805
-  253 55.949310302734375
-  254 48.791921615600586
-  255 68.47396392822266
o  256 85.85845184326172
o  257 200.06482543945313
o  258 101.1684326171875
o  259 104.27263565063477
o  260 190.90434722900392
o  261 127.20354080200195
-  262 64.95013732910157
o  263 90.55942535400392
o  264 116.98455657958985
o  265 140.23272705078125
o  266 86.88879089355468
o  267 108.41556930541994
-  268 50.83239898681641
o  269 102.51695556640627
o  270 126.93021392822266
o  271 107.690731048584
o  272 155.84538574218752
o  273 84.39642944335938
o  274 83.41875457763672
o  275 94.38887329101563
o  276 138.85443572998048
o  277 106.4805763244629
o  278 95.83465042114258
o  279 89.27790298461915
o  280 176.27704467773438
o  281 80.62609024047852
o  282 90.9362159729004
o  283 70.51200942993164
o  284 137.08782653808595
o  285 114.543514251709
o  286 176.3472091674805
-  287 64.72055511474609
o  288 87.20435180664063
-  289 68.95698394775391
o  290 89.14742279052736
-  291 67.36632232666015
-  292 43.063302993774414
-  293 66.05284881591797
o  294 106.24157409667968
-  295 59.675035858154295
o  296 81.39755325317383
-  297 64.2825309753418
-  298 57.883567810058594
-  299 50.72239303588867
-  300 62.311617660522465
o  301 78.42091522216796
o  302 97.81740570068361
-  303 65.78792877197266
o  304 100.05301361083986
-  305 53.67313003540039
-  306 65.58594589233398
-  307 66.2244354248047
-  308 53.994205474853516
-  309 67.49986190795899
o  310 76.99055480957031
-  311 56.48042335510254
o  312 78.82228546142578
-  313 51.70027542114258
o  314 79.99634246826172
-  315 56.83137855529785
o  316 76.20538864135742
-  317 50.64972610473633
o  318 96.25165328979493
-  319 61.51771621704102
o  320 71.70805435180664
-  321 60.39003143310548
-  322 52.76914291381836
-  323 44.8484375
-  324 57.14113235473633
-  325 46.450526809692384
-  326 66.59435729980468
-  327 61.62563400268555
o  328 79.24991455078126
-  329 66.12214889526368
o  330 77.80061111450196
-  331 61.890215682983396
-  332 60.04970169067383
o  333 90.61001358032227
o  334 76.7575309753418
-  335 47.93784446716309
-  336 57.87575263977051
-  337 54.54773712158203
-  338 52.23845176696777
-  339 58.527296447753905
-  340 35.47724304199219
-  341 58.51338348388672
-  342 61.36659240722656
o  343 80.62860260009765
o  344 82.15951461791992
-  345 61.55494918823242
o  346 96.99386749267579
-  347 62.290042877197266
-  348 61.776740646362306
o  349 101.39890899658204
-  350 62.50719375610352
o  351 87.04169082641602
o  352 111.17218017578125
o  353 71.80141754150391
o  354 71.32414093017579
-  355 59.94085922241211
-  356 63.872389221191405
-  357 38.606950759887695
-  358 57.830715560913085
-  359 50.4404296875
-  360 67.36530838012696
-  361 56.724374771118164
-  362 58.27755889892578
-  363 64.35935821533204
-  364 59.73833122253418
-  365 46.32350044250489
o  366 71.59865188598633
o  367 74.65164413452149
o  368 82.6249252319336
o  369 76.2406723022461
-  370 51.504225158691405
o  371 95.4997055053711
-  372 63.52554054260254
o  373 96.0804214477539
-  374 64.27255477905274
o  375 75.66929779052735
-  376 59.03804168701172
-  377 65.48707885742188
-  378 62.0864143371582
-  379 64.07446670532227
-  380 50.05590972900391
o  381 102.43929290771484
-  382 63.24454536437988
-  383 57.18334693908692
-  384 63.2460075378418
o  385 133.89635467529297
o  386 104.61388549804687
o  387 148.3811492919922
o  388 167.7071273803711
o  389 484.7919250488282
o  390 155.9050491333008
-  391 64.72459487915039
-  392 59.307297515869145
o  393 102.21038665771486
o  394 122.04725723266601
o  395 84.99583129882814
o  396 96.80938034057618
o  397 81.44146041870118
o  398 72.18138732910157
o  399 93.89944000244141
o  400 120.11569671630859
o  401 129.5833938598633
o  402 162.17811431884766
o  403 100.58709106445312
o  404 145.16400299072265
o  405 139.51884918212892
o  406 92.07083129882812
o  407 137.52503204345703
o  408 73.11993865966798
-  409 61.35277061462402
o  410 197.63924255371094
o  411 153.28421478271486
-  412 60.79781112670899
o  413 141.38671264648437
o  414 155.5705993652344
o  415 103.22595596313477
o  416 119.15671768188477
o  417 75.01794204711915
o  418 94.66784820556641
o  419 71.04573516845704
-  420 61.99414749145508
-  421 54.30499076843262
o  422 95.9200927734375
o  423 89.85424041748047
-  424 68.0490119934082
-  425 68.06893310546874
-  426 51.90628433227539
-  427 60.97346801757813
-  428 26.00811309814453
o  429 119.65631561279298
o  430 75.11852645874023
-  431 49.63441734313965
-  432 53.66374168395996
o  433 93.15924911499023
o  434 108.20101318359374
o  435 166.60639038085938
o  436 161.51554870605472
o  437 85.97569427490235
o  438 125.32010192871094
o  439 88.5990951538086
-  440 56.03309364318848
o  441 106.61256790161133
-  442 59.723408508300786
o  443 179.85931701660158
-  444 36.84068908691406
o  445 94.44808044433594
-  446 33.18281021118164
o  447 81.05898437500001
-  448 28.492034721374512
o  449 858.8096618652345
o  450 87.83609771728516
o  451 81.72127532958984
o  452 78.19880981445313
o  453 89.88974914550782
o  454 133.03699951171876
o  455 110.74587554931641
-  456 46.51711311340332
o  457 226.14704589843754
-  458 61.785817337036136
o  459 199.51001129150393
o  460 125.2533576965332
o  461 146.84071350097665
-  462 56.3821762084961
o  463 84.48873138427734
o  464 121.61434631347662
o  465 103.70204238891606
o  466 77.54516830444337
o  467 94.58667755126953
o  468 117.63696746826172
o  469 143.75538330078126
o  470 82.53487243652344
o  471 89.49221038818361
o  472 85.71981582641602
o  473 125.29704284667972
o  474 76.20619888305664
o  475 92.70186767578126
-  476 53.05752296447754
o  477 139.05584106445312
o  478 105.29784698486328
o  479 72.2844467163086
o  480 75.41327285766602
o  481 71.77266159057618
-  482 36.81549911499024
o  483 110.2579948425293
o  484 126.6875473022461
o  485 166.36297607421875
o  486 157.57456970214844
o  487 81.9480079650879
o  488 123.81530761718751
o  489 118.49560623168945
o  490 141.4908935546875
o  491 112.95933380126954
o  492 243.48755798339846
o  493 120.6584838867188
o  494 110.78821334838868
o  495 76.60453567504882
o  496 90.67294235229492
o  497 123.78264236450195
o  498 171.3817596435547
-  499 55.252449417114256
o  500 167.69637756347657
o  501 93.85818481445312
o  502 219.43219451904298
o  503 85.1436248779297
o  504 128.02153625488282
o  505 92.66090850830078
o  506 175.98504943847658
-  507 30.419240760803223
-  508 56.39633483886719
-  509 68.65046081542968
o  510 93.00208435058593
-  511 34.87723808288574
o  512 105.65067825317384
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/221125/Pancho-221125-131909/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221125-sess_0/Trials.pkl
This many vals across loaded session
0 : 2266504
Assigning to SP.Params this item:
{'which_level': 'trial', '_list_events': ['fixcue', 'fix_touch', 'rulecue2', 'samp', 'go_cue', 'first_raise', 'on_strokeidx_0', 'off_stroke_last', 'doneb', 'post', 'reward_all'], 'list_events_uniqnames': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_features_extraction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize'], 'list_features_get_conjunction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65], 'map_var_to_othervars': None, 'strokes_only_keep_single': False, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'map_var_to_levels': None}
Assigning to SP.ParamsGlobals this item:
{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.65, 'POST_DUR_CALC': 0.65, 'list_events': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65]}
stored in self.Dat[BehClass]
0
200
400
600
Running D.behclass_tokens_extract_datsegs
0
200
400
600
TODO!!! Merge this with other learning-related code
stored in self.Dat[BehClass]
0
200
400
600
Running D.behclass_tokens_extract_datsegs
0
200
400
600
trial # 0
trial # 100
trial # 200
trial # 300
trial # 400
trial # 500
trial # 600
Generated column called 'agent', which connects agent_kind-rule
n samples for conjunctions of score_name, agent_rule, agent_kind:
('binsucc', 'L', 'model') :     357
('binsucc', 'llV1', 'model') :     341
TODO! _preprocess_sanity_check
Starting length of D.Dat: 698
self.Dat modified!!
Len, after remove aborts: 471
############ TAKING ONLY NO SUPERVISION TRIALS
--BEFORE REMOVE; existing supervision_stage_concise:
off|1|solid|0     408
mask|1|solid|0     63
Name: supervision_stage_concise, dtype: int64
self.Dat modified!!
--AFTER REMOVE; existing supervision_stage_concise:
off|1|solid|0    408
Name: supervision_stage_concise, dtype: int64
Dataset final len: 408
-- Len of D, before applying this param: remove_repeated_trials, ... 408
appended col to self.Dat:
dummy
self.Dat starting legnth:  394
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  394
after: 394
-- Len of D, before applying this param: correct_sequencing_binary_score, ... 394
self.Dat starting legnth:  358
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  358
after: 358
-- Len of D, before applying this param: one_to_one_beh_task_strokes, ... 358
after: 357
Done!, new len of dataset 357
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_taskgroup_probe/SV_r2_maxtime_1way_mshuff
starting sites:  318
starting sites:  [1, 4, 5, 6, 7, 9, 10, 11, 13, 15, 16, 17, 19, 21, 22, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 50, 51, 53, 54, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 73, 74, 75, 76, 78, 80, 82, 88, 90, 91, 94, 95, 96, 97, 98, 100, 102, 104, 106, 112, 113, 114, 116, 118, 119, 121, 122, 123, 124, 126, 127, 129, 130, 132, 133, 135, 136, 137, 138, 140, 141, 142, 143, 144, 148, 150, 152, 158, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 177, 178, 179, 181, 184, 186, 187, 189, 192, 193, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 210, 213, 214, 215, 216, 217, 218, 219, 221, 222, 224, 225, 227, 229, 233, 234, 237, 240, 243, 244, 246, 248, 251, 252, 256, 258, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 290, 294, 296, 301, 302, 304, 310, 312, 314, 316, 318, 320, 328, 330, 333, 334, 343, 344, 346, 349, 351, 352, 353, 354, 366, 367, 368, 369, 371, 373, 375, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 413, 414, 415, 416, 417, 418, 419, 422, 423, 429, 430, 433, 434, 435, 436, 437, 438, 439, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 510, 512]
For percentile 10, using this threshold: 12.145155311284642
sites_good:  286
sites_bad:  32
Updates self.Sites
ending sites:  286
ending sites:  [1, 4, 5, 7, 9, 10, 11, 13, 15, 16, 17, 21, 22, 24, 25, 27, 29, 30, 31, 32, 33, 34, 38, 39, 40, 42, 43, 44, 45, 47, 49, 50, 51, 53, 54, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 74, 75, 76, 78, 80, 82, 91, 94, 95, 97, 98, 100, 102, 104, 106, 112, 113, 114, 116, 118, 119, 121, 122, 123, 124, 126, 127, 130, 132, 133, 135, 136, 138, 140, 141, 142, 143, 144, 148, 150, 152, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 177, 178, 179, 181, 184, 187, 189, 192, 193, 195, 197, 199, 200, 203, 204, 205, 206, 210, 213, 214, 215, 217, 218, 219, 222, 225, 227, 229, 233, 237, 243, 244, 246, 248, 252, 256, 258, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 294, 296, 301, 302, 304, 310, 312, 314, 318, 320, 330, 333, 334, 343, 349, 351, 352, 353, 354, 367, 369, 371, 373, 375, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 413, 414, 415, 417, 418, 419, 422, 423, 429, 430, 433, 434, 435, 436, 437, 438, 439, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_taskgroup_probe/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_taskgroup_probe/SV_r2_maxtime_1way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['02_rulecue2_-600_to_-40', '02_rulecue2_40_to_600', '03_samp_-600_to_-40', '03_samp_40_to_600', '03_samp_50_to_600', '04_go_cue_-600_to_-50', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
Running grouping_print_n_samples...
DOing these! ...
list_events ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['02_rulecue2_-600_to_-40', '02_rulecue2_40_to_600', '03_samp_-600_to_-40', '03_samp_40_to_600', '03_samp_50_to_600', '04_go_cue_-600_to_-50', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  0 148
 
Updated ParamsGlobals for event 02_rulecue2 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  02_rulecue2_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
02_rulecue2_-600_to_-40
 
Updated ParamsGlobals for event 02_rulecue2 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  02_rulecue2_40_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
02_rulecue2_40_to_600
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-40
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_40_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_40_to_600
 
Updated ParamsGlobals for event 04_go_cue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  04_go_cue_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
04_go_cue_-600_to_-50
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.25
POST_DUR_CALC  =  0.35
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-250_to_350
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-250_to_350
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['taskgroup', 'probe']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff
starting sites:  286
starting sites:  [1, 4, 5, 7, 9, 10, 11, 13, 15, 16, 17, 21, 22, 24, 25, 27, 29, 30, 31, 32, 33, 34, 38, 39, 40, 42, 43, 44, 45, 47, 49, 50, 51, 53, 54, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 74, 75, 76, 78, 80, 82, 91, 94, 95, 97, 98, 100, 102, 104, 106, 112, 113, 114, 116, 118, 119, 121, 122, 123, 124, 126, 127, 130, 132, 133, 135, 136, 138, 140, 141, 142, 143, 144, 148, 150, 152, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 177, 178, 179, 181, 184, 187, 189, 192, 193, 195, 197, 199, 200, 203, 204, 205, 206, 210, 213, 214, 215, 217, 218, 219, 222, 225, 227, 229, 233, 237, 243, 244, 246, 248, 252, 256, 258, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 294, 296, 301, 302, 304, 310, 312, 314, 318, 320, 330, 333, 334, 343, 349, 351, 352, 353, 354, 367, 369, 371, 373, 375, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 413, 414, 415, 417, 418, 419, 422, 423, 429, 430, 433, 434, 435, 436, 437, 438, 439, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 510, 512]
For percentile 10, using this threshold: 12.145155311284642
sites_good:  286
sites_bad:  32
Updates self.Sites
ending sites:  286
ending sites:  [1, 4, 5, 7, 9, 10, 11, 13, 15, 16, 17, 21, 22, 24, 25, 27, 29, 30, 31, 32, 33, 34, 38, 39, 40, 42, 43, 44, 45, 47, 49, 50, 51, 53, 54, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 74, 75, 76, 78, 80, 82, 91, 94, 95, 97, 98, 100, 102, 104, 106, 112, 113, 114, 116, 118, 119, 121, 122, 123, 124, 126, 127, 130, 132, 133, 135, 136, 138, 140, 141, 142, 143, 144, 148, 150, 152, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 177, 178, 179, 181, 184, 187, 189, 192, 193, 195, 197, 199, 200, 203, 204, 205, 206, 210, 213, 214, 215, 217, 218, 219, 222, 225, 227, 229, 233, 237, 243, 244, 246, 248, 252, 256, 258, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 294, 296, 301, 302, 304, 310, 312, 314, 318, 320, 330, 333, 334, 343, 349, 351, 352, 353, 354, 367, 369, 371, 373, 375, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 413, 414, 415, 417, 418, 419, 422, 423, 429, 430, 433, 434, 435, 436, 437, 438, 439, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['02_rulecue2_-600_to_-40', '02_rulecue2_40_to_600', '03_samp_-600_to_-40', '03_samp_40_to_600', '04_go_cue_-600_to_-50', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
Running grouping_print_n_samples...
DOing these! ...
list_events ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['02_rulecue2_-600_to_-40', '02_rulecue2_40_to_600', '03_samp_-600_to_-40', '03_samp_40_to_600', '04_go_cue_-600_to_-50', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  0 65
 
Updated ParamsGlobals for event 02_rulecue2 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  02_rulecue2_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
02_rulecue2_-600_to_-40
 
Updated ParamsGlobals for event 02_rulecue2 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  02_rulecue2_40_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
02_rulecue2_40_to_600
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-40
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_40_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_40_to_600
 
Updated ParamsGlobals for event 04_go_cue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  04_go_cue_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
04_go_cue_-600_to_-50
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.25
POST_DUR_CALC  =  0.35
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-250_to_350
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-250_to_350
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff
starting sites:  286
starting sites:  [1, 4, 5, 7, 9, 10, 11, 13, 15, 16, 17, 21, 22, 24, 25, 27, 29, 30, 31, 32, 33, 34, 38, 39, 40, 42, 43, 44, 45, 47, 49, 50, 51, 53, 54, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 74, 75, 76, 78, 80, 82, 91, 94, 95, 97, 98, 100, 102, 104, 106, 112, 113, 114, 116, 118, 119, 121, 122, 123, 124, 126, 127, 130, 132, 133, 135, 136, 138, 140, 141, 142, 143, 144, 148, 150, 152, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 177, 178, 179, 181, 184, 187, 189, 192, 193, 195, 197, 199, 200, 203, 204, 205, 206, 210, 213, 214, 215, 217, 218, 219, 222, 225, 227, 229, 233, 237, 243, 244, 246, 248, 252, 256, 258, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 294, 296, 301, 302, 304, 310, 312, 314, 318, 320, 330, 333, 334, 343, 349, 351, 352, 353, 354, 367, 369, 371, 373, 375, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 413, 414, 415, 417, 418, 419, 422, 423, 429, 430, 433, 434, 435, 436, 437, 438, 439, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 510, 512]
For percentile 10, using this threshold: 12.145155311284642
sites_good:  286
sites_bad:  32
Updates self.Sites
ending sites:  286
ending sites:  [1, 4, 5, 7, 9, 10, 11, 13, 15, 16, 17, 21, 22, 24, 25, 27, 29, 30, 31, 32, 33, 34, 38, 39, 40, 42, 43, 44, 45, 47, 49, 50, 51, 53, 54, 55, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 74, 75, 76, 78, 80, 82, 91, 94, 95, 97, 98, 100, 102, 104, 106, 112, 113, 114, 116, 118, 119, 121, 122, 123, 124, 126, 127, 130, 132, 133, 135, 136, 138, 140, 141, 142, 143, 144, 148, 150, 152, 158, 160, 162, 163, 164, 165, 167, 168, 169, 170, 171, 173, 177, 178, 179, 181, 184, 187, 189, 192, 193, 195, 197, 199, 200, 203, 204, 205, 206, 210, 213, 214, 215, 217, 218, 219, 222, 225, 227, 229, 233, 237, 243, 244, 246, 248, 252, 256, 258, 259, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 294, 296, 301, 302, 304, 310, 312, 314, 318, 320, 330, 333, 334, 343, 349, 351, 352, 353, 354, 367, 369, 371, 373, 375, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 413, 414, 415, 417, 418, 419, 422, 423, 429, 430, 433, 434, 435, 436, 437, 438, 439, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['02_rulecue2', '02_rulecue2', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, -0.6, 0.04, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.04, 0.6, -0.04, 0.6, -0.05, -0.05, 0.35, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221125-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
df_var doesnt exist...!
COMPUTING df_var!!!
Running grouping_print_n_samples...
SKipping, you dont have a variable in self.DfScalar. Need to rerun extraction..
These are the existing columns
Index(['trialcode', 'chan', 'event_aligned', '_event_aligned', 'spike_times',
       'trial_neural', 'event_time', 'fr_sm', 'probe', 'taskgroup',
       'character', 'epoch', 'task_kind', 'supervision_stage_concise',
       'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc',
       'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc',
       'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'event', 'fr_sm_times',
       'fr_sm_sqrt', 'session_idx', 'fr_scalar_raw'],
      dtype='object')
failed searchign for these columns:
['chan', 'event', 'epoch', 'epochset']
!! SKIPPING:  epoch ['epochset']
