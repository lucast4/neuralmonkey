Got these LIST_VAR and LIST_VARS_CONJUNCTION:
['epoch', 'epoch', 'character', 'seqc_0_loc_shape', 'seqc_0_loc', 'seqc_1_loc_shape']
[['epochset'], ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh'], ['epoch', 'epochset'], ['epoch', 'epochset'], ['epoch', 'epochset'], ['epoch', 'epochset', 'seqc_0_loc_shape']]
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221014*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758
session:  0
1
Beh Sessions that exist on this date:  {221014: [(1, 'grammardircolor3d')]}
taking this beh session: 1
------------------------------
Loading this neural session: 0
Loading these beh expts: ['grammardircolor3d']
Loading these beh sessions: [1]
Using this beh_trial_map_list: [(1, 0)]
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221014*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758
{'filename_components_hyphened': ['Pancho', '221014', '151758'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/221014'], 'basedirs_filenames': ['221014', 'Pancho-221014-151758'], 'filename_final_ext': 'Pancho-221014-151758', 'filename_final_noext': 'Pancho-221014-151758'}
FOund this path for spikes:  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758/spikes_tdt_quick-4.5
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758/Pancho-221014-151758
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758/spikes_tdt_quick-4.5
final_dir_name  --  Pancho-221014-151758
time  --  151758
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/221014.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
o  1 70.07001495361328
o  2 119.39619674682618
o  3 92.84320068359375
-  4 69.22887573242187
o  5 98.32090225219727
o  6 74.40170669555664
o  7 133.85044555664064
o  8 83.40576629638672
o  9 217.99324645996094
o  10 80.13178100585938
o  11 149.97047576904296
o  12 78.5964958190918
o  13 287.0962341308594
-  14 65.85949630737305
o  15 152.89322814941406
o  16 90.90266876220703
o  17 83.47337341308594
o  18 125.39983520507813
o  19 672.7095642089844
-  20 54.108445739746095
o  21 110.02447204589843
o  22 110.56966171264651
-  23 51.97209129333496
o  24 87.21598510742187
-  25 62.71386032104492
o  26 388.7219604492188
o  27 139.43396453857423
o  28 75.85004806518555
o  29 185.2539810180664
o  30 94.97772064208985
o  31 182.09425659179686
o  32 97.87480926513672
o  33 171.04986724853518
o  34 113.90146942138672
-  35 45.129401016235356
o  36 328.52210388183596
o  37 132.3924743652344
o  38 108.6971534729004
o  39 71.29547958374023
o  40 318.4283813476563
o  41 169.03667144775392
o  42 173.64176025390626
o  43 166.9941619873047
o  44 187.22461395263673
o  45 1112.8847900390624
-  46 69.86896057128907
o  47 151.3925582885742
-  48 48.022822570800784
o  49 113.45245666503908
o  50 85.42860488891601
o  51 380.717822265625
o  52 92.12309417724609
o  53 187.13748016357422
o  54 128.95819396972658
o  55 503.33326416015626
o  56 98.39811401367187
o  57 157.87503814697266
o  58 101.59659881591797
o  59 168.6999282836914
o  60 103.42988662719728
o  61 118.33864288330078
o  62 105.55364227294922
o  63 277.8090026855469
o  64 97.60985717773437
o  65 85.44774856567383
o  66 93.93840942382813
o  67 83.75935974121094
o  68 160.73418273925782
o  69 103.59523696899414
-  70 58.05966758728027
o  71 72.60229034423828
o  72 92.20206680297852
o  73 75.65871124267578
o  74 147.55613403320314
o  75 102.96719818115236
o  76 83.77497177124025
o  77 80.76305389404298
o  78 174.86832427978516
o  79 116.7929458618164
o  80 111.48871154785158
-  81 61.312757110595705
-  82 62.32795677185059
-  83 62.55381202697754
-  84 63.95877265930176
-  85 60.64043350219727
o  86 83.7136817932129
-  87 47.25300407409668
-  88 69.0390007019043
-  89 61.06460266113281
o  90 98.20525665283203
o  91 71.99164581298828
o  92 71.03368072509765
-  93 58.629022216796876
o  94 90.50437088012696
-  95 57.47223167419434
o  96 74.82312393188477
o  97 137.08795623779298
o  98 97.6595184326172
o  99 118.54615173339845
o  100 135.40330200195314
o  101 76.31842422485352
o  102 95.20481491088867
-  103 59.958503341674806
-  104 61.98924522399902
o  105 137.5545166015625
o  106 113.866561126709
-  107 47.28807525634766
o  108 81.5717758178711
-  109 69.14262313842774
-  110 63.746114349365236
o  111 73.84794845581055
o  112 87.55932159423828
o  113 109.39519119262695
o  114 167.9239959716797
o  115 110.37302169799806
o  116 145.57748107910157
-  117 59.21947059631348
o  118 81.35688858032226
o  119 125.3233154296875
o  120 70.51404113769532
o  121 212.1708709716797
o  122 71.28085861206054
o  123 83.35153121948242
-  124 69.20288772583008
o  125 75.50782775878906
o  126 114.16984710693359
o  127 70.20228271484375
-  128 47.0530029296875
-  129 47.52632522583008
o  130 506.9996643066406
-  131 66.01510009765624
o  132 266.7898345947266
o  133 121.9940055847168
o  134 112.57147293090821
o  135 93.09648056030275
o  136 217.29194946289064
o  137 75.61439514160156
o  138 148.77705078125004
-  139 66.89347534179687
o  140 147.45016326904297
o  141 85.3313217163086
o  142 519.0963684082031
o  143 93.15379791259767
o  144 159.40061492919924
o  145 135.5989959716797
o  146 85.28827819824218
-  147 55.696991729736325
o  148 85.39910736083985
-  149 57.2765510559082
o  150 376.0773620605469
o  151 105.16657180786133
o  152 99.39850158691407
o  153 92.89810485839844
o  154 96.99705886840822
-  155 55.53313941955567
o  156 93.8808090209961
o  157 92.53886184692384
o  158 324.9564208984375
o  159 124.8187942504883
o  160 78.78099060058594
o  161 136.4160614013672
o  162 97.67760238647462
o  163 144.65334625244142
o  164 71.17960968017579
o  165 99.30065002441407
o  166 87.49186096191406
o  167 123.55106582641604
-  168 66.55731201171875
o  169 78.96839294433593
o  170 103.47154693603515
-  171 66.33852081298828
-  172 59.08343734741211
o  173 222.3491241455078
o  174 70.52276306152343
o  175 81.11002731323242
-  176 52.75696220397949
o  177 176.69336395263673
o  178 95.22745971679687
o  179 256.5112243652344
o  180 74.18759384155274
o  181 149.349072265625
o  182 76.17356796264649
-  183 65.164794921875
o  184 81.00478668212891
-  185 29.589839935302734
o  186 84.15338439941407
-  187 61.63133010864258
o  188 83.21488418579102
o  189 122.4232147216797
-  190 64.96926345825196
o  191 103.0073486328125
o  192 84.61698379516602
o  193 157.16852264404298
o  194 85.44417877197266
o  195 146.14857635498046
o  196 75.75224914550782
o  197 298.53297424316406
-  198 53.37626152038574
o  199 72.83033828735351
o  200 80.55078811645508
o  201 70.17307739257812
o  202 103.55473785400392
o  203 217.7834930419923
-  204 67.74217224121095
o  205 73.68449554443359
-  206 69.2004524230957
o  207 79.99010620117188
-  208 62.41448669433594
-  209 60.4757682800293
o  210 82.23128051757813
o  211 81.03816299438476
-  212 68.08968200683594
o  213 74.54648132324219
-  214 62.92794494628906
o  215 79.09303283691406
o  216 78.2021713256836
-  217 68.18418273925782
o  218 104.41276626586914
o  219 125.95899887084961
-  220 69.62132186889649
o  221 102.92330703735352
o  222 80.43453674316406
o  223 76.54113693237305
o  224 89.9914321899414
o  225 88.48144226074218
o  226 84.55942916870117
o  227 95.62439804077148
-  228 62.50282516479493
o  229 70.07327728271486
-  230 61.87736473083496
-  231 59.57387809753418
-  232 46.021240615844725
o  233 120.24692916870117
-  234 59.3451717376709
o  235 106.4603385925293
o  236 71.90728302001953
o  237 99.1531120300293
o  238 70.190185546875
-  239 55.55191116333008
o  240 89.48558731079102
o  241 74.94077987670899
-  242 63.67621765136719
o  243 81.63639144897462
o  244 77.05525741577148
o  245 81.77744903564454
-  246 63.73132133483887
-  247 68.35110855102539
o  248 101.29867553710938
-  249 65.83886337280273
-  250 51.100903320312504
o  251 82.04973831176758
o  252 115.724755859375
o  253 74.58620529174804
o  254 80.83525314331055
o  255 113.17791976928712
o  256 101.31348800659181
o  257 159.08499298095705
o  258 94.20059432983399
o  259 102.81791915893555
o  260 236.57092895507813
o  261 95.06597061157227
o  262 96.87097320556641
o  263 143.87831115722656
o  264 103.08134307861329
o  265 117.13843383789063
-  266 68.36220932006836
o  267 107.21416244506837
-  268 59.687156677246094
o  269 96.59871978759766
o  270 89.45384216308594
-  271 62.13063697814941
o  272 113.42986602783203
o  273 257.9465393066406
o  274 102.45425567626954
o  275 79.47071914672851
o  276 78.85202941894532
o  277 101.87722167968751
o  278 141.11019744873047
o  279 87.23502655029297
o  280 83.68796615600586
o  281 129.91513214111328
o  282 111.02151489257812
o  283 73.66617050170899
o  284 98.24857025146484
o  285 120.51009902954101
o  286 99.9341033935547
o  287 75.1629852294922
o  288 99.64851989746094
o  289 70.71280822753906
o  290 99.87283935546876
o  291 71.15840148925781
-  292 40.82721862792969
-  293 53.69741592407227
o  294 95.40920333862306
o  295 83.4584327697754
o  296 80.23085021972656
-  297 57.253458404541014
-  298 64.20526733398438
o  299 71.50268783569337
o  300 117.87270889282227
-  301 60.22169799804688
o  302 98.3749153137207
o  303 111.82118301391601
o  304 77.43159637451173
-  305 59.03513679504395
o  306 87.82177352905273
-  307 63.47863883972168
-  308 66.15353164672852
o  309 75.4908348083496
o  310 81.5715721130371
o  311 77.54899063110352
o  312 77.47124252319337
-  313 52.21627426147461
-  314 60.977572250366215
-  315 60.01525382995606
o  316 129.323876953125
o  317 70.99810943603516
o  318 85.30386962890626
o  319 76.84476013183594
o  320 82.29061279296876
-  321 54.160247802734375
-  322 57.92383155822754
o  323 103.98460083007812
-  324 52.92584228515625
-  325 51.48751373291016
o  326 71.3086654663086
o  327 189.77355194091797
o  328 101.48578262329103
o  329 101.03164520263672
o  330 72.49633407592773
o  331 150.0833969116211
o  332 106.21741256713868
o  333 87.82629928588868
o  334 94.89099655151368
-  335 52.30215682983398
-  336 42.45133285522461
-  337 47.5807903289795
-  338 68.51712493896484
o  339 70.14190444946288
-  340 40.18007049560547
o  341 73.37190551757813
-  342 54.66943550109863
o  343 89.49747848510742
o  344 170.4292007446289
o  345 83.2826431274414
o  346 72.14065856933594
o  347 94.18527221679688
o  348 94.83524856567382
o  349 103.85513610839844
-  350 69.73488922119141
o  351 93.77596282958984
o  352 112.58528900146484
-  353 60.82431106567383
-  354 59.3052303314209
o  355 78.40587692260742
o  356 72.77914657592774
-  357 42.72082023620605
o  358 78.34797897338868
-  359 57.17334175109863
o  360 87.67100067138672
o  361 90.12119674682617
o  362 73.76628036499024
-  363 67.43524322509766
-  364 67.72242126464845
-  365 48.718485260009764
o  366 179.67342071533204
-  367 67.64673690795898
o  368 148.4550994873047
-  369 64.92953338623047
o  370 83.39576034545898
o  371 87.73883819580078
-  372 59.41602096557617
o  373 77.99143142700196
o  374 75.39197540283203
o  375 91.05914764404297
o  376 185.17770233154297
-  377 64.09656219482422
-  378 65.49094696044922
o  379 133.8427963256836
-  380 67.42978286743164
o  381 103.5524658203125
-  382 61.05181541442871
-  383 53.119581604003905
o  384 87.47046279907228
o  385 372.5703063964844
o  386 125.24899139404297
o  387 136.37142639160157
o  388 143.10113830566405
o  389 261.8381317138672
o  390 190.68931274414064
-  391 52.82984085083008
-  392 66.64052886962891
o  393 112.93611450195314
o  394 76.13117294311523
o  395 101.09572677612304
o  396 127.67452545166016
o  397 83.11093673706054
o  398 70.64347915649414
o  399 282.0115600585938
o  400 118.83141555786133
o  401 102.97473983764648
o  402 163.53166656494142
o  403 125.26416702270508
o  404 130.1272933959961
o  405 163.9676040649414
o  406 94.40434112548829
o  407 236.85559692382813
o  408 116.06289443969727
-  409 53.045603561401364
o  410 192.38845825195312
o  411 91.89567184448244
o  412 130.53514709472657
o  413 124.47970275878907
o  414 163.1252670288086
o  415 154.1702575683594
o  416 88.0114356994629
o  417 112.92021255493164
o  418 140.19553833007814
o  419 76.2939582824707
-  420 67.96377944946289
o  421 107.48730621337891
o  422 121.87740631103516
o  423 116.81532821655274
-  424 65.70640335083007
o  425 77.84573974609376
-  426 56.59717636108399
-  427 68.66922607421876
-  428 27.911373329162597
o  429 88.45717163085938
o  430 92.20145111083984
-  431 56.619218826293945
-  432 52.50416030883789
o  433 128.12307739257812
o  434 142.78850708007812
o  435 235.12962646484377
o  436 128.4585906982422
o  437 74.89149551391601
-  438 63.58048934936524
o  439 102.44144668579102
o  440 118.59660491943359
o  441 156.44501495361328
o  442 73.18268432617188
o  443 432.3601013183594
-  444 33.045706558227536
o  445 92.65974960327148
-  446 34.87873306274414
o  447 96.79855804443359
-  448 30.228604507446292
o  449 110.73665161132813
o  450 103.70192337036133
o  451 262.52626953125
o  452 121.67299880981446
o  453 93.85409011840821
o  454 154.58413391113282
o  455 123.69183502197265
-  456 58.5660140991211
o  457 130.89010467529297
o  458 73.74380264282226
o  459 190.92642517089843
o  460 173.3803466796875
o  461 142.13896636962892
o  462 77.71271896362305
o  463 78.97619171142578
o  464 146.0954345703125
o  465 125.60858917236328
-  466 51.99344177246094
o  467 105.5820671081543
o  468 119.03297271728516
o  469 132.49508361816407
o  470 163.22911376953127
o  471 169.72559356689453
o  472 118.4337776184082
o  473 132.8943328857422
o  474 78.9440040588379
o  475 101.9996810913086
-  476 65.0421516418457
o  477 205.2397186279297
o  478 110.85668716430665
o  479 74.91819915771485
o  480 132.9975082397461
o  481 95.68842544555665
-  482 38.316140747070314
o  483 119.58369369506836
o  484 121.46318206787109
o  485 272.42510986328125
o  486 402.87599487304686
o  487 89.61747131347656
o  488 93.54599304199219
o  489 159.11409606933594
o  490 97.07923431396485
o  491 114.45055160522462
o  492 153.15033721923828
o  493 106.8233154296875
o  494 200.5758804321289
o  495 88.51542282104492
o  496 121.22686080932617
o  497 153.01068267822268
o  498 409.6443115234375
o  499 107.94572982788087
o  500 243.5915267944336
o  501 85.22264556884765
o  502 169.7681411743164
o  503 88.75540084838867
o  504 113.13787231445313
o  505 122.05057296752929
o  506 109.0451644897461
-  507 34.304121017456055
-  508 66.06158294677735
o  509 84.1277069091797
o  510 71.37265701293946
-  511 38.018928909301756
o  512 89.66210021972657
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
... Generated these...
self.BehTrialMapList [(1, 0)]
self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27), 27: (0, 28), 28: (0, 29), 29: (0, 30), 30: (0, 31), 31: (0, 32), 32: (0, 33), 33: (0, 34), 34: (0, 35), 35: (0, 36), 36: (0, 37), 37: (0, 38), 38: (0, 39), 39: (0, 40), 40: (0, 41), 41: (0, 42), 42: (0, 43), 43: (0, 44), 44: (0, 45), 45: (0, 46), 46: (0, 47), 47: (0, 48), 48: (0, 49), 49: (0, 50), 50: (0, 51), 51: (0, 52), 52: (0, 53), 53: (0, 54), 54: (0, 55), 55: (0, 56), 56: (0, 57), 57: (0, 58), 58: (0, 59), 59: (0, 60), 60: (0, 61), 61: (0, 62), 62: (0, 63), 63: (0, 64), 64: (0, 65), 65: (0, 66), 66: (0, 67), 67: (0, 68), 68: (0, 69), 69: (0, 70), 70: (0, 71), 71: (0, 72), 72: (0, 73), 73: (0, 74), 74: (0, 75), 75: (0, 76), 76: (0, 77), 77: (0, 78), 78: (0, 79), 79: (0, 80), 80: (0, 81), 81: (0, 82), 82: (0, 83), 83: (0, 84), 84: (0, 85), 85: (0, 86), 86: (0, 87), 87: (0, 88), 88: (0, 89), 89: (0, 90), 90: (0, 91), 91: (0, 92), 92: (0, 93), 93: (0, 94), 94: (0, 95), 95: (0, 96), 96: (0, 97), 97: (0, 98), 98: (0, 99), 99: (0, 100), 100: (0, 101), 101: (0, 102), 102: (0, 103), 103: (0, 104), 104: (0, 105), 105: (0, 106), 106: (0, 107), 107: (0, 108), 108: (0, 109), 109: (0, 110), 110: (0, 111), 111: (0, 112), 112: (0, 113), 113: (0, 114), 114: (0, 115), 115: (0, 116), 116: (0, 117), 117: (0, 118), 118: (0, 119), 119: (0, 120), 120: (0, 121), 121: (0, 122), 122: (0, 123), 123: (0, 124), 124: (0, 125), 125: (0, 126), 126: (0, 127), 127: (0, 128), 128: (0, 129), 129: (0, 130), 130: (0, 131), 131: (0, 132), 132: (0, 133), 133: (0, 134), 134: (0, 135), 135: (0, 136), 136: (0, 137), 137: (0, 138), 138: (0, 139), 139: (0, 140), 140: (0, 141), 141: (0, 142), 142: (0, 143), 143: (0, 144), 144: (0, 145), 145: (0, 146), 146: (0, 147), 147: (0, 148), 148: (0, 149), 149: (0, 150), 150: (0, 151), 151: (0, 152), 152: (0, 153), 153: (0, 154), 154: (0, 155), 155: (0, 156), 156: (0, 157), 157: (0, 158), 158: (0, 159), 159: (0, 160), 160: (0, 161), 161: (0, 162), 162: (0, 163), 163: (0, 164), 164: (0, 165), 165: (0, 166), 166: (0, 167), 167: (0, 168), 168: (0, 169), 169: (0, 170), 170: (0, 171), 171: (0, 172), 172: (0, 173), 173: (0, 174), 174: (0, 175), 175: (0, 176), 176: (0, 177), 177: (0, 178), 178: (0, 179), 179: (0, 180), 180: (0, 181), 181: (0, 182), 182: (0, 183), 183: (0, 184), 184: (0, 185), 185: (0, 186), 186: (0, 187), 187: (0, 188), 188: (0, 189), 189: (0, 190), 190: (0, 191), 191: (0, 192), 192: (0, 193), 193: (0, 194), 194: (0, 195), 195: (0, 196), 196: (0, 197), 197: (0, 198), 198: (0, 199), 199: (0, 200), 200: (0, 201), 201: (0, 202), 202: (0, 203), 203: (0, 204), 204: (0, 205), 205: (0, 206), 206: (0, 207), 207: (0, 208), 208: (0, 209), 209: (0, 210), 210: (0, 211), 211: (0, 212), 212: (0, 213), 213: (0, 214), 214: (0, 215), 215: (0, 216), 216: (0, 217), 217: (0, 218), 218: (0, 219), 219: (0, 220), 220: (0, 221), 221: (0, 222), 222: (0, 223), 223: (0, 224), 224: (0, 225), 225: (0, 226), 226: (0, 227), 227: (0, 228), 228: (0, 229), 229: (0, 230), 230: (0, 231), 231: (0, 232), 232: (0, 233), 233: (0, 234), 234: (0, 235), 235: (0, 236), 236: (0, 237), 237: (0, 238), 238: (0, 239), 239: (0, 240), 240: (0, 241), 241: (0, 242), 242: (0, 243), 243: (0, 244), 244: (0, 245), 245: (0, 246), 246: (0, 247), 247: (0, 248), 248: (0, 249), 249: (0, 250), 250: (0, 251), 251: (0, 252), 252: (0, 253), 253: (0, 254), 254: (0, 255), 255: (0, 256), 256: (0, 257), 257: (0, 258), 258: (0, 259), 259: (0, 260), 260: (0, 261), 261: (0, 262), 262: (0, 263), 263: (0, 264), 264: (0, 265), 265: (0, 266), 266: (0, 267), 267: (0, 268), 268: (0, 269), 269: (0, 270), 270: (0, 271), 271: (0, 272), 272: (0, 273), 273: (0, 274), 274: (0, 275), 275: (0, 276), 276: (0, 277), 277: (0, 278), 278: (0, 279), 279: (0, 280), 280: (0, 281), 281: (0, 282), 282: (0, 283), 283: (0, 284), 284: (0, 285), 285: (0, 286), 286: (0, 287), 287: (0, 288), 288: (0, 289), 289: (0, 290), 290: (0, 291), 291: (0, 292), 292: (0, 293), 293: (0, 294), 294: (0, 295), 295: (0, 296), 296: (0, 297), 297: (0, 298), 298: (0, 299), 299: (0, 300), 300: (0, 301), 301: (0, 302), 302: (0, 303), 303: (0, 304), 304: (0, 305), 305: (0, 306), 306: (0, 307), 307: (0, 308), 308: (0, 309), 309: (0, 310), 310: (0, 311), 311: (0, 312), 312: (0, 313), 313: (0, 314), 314: (0, 315), 315: (0, 316), 316: (0, 317), 317: (0, 318), 318: (0, 319), 319: (0, 320), 320: (0, 321), 321: (0, 322), 322: (0, 323), 323: (0, 324), 324: (0, 325), 325: (0, 326), 326: (0, 327), 327: (0, 328), 328: (0, 329), 329: (0, 330), 330: (0, 331), 331: (0, 332), 332: (0, 333), 333: (0, 334), 334: (0, 335), 335: (0, 336), 336: (0, 337), 337: (0, 338), 338: (0, 339), 339: (0, 340), 340: (0, 341), 341: (0, 342), 342: (0, 343), 343: (0, 344), 344: (0, 345), 345: (0, 346), 346: (0, 347), 347: (0, 348), 348: (0, 349), 349: (0, 350), 350: (0, 351), 351: (0, 352), 352: (0, 353), 353: (0, 354), 354: (0, 355), 355: (0, 356), 356: (0, 357), 357: (0, 358), 358: (0, 359), 359: (0, 360), 360: (0, 361), 361: (0, 362), 362: (0, 363), 363: (0, 364), 364: (0, 365), 365: (0, 366), 366: (0, 367), 367: (0, 368), 368: (0, 369), 369: (0, 370), 370: (0, 371), 371: (0, 372), 372: (0, 373), 373: (0, 374), 374: (0, 375), 375: (0, 376), 376: (0, 377), 377: (0, 378), 378: (0, 379), 379: (0, 380), 380: (0, 381), 381: (0, 382), 382: (0, 383), 383: (0, 384), 384: (0, 385), 385: (0, 386), 386: (0, 387), 387: (0, 388), 388: (0, 389), 389: (0, 390), 390: (0, 391), 391: (0, 392), 392: (0, 393), 393: (0, 394), 394: (0, 395), 395: (0, 396), 396: (0, 397), 397: (0, 398), 398: (0, 399), 399: (0, 400), 400: (0, 401), 401: (0, 402), 402: (0, 403), 403: (0, 404), 404: (0, 405), 405: (0, 406), 406: (0, 407), 407: (0, 408), 408: (0, 409), 409: (0, 410), 410: (0, 411), 411: (0, 412), 412: (0, 413), 413: (0, 414), 414: (0, 415), 415: (0, 416), 416: (0, 417), 417: (0, 418), 418: (0, 419), 419: (0, 420), 420: (0, 421), 421: (0, 422), 422: (0, 423), 423: (0, 424), 424: (0, 425), 425: (0, 426), 426: (0, 427), 427: (0, 428), 428: (0, 429), 429: (0, 430), 430: (0, 431), 431: (0, 432), 432: (0, 433), 433: (0, 434), 434: (0, 435), 435: (0, 436), 436: (0, 437), 437: (0, 438), 438: (0, 439), 439: (0, 440), 440: (0, 441), 441: (0, 442), 442: (0, 443), 443: (0, 444), 444: (0, 445), 445: (0, 446), 446: (0, 447), 447: (0, 448), 448: (0, 449), 449: (0, 450), 450: (0, 451), 451: (0, 452), 452: (0, 453), 453: (0, 454), 454: (0, 455), 455: (0, 456), 456: (0, 457), 457: (0, 458), 458: (0, 459), 459: (0, 460), 460: (0, 461), 461: (0, 462), 462: (0, 463), 463: (0, 464), 464: (0, 465), 465: (0, 466), 466: (0, 467), 467: (0, 468), 468: (0, 469), 469: (0, 470), 470: (0, 471), 471: (0, 472), 472: (0, 473), 473: (0, 474), 474: (0, 475), 475: (0, 476), 476: (0, 477), 477: (0, 478), 478: (0, 479), 479: (0, 480), 480: (0, 481), 481: (0, 482), 482: (0, 483), 483: (0, 484), 484: (0, 485), 485: (0, 486), 486: (0, 487), 487: (0, 488), 488: (0, 489), 489: (0, 490), 490: (0, 491), 491: (0, 492), 492: (0, 493), 493: (0, 494), 494: (0, 495), 495: (0, 496), 496: (0, 497), 497: (0, 498), 498: (0, 499), 499: (0, 500), 500: (0, 501), 501: (0, 502), 502: (0, 503), 503: (0, 504), 504: (0, 505), 505: (0, 506), 506: (0, 507), 507: (0, 508), 508: (0, 509), 509: (0, 510), 510: (0, 511), 511: (0, 512), 512: (0, 513), 513: (0, 514), 514: (0, 515), 515: (0, 516), 516: (0, 517), 517: (0, 518), 518: (0, 519), 519: (0, 520), 520: (0, 521), 521: (0, 522), 522: (0, 523), 523: (0, 524), 524: (0, 525), 525: (0, 526), 526: (0, 527), 527: (0, 528), 528: (0, 529), 529: (0, 530), 530: (0, 531), 531: (0, 532), 532: (0, 533), 533: (0, 534), 534: (0, 535), 535: (0, 536), 536: (0, 537), 537: (0, 538), 538: (0, 539), 539: (0, 540), 540: (0, 541), 541: (0, 542), 542: (0, 543), 543: (0, 544), 544: (0, 545), 545: (0, 546), 546: (0, 547), 547: (0, 548), 548: (0, 549), 549: (0, 550), 550: (0, 551), 551: (0, 552), 552: (0, 553), 553: (0, 554), 554: (0, 555), 555: (0, 556), 556: (0, 557), 557: (0, 558), 558: (0, 559), 559: (0, 560), 560: (0, 561), 561: (0, 562), 562: (0, 563), 563: (0, 564), 564: (0, 565), 565: (0, 566), 566: (0, 567), 567: (0, 568), 568: (0, 569), 569: (0, 570), 570: (0, 571), 571: (0, 572), 572: (0, 573), 573: (0, 574), 574: (0, 575), 575: (0, 576), 576: (0, 577), 577: (0, 578), 578: (0, 579), 579: (0, 580), 580: (0, 581), 581: (0, 582), 582: (0, 583), 583: (0, 584), 584: (0, 585), 585: (0, 586), 586: (0, 587), 587: (0, 588), 588: (0, 589), 589: (0, 590), 590: (0, 591), 591: (0, 592), 592: (0, 593), 593: (0, 594), 594: (0, 595), 595: (0, 596), 596: (0, 597), 597: (0, 598), 598: (0, 599), 599: (0, 600), 600: (0, 601), 601: (0, 602), 602: (0, 603), 603: (0, 604), 604: (0, 605), 605: (0, 606), 606: (0, 607), 607: (0, 608), 608: (0, 609), 609: (0, 610), 610: (0, 611), 611: (0, 612), 612: (0, 613), 613: (0, 614), 614: (0, 615), 615: (0, 616), 616: (0, 617), 617: (0, 618), 618: (0, 619), 619: (0, 620), 620: (0, 621), 621: (0, 622), 622: (0, 623), 623: (0, 624), 624: (0, 625), 625: (0, 626), 626: (0, 627), 627: (0, 628), 628: (0, 629), 629: (0, 630), 630: (0, 631), 631: (0, 632), 632: (0, 633), 633: (0, 634), 634: (0, 635), 635: (0, 636), 636: (0, 637), 637: (0, 638), 638: (0, 639), 639: (0, 640), 640: (0, 641), 641: (0, 642), 642: (0, 643), 643: (0, 644), 644: (0, 645), 645: (0, 646), 646: (0, 647), 647: (0, 648), 648: (0, 649), 649: (0, 650), 650: (0, 651), 651: (0, 652), 652: (0, 653), 653: (0, 654), 654: (0, 655), 655: (0, 656), 656: (0, 657), 657: (0, 658), 658: (0, 659), 659: (0, 660), 660: (0, 661), 661: (0, 662), 662: (0, 663), 663: (0, 664), 664: (0, 665), 665: (0, 666), 666: (0, 667), 667: (0, 668), 668: (0, 669), 669: (0, 670), 670: (0, 671), 671: (0, 672), 672: (0, 673), 673: (0, 674), 674: (0, 675), 675: (0, 676), 676: (0, 677), 677: (0, 678), 678: (0, 679), 679: (0, 680), 680: (0, 681), 681: (0, 682), 682: (0, 683), 683: (0, 684), 684: (0, 685), 685: (0, 686), 686: (0, 687), 687: (0, 688), 688: (0, 689), 689: (0, 690), 690: (0, 691), 691: (0, 692), 692: (0, 693), 693: (0, 694), 694: (0, 695), 695: (0, 696), 696: (0, 697), 697: (0, 698), 698: (0, 699), 699: (0, 700), 700: (0, 701), 701: (0, 702), 702: (0, 703), 703: (0, 704), 704: (0, 705), 705: (0, 706), 706: (0, 707), 707: (0, 708), 708: (0, 709), 709: (0, 710), 710: (0, 711), 711: (0, 712), 712: (0, 713), 713: (0, 714), 714: (0, 715), 715: (0, 716), 716: (0, 717), 717: (0, 718), 718: (0, 719), 719: (0, 720), 720: (0, 721), 721: (0, 722), 722: (0, 723), 723: (0, 724), 724: (0, 725), 725: (0, 726), 726: (0, 727), 727: (0, 728), 728: (0, 729), 729: (0, 730), 730: (0, 731), 731: (0, 732), 732: (0, 733), 733: (0, 734), 734: (0, 735), 735: (0, 736), 736: (0, 737), 737: (0, 738), 738: (0, 739), 739: (0, 740), 740: (0, 741), 741: (0, 742), 742: (0, 743), 743: (0, 744), 744: (0, 745), 745: (0, 746), 746: (0, 747), 747: (0, 748), 748: (0, 749), 749: (0, 750), 750: (0, 751), 751: (0, 752), 752: (0, 753), 753: (0, 754), 754: (0, 755), 755: (0, 756), 756: (0, 757), 757: (0, 758), 758: (0, 759), 759: (0, 760), 760: (0, 761), 761: (0, 762), 762: (0, 763), 763: (0, 764), 764: (0, 765), 765: (0, 766), 766: (0, 767), 767: (0, 768), 768: (0, 769), 769: (0, 770), 770: (0, 771), 771: (0, 772), 772: (0, 773), 773: (0, 774), 774: (0, 775), 775: (0, 776), 776: (0, 777), 777: (0, 778), 778: (0, 779), 779: (0, 780)}
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/Trials.pkl
** SKIPPING EXTRACTION, since was able to load snippets, for: 
(animal, DATE, which_level, ANALY_VER, session)
Pancho 221014 trial rulesw 0
Got these LIST_VAR and LIST_VARS_CONJUNCTION:
['epoch', 'epoch', 'character', 'seqc_0_loc_shape', 'seqc_0_loc', 'seqc_1_loc_shape']
[['epochset'], ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh'], ['epoch', 'epochset'], ['epoch', 'epochset'], ['epoch', 'epochset'], ['epoch', 'epochset', 'seqc_0_loc_shape']]
Got these LIST_VAR and LIST_VARS_CONJUNCTION:
['epoch', 'epoch', 'character', 'seqc_0_loc_shape', 'seqc_0_loc', 'seqc_1_loc_shape']
[['epochset'], ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh'], ['epoch', 'epochset'], ['epoch', 'epochset'], ['epoch', 'epochset'], ['epoch', 'epochset', 'seqc_0_loc_shape']]
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221014*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758
session:  0
1
Beh Sessions that exist on this date:  {221014: [(1, 'grammardircolor3d')]}
taking this beh session: 1
------------------------------
Loading this neural session: 0
Loading these beh expts: ['grammardircolor3d']
Loading these beh sessions: [1]
Using this beh_trial_map_list: [(1, 0)]
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*221014*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758
{'filename_components_hyphened': ['Pancho', '221014', '151758'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/221014'], 'basedirs_filenames': ['221014', 'Pancho-221014-151758'], 'filename_final_ext': 'Pancho-221014-151758', 'filename_final_noext': 'Pancho-221014-151758'}
FOund this path for spikes:  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758/spikes_tdt_quick-4.5
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758/Pancho-221014-151758
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/221014/Pancho-221014-151758/spikes_tdt_quick-4.5
final_dir_name  --  Pancho-221014-151758
time  --  151758
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/221014.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
o  1 70.07001495361328
o  2 119.39619674682618
o  3 92.84320068359375
-  4 69.22887573242187
o  5 98.32090225219727
o  6 74.40170669555664
o  7 133.85044555664064
o  8 83.40576629638672
o  9 217.99324645996094
o  10 80.13178100585938
o  11 149.97047576904296
o  12 78.5964958190918
o  13 287.0962341308594
-  14 65.85949630737305
o  15 152.89322814941406
o  16 90.90266876220703
o  17 83.47337341308594
o  18 125.39983520507813
o  19 672.7095642089844
-  20 54.108445739746095
o  21 110.02447204589843
o  22 110.56966171264651
-  23 51.97209129333496
o  24 87.21598510742187
-  25 62.71386032104492
o  26 388.7219604492188
o  27 139.43396453857423
o  28 75.85004806518555
o  29 185.2539810180664
o  30 94.97772064208985
o  31 182.09425659179686
o  32 97.87480926513672
o  33 171.04986724853518
o  34 113.90146942138672
-  35 45.129401016235356
o  36 328.52210388183596
o  37 132.3924743652344
o  38 108.6971534729004
o  39 71.29547958374023
o  40 318.4283813476563
o  41 169.03667144775392
o  42 173.64176025390626
o  43 166.9941619873047
o  44 187.22461395263673
o  45 1112.8847900390624
-  46 69.86896057128907
o  47 151.3925582885742
-  48 48.022822570800784
o  49 113.45245666503908
o  50 85.42860488891601
o  51 380.717822265625
o  52 92.12309417724609
o  53 187.13748016357422
o  54 128.95819396972658
o  55 503.33326416015626
o  56 98.39811401367187
o  57 157.87503814697266
o  58 101.59659881591797
o  59 168.6999282836914
o  60 103.42988662719728
o  61 118.33864288330078
o  62 105.55364227294922
o  63 277.8090026855469
o  64 97.60985717773437
o  65 85.44774856567383
o  66 93.93840942382813
o  67 83.75935974121094
o  68 160.73418273925782
o  69 103.59523696899414
-  70 58.05966758728027
o  71 72.60229034423828
o  72 92.20206680297852
o  73 75.65871124267578
o  74 147.55613403320314
o  75 102.96719818115236
o  76 83.77497177124025
o  77 80.76305389404298
o  78 174.86832427978516
o  79 116.7929458618164
o  80 111.48871154785158
-  81 61.312757110595705
-  82 62.32795677185059
-  83 62.55381202697754
-  84 63.95877265930176
-  85 60.64043350219727
o  86 83.7136817932129
-  87 47.25300407409668
-  88 69.0390007019043
-  89 61.06460266113281
o  90 98.20525665283203
o  91 71.99164581298828
o  92 71.03368072509765
-  93 58.629022216796876
o  94 90.50437088012696
-  95 57.47223167419434
o  96 74.82312393188477
o  97 137.08795623779298
o  98 97.6595184326172
o  99 118.54615173339845
o  100 135.40330200195314
o  101 76.31842422485352
o  102 95.20481491088867
-  103 59.958503341674806
-  104 61.98924522399902
o  105 137.5545166015625
o  106 113.866561126709
-  107 47.28807525634766
o  108 81.5717758178711
-  109 69.14262313842774
-  110 63.746114349365236
o  111 73.84794845581055
o  112 87.55932159423828
o  113 109.39519119262695
o  114 167.9239959716797
o  115 110.37302169799806
o  116 145.57748107910157
-  117 59.21947059631348
o  118 81.35688858032226
o  119 125.3233154296875
o  120 70.51404113769532
o  121 212.1708709716797
o  122 71.28085861206054
o  123 83.35153121948242
-  124 69.20288772583008
o  125 75.50782775878906
o  126 114.16984710693359
o  127 70.20228271484375
-  128 47.0530029296875
-  129 47.52632522583008
o  130 506.9996643066406
-  131 66.01510009765624
o  132 266.7898345947266
o  133 121.9940055847168
o  134 112.57147293090821
o  135 93.09648056030275
o  136 217.29194946289064
o  137 75.61439514160156
o  138 148.77705078125004
-  139 66.89347534179687
o  140 147.45016326904297
o  141 85.3313217163086
o  142 519.0963684082031
o  143 93.15379791259767
o  144 159.40061492919924
o  145 135.5989959716797
o  146 85.28827819824218
-  147 55.696991729736325
o  148 85.39910736083985
-  149 57.2765510559082
o  150 376.0773620605469
o  151 105.16657180786133
o  152 99.39850158691407
o  153 92.89810485839844
o  154 96.99705886840822
-  155 55.53313941955567
o  156 93.8808090209961
o  157 92.53886184692384
o  158 324.9564208984375
o  159 124.8187942504883
o  160 78.78099060058594
o  161 136.4160614013672
o  162 97.67760238647462
o  163 144.65334625244142
o  164 71.17960968017579
o  165 99.30065002441407
o  166 87.49186096191406
o  167 123.55106582641604
-  168 66.55731201171875
o  169 78.96839294433593
o  170 103.47154693603515
-  171 66.33852081298828
-  172 59.08343734741211
o  173 222.3491241455078
o  174 70.52276306152343
o  175 81.11002731323242
-  176 52.75696220397949
o  177 176.69336395263673
o  178 95.22745971679687
o  179 256.5112243652344
o  180 74.18759384155274
o  181 149.349072265625
o  182 76.17356796264649
-  183 65.164794921875
o  184 81.00478668212891
-  185 29.589839935302734
o  186 84.15338439941407
-  187 61.63133010864258
o  188 83.21488418579102
o  189 122.4232147216797
-  190 64.96926345825196
o  191 103.0073486328125
o  192 84.61698379516602
o  193 157.16852264404298
o  194 85.44417877197266
o  195 146.14857635498046
o  196 75.75224914550782
o  197 298.53297424316406
-  198 53.37626152038574
o  199 72.83033828735351
o  200 80.55078811645508
o  201 70.17307739257812
o  202 103.55473785400392
o  203 217.7834930419923
-  204 67.74217224121095
o  205 73.68449554443359
-  206 69.2004524230957
o  207 79.99010620117188
-  208 62.41448669433594
-  209 60.4757682800293
o  210 82.23128051757813
o  211 81.03816299438476
-  212 68.08968200683594
o  213 74.54648132324219
-  214 62.92794494628906
o  215 79.09303283691406
o  216 78.2021713256836
-  217 68.18418273925782
o  218 104.41276626586914
o  219 125.95899887084961
-  220 69.62132186889649
o  221 102.92330703735352
o  222 80.43453674316406
o  223 76.54113693237305
o  224 89.9914321899414
o  225 88.48144226074218
o  226 84.55942916870117
o  227 95.62439804077148
-  228 62.50282516479493
o  229 70.07327728271486
-  230 61.87736473083496
-  231 59.57387809753418
-  232 46.021240615844725
o  233 120.24692916870117
-  234 59.3451717376709
o  235 106.4603385925293
o  236 71.90728302001953
o  237 99.1531120300293
o  238 70.190185546875
-  239 55.55191116333008
o  240 89.48558731079102
o  241 74.94077987670899
-  242 63.67621765136719
o  243 81.63639144897462
o  244 77.05525741577148
o  245 81.77744903564454
-  246 63.73132133483887
-  247 68.35110855102539
o  248 101.29867553710938
-  249 65.83886337280273
-  250 51.100903320312504
o  251 82.04973831176758
o  252 115.724755859375
o  253 74.58620529174804
o  254 80.83525314331055
o  255 113.17791976928712
o  256 101.31348800659181
o  257 159.08499298095705
o  258 94.20059432983399
o  259 102.81791915893555
o  260 236.57092895507813
o  261 95.06597061157227
o  262 96.87097320556641
o  263 143.87831115722656
o  264 103.08134307861329
o  265 117.13843383789063
-  266 68.36220932006836
o  267 107.21416244506837
-  268 59.687156677246094
o  269 96.59871978759766
o  270 89.45384216308594
-  271 62.13063697814941
o  272 113.42986602783203
o  273 257.9465393066406
o  274 102.45425567626954
o  275 79.47071914672851
o  276 78.85202941894532
o  277 101.87722167968751
o  278 141.11019744873047
o  279 87.23502655029297
o  280 83.68796615600586
o  281 129.91513214111328
o  282 111.02151489257812
o  283 73.66617050170899
o  284 98.24857025146484
o  285 120.51009902954101
o  286 99.9341033935547
o  287 75.1629852294922
o  288 99.64851989746094
o  289 70.71280822753906
o  290 99.87283935546876
o  291 71.15840148925781
-  292 40.82721862792969
-  293 53.69741592407227
o  294 95.40920333862306
o  295 83.4584327697754
o  296 80.23085021972656
-  297 57.253458404541014
-  298 64.20526733398438
o  299 71.50268783569337
o  300 117.87270889282227
-  301 60.22169799804688
o  302 98.3749153137207
o  303 111.82118301391601
o  304 77.43159637451173
-  305 59.03513679504395
o  306 87.82177352905273
-  307 63.47863883972168
-  308 66.15353164672852
o  309 75.4908348083496
o  310 81.5715721130371
o  311 77.54899063110352
o  312 77.47124252319337
-  313 52.21627426147461
-  314 60.977572250366215
-  315 60.01525382995606
o  316 129.323876953125
o  317 70.99810943603516
o  318 85.30386962890626
o  319 76.84476013183594
o  320 82.29061279296876
-  321 54.160247802734375
-  322 57.92383155822754
o  323 103.98460083007812
-  324 52.92584228515625
-  325 51.48751373291016
o  326 71.3086654663086
o  327 189.77355194091797
o  328 101.48578262329103
o  329 101.03164520263672
o  330 72.49633407592773
o  331 150.0833969116211
o  332 106.21741256713868
o  333 87.82629928588868
o  334 94.89099655151368
-  335 52.30215682983398
-  336 42.45133285522461
-  337 47.5807903289795
-  338 68.51712493896484
o  339 70.14190444946288
-  340 40.18007049560547
o  341 73.37190551757813
-  342 54.66943550109863
o  343 89.49747848510742
o  344 170.4292007446289
o  345 83.2826431274414
o  346 72.14065856933594
o  347 94.18527221679688
o  348 94.83524856567382
o  349 103.85513610839844
-  350 69.73488922119141
o  351 93.77596282958984
o  352 112.58528900146484
-  353 60.82431106567383
-  354 59.3052303314209
o  355 78.40587692260742
o  356 72.77914657592774
-  357 42.72082023620605
o  358 78.34797897338868
-  359 57.17334175109863
o  360 87.67100067138672
o  361 90.12119674682617
o  362 73.76628036499024
-  363 67.43524322509766
-  364 67.72242126464845
-  365 48.718485260009764
o  366 179.67342071533204
-  367 67.64673690795898
o  368 148.4550994873047
-  369 64.92953338623047
o  370 83.39576034545898
o  371 87.73883819580078
-  372 59.41602096557617
o  373 77.99143142700196
o  374 75.39197540283203
o  375 91.05914764404297
o  376 185.17770233154297
-  377 64.09656219482422
-  378 65.49094696044922
o  379 133.8427963256836
-  380 67.42978286743164
o  381 103.5524658203125
-  382 61.05181541442871
-  383 53.119581604003905
o  384 87.47046279907228
o  385 372.5703063964844
o  386 125.24899139404297
o  387 136.37142639160157
o  388 143.10113830566405
o  389 261.8381317138672
o  390 190.68931274414064
-  391 52.82984085083008
-  392 66.64052886962891
o  393 112.93611450195314
o  394 76.13117294311523
o  395 101.09572677612304
o  396 127.67452545166016
o  397 83.11093673706054
o  398 70.64347915649414
o  399 282.0115600585938
o  400 118.83141555786133
o  401 102.97473983764648
o  402 163.53166656494142
o  403 125.26416702270508
o  404 130.1272933959961
o  405 163.9676040649414
o  406 94.40434112548829
o  407 236.85559692382813
o  408 116.06289443969727
-  409 53.045603561401364
o  410 192.38845825195312
o  411 91.89567184448244
o  412 130.53514709472657
o  413 124.47970275878907
o  414 163.1252670288086
o  415 154.1702575683594
o  416 88.0114356994629
o  417 112.92021255493164
o  418 140.19553833007814
o  419 76.2939582824707
-  420 67.96377944946289
o  421 107.48730621337891
o  422 121.87740631103516
o  423 116.81532821655274
-  424 65.70640335083007
o  425 77.84573974609376
-  426 56.59717636108399
-  427 68.66922607421876
-  428 27.911373329162597
o  429 88.45717163085938
o  430 92.20145111083984
-  431 56.619218826293945
-  432 52.50416030883789
o  433 128.12307739257812
o  434 142.78850708007812
o  435 235.12962646484377
o  436 128.4585906982422
o  437 74.89149551391601
-  438 63.58048934936524
o  439 102.44144668579102
o  440 118.59660491943359
o  441 156.44501495361328
o  442 73.18268432617188
o  443 432.3601013183594
-  444 33.045706558227536
o  445 92.65974960327148
-  446 34.87873306274414
o  447 96.79855804443359
-  448 30.228604507446292
o  449 110.73665161132813
o  450 103.70192337036133
o  451 262.52626953125
o  452 121.67299880981446
o  453 93.85409011840821
o  454 154.58413391113282
o  455 123.69183502197265
-  456 58.5660140991211
o  457 130.89010467529297
o  458 73.74380264282226
o  459 190.92642517089843
o  460 173.3803466796875
o  461 142.13896636962892
o  462 77.71271896362305
o  463 78.97619171142578
o  464 146.0954345703125
o  465 125.60858917236328
-  466 51.99344177246094
o  467 105.5820671081543
o  468 119.03297271728516
o  469 132.49508361816407
o  470 163.22911376953127
o  471 169.72559356689453
o  472 118.4337776184082
o  473 132.8943328857422
o  474 78.9440040588379
o  475 101.9996810913086
-  476 65.0421516418457
o  477 205.2397186279297
o  478 110.85668716430665
o  479 74.91819915771485
o  480 132.9975082397461
o  481 95.68842544555665
-  482 38.316140747070314
o  483 119.58369369506836
o  484 121.46318206787109
o  485 272.42510986328125
o  486 402.87599487304686
o  487 89.61747131347656
o  488 93.54599304199219
o  489 159.11409606933594
o  490 97.07923431396485
o  491 114.45055160522462
o  492 153.15033721923828
o  493 106.8233154296875
o  494 200.5758804321289
o  495 88.51542282104492
o  496 121.22686080932617
o  497 153.01068267822268
o  498 409.6443115234375
o  499 107.94572982788087
o  500 243.5915267944336
o  501 85.22264556884765
o  502 169.7681411743164
o  503 88.75540084838867
o  504 113.13787231445313
o  505 122.05057296752929
o  506 109.0451644897461
-  507 34.304121017456055
-  508 66.06158294677735
o  509 84.1277069091797
o  510 71.37265701293946
-  511 38.018928909301756
o  512 89.66210021972657
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/221014/Pancho-221014-151758/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
... Generated these...
self.BehTrialMapList [(1, 0)]
self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27), 27: (0, 28), 28: (0, 29), 29: (0, 30), 30: (0, 31), 31: (0, 32), 32: (0, 33), 33: (0, 34), 34: (0, 35), 35: (0, 36), 36: (0, 37), 37: (0, 38), 38: (0, 39), 39: (0, 40), 40: (0, 41), 41: (0, 42), 42: (0, 43), 43: (0, 44), 44: (0, 45), 45: (0, 46), 46: (0, 47), 47: (0, 48), 48: (0, 49), 49: (0, 50), 50: (0, 51), 51: (0, 52), 52: (0, 53), 53: (0, 54), 54: (0, 55), 55: (0, 56), 56: (0, 57), 57: (0, 58), 58: (0, 59), 59: (0, 60), 60: (0, 61), 61: (0, 62), 62: (0, 63), 63: (0, 64), 64: (0, 65), 65: (0, 66), 66: (0, 67), 67: (0, 68), 68: (0, 69), 69: (0, 70), 70: (0, 71), 71: (0, 72), 72: (0, 73), 73: (0, 74), 74: (0, 75), 75: (0, 76), 76: (0, 77), 77: (0, 78), 78: (0, 79), 79: (0, 80), 80: (0, 81), 81: (0, 82), 82: (0, 83), 83: (0, 84), 84: (0, 85), 85: (0, 86), 86: (0, 87), 87: (0, 88), 88: (0, 89), 89: (0, 90), 90: (0, 91), 91: (0, 92), 92: (0, 93), 93: (0, 94), 94: (0, 95), 95: (0, 96), 96: (0, 97), 97: (0, 98), 98: (0, 99), 99: (0, 100), 100: (0, 101), 101: (0, 102), 102: (0, 103), 103: (0, 104), 104: (0, 105), 105: (0, 106), 106: (0, 107), 107: (0, 108), 108: (0, 109), 109: (0, 110), 110: (0, 111), 111: (0, 112), 112: (0, 113), 113: (0, 114), 114: (0, 115), 115: (0, 116), 116: (0, 117), 117: (0, 118), 118: (0, 119), 119: (0, 120), 120: (0, 121), 121: (0, 122), 122: (0, 123), 123: (0, 124), 124: (0, 125), 125: (0, 126), 126: (0, 127), 127: (0, 128), 128: (0, 129), 129: (0, 130), 130: (0, 131), 131: (0, 132), 132: (0, 133), 133: (0, 134), 134: (0, 135), 135: (0, 136), 136: (0, 137), 137: (0, 138), 138: (0, 139), 139: (0, 140), 140: (0, 141), 141: (0, 142), 142: (0, 143), 143: (0, 144), 144: (0, 145), 145: (0, 146), 146: (0, 147), 147: (0, 148), 148: (0, 149), 149: (0, 150), 150: (0, 151), 151: (0, 152), 152: (0, 153), 153: (0, 154), 154: (0, 155), 155: (0, 156), 156: (0, 157), 157: (0, 158), 158: (0, 159), 159: (0, 160), 160: (0, 161), 161: (0, 162), 162: (0, 163), 163: (0, 164), 164: (0, 165), 165: (0, 166), 166: (0, 167), 167: (0, 168), 168: (0, 169), 169: (0, 170), 170: (0, 171), 171: (0, 172), 172: (0, 173), 173: (0, 174), 174: (0, 175), 175: (0, 176), 176: (0, 177), 177: (0, 178), 178: (0, 179), 179: (0, 180), 180: (0, 181), 181: (0, 182), 182: (0, 183), 183: (0, 184), 184: (0, 185), 185: (0, 186), 186: (0, 187), 187: (0, 188), 188: (0, 189), 189: (0, 190), 190: (0, 191), 191: (0, 192), 192: (0, 193), 193: (0, 194), 194: (0, 195), 195: (0, 196), 196: (0, 197), 197: (0, 198), 198: (0, 199), 199: (0, 200), 200: (0, 201), 201: (0, 202), 202: (0, 203), 203: (0, 204), 204: (0, 205), 205: (0, 206), 206: (0, 207), 207: (0, 208), 208: (0, 209), 209: (0, 210), 210: (0, 211), 211: (0, 212), 212: (0, 213), 213: (0, 214), 214: (0, 215), 215: (0, 216), 216: (0, 217), 217: (0, 218), 218: (0, 219), 219: (0, 220), 220: (0, 221), 221: (0, 222), 222: (0, 223), 223: (0, 224), 224: (0, 225), 225: (0, 226), 226: (0, 227), 227: (0, 228), 228: (0, 229), 229: (0, 230), 230: (0, 231), 231: (0, 232), 232: (0, 233), 233: (0, 234), 234: (0, 235), 235: (0, 236), 236: (0, 237), 237: (0, 238), 238: (0, 239), 239: (0, 240), 240: (0, 241), 241: (0, 242), 242: (0, 243), 243: (0, 244), 244: (0, 245), 245: (0, 246), 246: (0, 247), 247: (0, 248), 248: (0, 249), 249: (0, 250), 250: (0, 251), 251: (0, 252), 252: (0, 253), 253: (0, 254), 254: (0, 255), 255: (0, 256), 256: (0, 257), 257: (0, 258), 258: (0, 259), 259: (0, 260), 260: (0, 261), 261: (0, 262), 262: (0, 263), 263: (0, 264), 264: (0, 265), 265: (0, 266), 266: (0, 267), 267: (0, 268), 268: (0, 269), 269: (0, 270), 270: (0, 271), 271: (0, 272), 272: (0, 273), 273: (0, 274), 274: (0, 275), 275: (0, 276), 276: (0, 277), 277: (0, 278), 278: (0, 279), 279: (0, 280), 280: (0, 281), 281: (0, 282), 282: (0, 283), 283: (0, 284), 284: (0, 285), 285: (0, 286), 286: (0, 287), 287: (0, 288), 288: (0, 289), 289: (0, 290), 290: (0, 291), 291: (0, 292), 292: (0, 293), 293: (0, 294), 294: (0, 295), 295: (0, 296), 296: (0, 297), 297: (0, 298), 298: (0, 299), 299: (0, 300), 300: (0, 301), 301: (0, 302), 302: (0, 303), 303: (0, 304), 304: (0, 305), 305: (0, 306), 306: (0, 307), 307: (0, 308), 308: (0, 309), 309: (0, 310), 310: (0, 311), 311: (0, 312), 312: (0, 313), 313: (0, 314), 314: (0, 315), 315: (0, 316), 316: (0, 317), 317: (0, 318), 318: (0, 319), 319: (0, 320), 320: (0, 321), 321: (0, 322), 322: (0, 323), 323: (0, 324), 324: (0, 325), 325: (0, 326), 326: (0, 327), 327: (0, 328), 328: (0, 329), 329: (0, 330), 330: (0, 331), 331: (0, 332), 332: (0, 333), 333: (0, 334), 334: (0, 335), 335: (0, 336), 336: (0, 337), 337: (0, 338), 338: (0, 339), 339: (0, 340), 340: (0, 341), 341: (0, 342), 342: (0, 343), 343: (0, 344), 344: (0, 345), 345: (0, 346), 346: (0, 347), 347: (0, 348), 348: (0, 349), 349: (0, 350), 350: (0, 351), 351: (0, 352), 352: (0, 353), 353: (0, 354), 354: (0, 355), 355: (0, 356), 356: (0, 357), 357: (0, 358), 358: (0, 359), 359: (0, 360), 360: (0, 361), 361: (0, 362), 362: (0, 363), 363: (0, 364), 364: (0, 365), 365: (0, 366), 366: (0, 367), 367: (0, 368), 368: (0, 369), 369: (0, 370), 370: (0, 371), 371: (0, 372), 372: (0, 373), 373: (0, 374), 374: (0, 375), 375: (0, 376), 376: (0, 377), 377: (0, 378), 378: (0, 379), 379: (0, 380), 380: (0, 381), 381: (0, 382), 382: (0, 383), 383: (0, 384), 384: (0, 385), 385: (0, 386), 386: (0, 387), 387: (0, 388), 388: (0, 389), 389: (0, 390), 390: (0, 391), 391: (0, 392), 392: (0, 393), 393: (0, 394), 394: (0, 395), 395: (0, 396), 396: (0, 397), 397: (0, 398), 398: (0, 399), 399: (0, 400), 400: (0, 401), 401: (0, 402), 402: (0, 403), 403: (0, 404), 404: (0, 405), 405: (0, 406), 406: (0, 407), 407: (0, 408), 408: (0, 409), 409: (0, 410), 410: (0, 411), 411: (0, 412), 412: (0, 413), 413: (0, 414), 414: (0, 415), 415: (0, 416), 416: (0, 417), 417: (0, 418), 418: (0, 419), 419: (0, 420), 420: (0, 421), 421: (0, 422), 422: (0, 423), 423: (0, 424), 424: (0, 425), 425: (0, 426), 426: (0, 427), 427: (0, 428), 428: (0, 429), 429: (0, 430), 430: (0, 431), 431: (0, 432), 432: (0, 433), 433: (0, 434), 434: (0, 435), 435: (0, 436), 436: (0, 437), 437: (0, 438), 438: (0, 439), 439: (0, 440), 440: (0, 441), 441: (0, 442), 442: (0, 443), 443: (0, 444), 444: (0, 445), 445: (0, 446), 446: (0, 447), 447: (0, 448), 448: (0, 449), 449: (0, 450), 450: (0, 451), 451: (0, 452), 452: (0, 453), 453: (0, 454), 454: (0, 455), 455: (0, 456), 456: (0, 457), 457: (0, 458), 458: (0, 459), 459: (0, 460), 460: (0, 461), 461: (0, 462), 462: (0, 463), 463: (0, 464), 464: (0, 465), 465: (0, 466), 466: (0, 467), 467: (0, 468), 468: (0, 469), 469: (0, 470), 470: (0, 471), 471: (0, 472), 472: (0, 473), 473: (0, 474), 474: (0, 475), 475: (0, 476), 476: (0, 477), 477: (0, 478), 478: (0, 479), 479: (0, 480), 480: (0, 481), 481: (0, 482), 482: (0, 483), 483: (0, 484), 484: (0, 485), 485: (0, 486), 486: (0, 487), 487: (0, 488), 488: (0, 489), 489: (0, 490), 490: (0, 491), 491: (0, 492), 492: (0, 493), 493: (0, 494), 494: (0, 495), 495: (0, 496), 496: (0, 497), 497: (0, 498), 498: (0, 499), 499: (0, 500), 500: (0, 501), 501: (0, 502), 502: (0, 503), 503: (0, 504), 504: (0, 505), 505: (0, 506), 506: (0, 507), 507: (0, 508), 508: (0, 509), 509: (0, 510), 510: (0, 511), 511: (0, 512), 512: (0, 513), 513: (0, 514), 514: (0, 515), 515: (0, 516), 516: (0, 517), 517: (0, 518), 518: (0, 519), 519: (0, 520), 520: (0, 521), 521: (0, 522), 522: (0, 523), 523: (0, 524), 524: (0, 525), 525: (0, 526), 526: (0, 527), 527: (0, 528), 528: (0, 529), 529: (0, 530), 530: (0, 531), 531: (0, 532), 532: (0, 533), 533: (0, 534), 534: (0, 535), 535: (0, 536), 536: (0, 537), 537: (0, 538), 538: (0, 539), 539: (0, 540), 540: (0, 541), 541: (0, 542), 542: (0, 543), 543: (0, 544), 544: (0, 545), 545: (0, 546), 546: (0, 547), 547: (0, 548), 548: (0, 549), 549: (0, 550), 550: (0, 551), 551: (0, 552), 552: (0, 553), 553: (0, 554), 554: (0, 555), 555: (0, 556), 556: (0, 557), 557: (0, 558), 558: (0, 559), 559: (0, 560), 560: (0, 561), 561: (0, 562), 562: (0, 563), 563: (0, 564), 564: (0, 565), 565: (0, 566), 566: (0, 567), 567: (0, 568), 568: (0, 569), 569: (0, 570), 570: (0, 571), 571: (0, 572), 572: (0, 573), 573: (0, 574), 574: (0, 575), 575: (0, 576), 576: (0, 577), 577: (0, 578), 578: (0, 579), 579: (0, 580), 580: (0, 581), 581: (0, 582), 582: (0, 583), 583: (0, 584), 584: (0, 585), 585: (0, 586), 586: (0, 587), 587: (0, 588), 588: (0, 589), 589: (0, 590), 590: (0, 591), 591: (0, 592), 592: (0, 593), 593: (0, 594), 594: (0, 595), 595: (0, 596), 596: (0, 597), 597: (0, 598), 598: (0, 599), 599: (0, 600), 600: (0, 601), 601: (0, 602), 602: (0, 603), 603: (0, 604), 604: (0, 605), 605: (0, 606), 606: (0, 607), 607: (0, 608), 608: (0, 609), 609: (0, 610), 610: (0, 611), 611: (0, 612), 612: (0, 613), 613: (0, 614), 614: (0, 615), 615: (0, 616), 616: (0, 617), 617: (0, 618), 618: (0, 619), 619: (0, 620), 620: (0, 621), 621: (0, 622), 622: (0, 623), 623: (0, 624), 624: (0, 625), 625: (0, 626), 626: (0, 627), 627: (0, 628), 628: (0, 629), 629: (0, 630), 630: (0, 631), 631: (0, 632), 632: (0, 633), 633: (0, 634), 634: (0, 635), 635: (0, 636), 636: (0, 637), 637: (0, 638), 638: (0, 639), 639: (0, 640), 640: (0, 641), 641: (0, 642), 642: (0, 643), 643: (0, 644), 644: (0, 645), 645: (0, 646), 646: (0, 647), 647: (0, 648), 648: (0, 649), 649: (0, 650), 650: (0, 651), 651: (0, 652), 652: (0, 653), 653: (0, 654), 654: (0, 655), 655: (0, 656), 656: (0, 657), 657: (0, 658), 658: (0, 659), 659: (0, 660), 660: (0, 661), 661: (0, 662), 662: (0, 663), 663: (0, 664), 664: (0, 665), 665: (0, 666), 666: (0, 667), 667: (0, 668), 668: (0, 669), 669: (0, 670), 670: (0, 671), 671: (0, 672), 672: (0, 673), 673: (0, 674), 674: (0, 675), 675: (0, 676), 676: (0, 677), 677: (0, 678), 678: (0, 679), 679: (0, 680), 680: (0, 681), 681: (0, 682), 682: (0, 683), 683: (0, 684), 684: (0, 685), 685: (0, 686), 686: (0, 687), 687: (0, 688), 688: (0, 689), 689: (0, 690), 690: (0, 691), 691: (0, 692), 692: (0, 693), 693: (0, 694), 694: (0, 695), 695: (0, 696), 696: (0, 697), 697: (0, 698), 698: (0, 699), 699: (0, 700), 700: (0, 701), 701: (0, 702), 702: (0, 703), 703: (0, 704), 704: (0, 705), 705: (0, 706), 706: (0, 707), 707: (0, 708), 708: (0, 709), 709: (0, 710), 710: (0, 711), 711: (0, 712), 712: (0, 713), 713: (0, 714), 714: (0, 715), 715: (0, 716), 716: (0, 717), 717: (0, 718), 718: (0, 719), 719: (0, 720), 720: (0, 721), 721: (0, 722), 722: (0, 723), 723: (0, 724), 724: (0, 725), 725: (0, 726), 726: (0, 727), 727: (0, 728), 728: (0, 729), 729: (0, 730), 730: (0, 731), 731: (0, 732), 732: (0, 733), 733: (0, 734), 734: (0, 735), 735: (0, 736), 736: (0, 737), 737: (0, 738), 738: (0, 739), 739: (0, 740), 740: (0, 741), 741: (0, 742), 742: (0, 743), 743: (0, 744), 744: (0, 745), 745: (0, 746), 746: (0, 747), 747: (0, 748), 748: (0, 749), 749: (0, 750), 750: (0, 751), 751: (0, 752), 752: (0, 753), 753: (0, 754), 754: (0, 755), 755: (0, 756), 756: (0, 757), 757: (0, 758), 758: (0, 759), 759: (0, 760), 760: (0, 761), 761: (0, 762), 762: (0, 763), 763: (0, 764), 764: (0, 765), 765: (0, 766), 766: (0, 767), 767: (0, 768), 768: (0, 769), 769: (0, 770), 770: (0, 771), 771: (0, 772), 772: (0, 773), 773: (0, 774), 774: (0, 775), 775: (0, 776), 776: (0, 777), 777: (0, 778), 778: (0, 779), 779: (0, 780)}
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-221014-sess_0/Trials.pkl
This many vals across loaded session
0 : 2640421
Assigning to SP.Params this item:
{'which_level': 'trial', '_list_events': ['fixcue', 'fix_touch', 'rulecue2', 'samp', 'go_cue', 'first_raise', 'on_strokeidx_0', 'off_stroke_last', 'doneb', 'post', 'reward_all'], 'list_events_uniqnames': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_features_extraction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'char_seq', 'epochset'], 'list_features_get_conjunction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'char_seq', 'epochset'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65], 'map_var_to_othervars': None, 'strokes_only_keep_single': False, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'map_var_to_levels': None}
Assigning to SP.ParamsGlobals this item:
{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.65, 'POST_DUR_CALC': 0.65, 'list_events': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65]}
stored in self.Dat[BehClass]
0
200
400
600
Running D.behclass_tokens_extract_datsegs
0
200
400
600
TODO!!! Merge this with other learning-related code
stored in self.Dat[BehClass]
0
200
400
600
Running D.behclass_tokens_extract_datsegs
0
200
400
600
trial # 0
trial # 100
trial # 200
trial # 300
trial # 400
trial # 500
trial # 600
trial # 700
Generated column called 'agent', which connects agent_kind-rule
n samples for conjunctions of score_name, agent_rule, agent_kind:
('binsucc', 'AnBmTR|0', 'model') :     205
('binsucc', 'AnBmTR|1', 'model') :     69
('binsucc', 'TR|0', 'model') :     197
('binsucc', 'TR|1', 'model') :     65
('binsucc', 'rndstr', 'model') :     223
TODO! _preprocess_sanity_check
Resulting taskgroup/probe combo, after taskgroup_reassign_simple_neural...
('I', 0) :     663
('I', 1) :     96
.. Appended new column 'char_seq', version: task_matlab
Done!, new len of dataset 759
stored in self.Dat[BehClass]
0
200
400
600
Running D.behclass_tokens_extract_datsegs
0
200
400
600
Appended columns gridsize!
Mergin these epoch's .. 
['rndstr', 'AnBmTR|1', 'TR|1']
Into this new epoch: rank|1
Defined new column: epochset
... value_counts:
(AnBmTR|0, rank|1)          270
(TR|0, rank|1)              253
(rank|1,)                   213
(AnBmTR|0, TR|0, rank|1)     23
Name: epochset, dtype: int64
... merge_sets_with_only_single_epoch... 
('rank|1',) only has one epoch!:  ['rank|1']
Mergin these epochset's .. 
[('rank|1',)]
Into this new epochset: ('LEFTOVER',)
Final epochsets:
(AnBmTR|0, rank|1)          270
(TR|0, rank|1)              253
(LEFTOVER,)                 213
(AnBmTR|0, TR|0, rank|1)     23
Name: epochset, dtype: int64
Updating this column of SP.DfScalar with Dataset beh:
epoch
Updating this column of SP.DfScalar with Dataset beh:
epochset
Updating this column of SP.DfScalar with Dataset beh:
seqc_0_loc
Updating this column of SP.DfScalar with Dataset beh:
seqc_0_shape
Updating this column of SP.DfScalar with Dataset beh:
seqc_nstrokes_beh
Updating this column of SP.DfScalar with Dataset beh:
character
Updating this column of SP.DfScalar with Dataset beh:
seqc_0_loc_shape
Updating this column of SP.DfScalar with Dataset beh:
seqc_1_loc_shape
Starting length of D.Dat: 759
self.Dat modified!!
Len, after remove aborts: 464
############ TAKING ONLY NO SUPERVISION TRIALS
--BEFORE REMOVE; existing supervision_stage_concise:
off|1|rank|0      216
off|1|solid|0     202
mask|1|solid|0     31
mask|1|rank|0      15
Name: supervision_stage_concise, dtype: int64
self.Dat modified!!
--AFTER REMOVE; existing supervision_stage_concise:
off|1|rank|0     216
off|1|solid|0    202
Name: supervision_stage_concise, dtype: int64
Dataset final len: 418
-- Len of D, before applying this param: remove_repeated_trials, ... 418
appended col to self.Dat:
dummy
self.Dat starting legnth:  416
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  416
after: 416
-- Len of D, before applying this param: correct_sequencing_binary_score, ... 416
self.Dat starting legnth:  387
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  387
after: 387
-- Len of D, before applying this param: one_to_one_beh_task_strokes, ... 387
after: 387
-- Len of D, before applying this param: beh_strokes_at_least_one, ... 387
after: 387
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff
starting sites:  378
starting sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 86, 90, 91, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 118, 119, 120, 121, 122, 123, 125, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 173, 174, 175, 177, 178, 179, 180, 181, 182, 184, 186, 188, 189, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 236, 237, 238, 240, 241, 243, 244, 245, 248, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 270, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 294, 295, 296, 299, 300, 302, 303, 304, 306, 309, 310, 311, 312, 316, 317, 318, 319, 320, 323, 326, 327, 328, 329, 330, 331, 332, 333, 339, 341, 343, 344, 345, 346, 347, 348, 349, 351, 352, 355, 356, 358, 360, 361, 362, 368, 370, 371, 373, 374, 375, 376, 379, 381, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509, 510]
For percentile 10, using this threshold: 3.0896500159178713
sites_good:  340
sites_bad:  38
Updates self.Sites
ending sites:  340
ending sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables
var -- vars_others:  epoch  ---  ['epochset']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epoch-vs-varothers-levothers-levvar.txt
var -- vars_others:  epochset  ---  ['epoch']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epochset-vs-varothers-levothers-levvar.txt
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['00_fixcue_-600_to_-50', '00_fixcue_260_to_600', '00_fixcue_40_to_240', '00_fixcue_50_to_600', '03_samp_-600_to_-40', '03_samp_260_to_600', '03_samp_40_to_240', '03_samp_50_to_600', '04_go_cue_-600_to_-40', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
Running grouping_print_n_samples...
DOing these! ...
list_events ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['00_fixcue_-600_to_-50', '00_fixcue_260_to_600', '00_fixcue_40_to_240', '00_fixcue_50_to_600', '03_samp_-600_to_-40', '03_samp_260_to_600', '03_samp_40_to_240', '03_samp_50_to_600', '04_go_cue_-600_to_-40', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  3 98
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_-600_to_-50
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_40_to_240
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_40_to_240
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_260_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_260_to_600
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-40
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_40_to_240
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_40_to_240
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_260_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_260_to_600
 
Updated ParamsGlobals for event 04_go_cue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  04_go_cue_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
04_go_cue_-600_to_-40
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.25
POST_DUR_CALC  =  0.35
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-250_to_350
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-250_to_350
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['epochset']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff
starting sites:  340
starting sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
For percentile 10, using this threshold: 3.0896500159178713
sites_good:  340
sites_bad:  38
Updates self.Sites
ending sites:  340
ending sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables
var -- vars_others:  epoch  ---  ['seqc_0_loc']
var -- vars_others:  epoch  ---  ['seqc_0_shape']
var -- vars_others:  epoch  ---  ['seqc_nstrokes_beh']
var -- vars_others:  epoch  ---  ['seqc_0_loc', 'seqc_0_shape']
var -- vars_others:  epoch  ---  ['seqc_0_loc', 'seqc_nstrokes_beh']
var -- vars_others:  epoch  ---  ['seqc_0_shape', 'seqc_nstrokes_beh']
var -- vars_others:  epoch  ---  ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epoch-vs-varothers-levothers-levvar.txt
var -- vars_others:  seqc_0_loc  ---  ['epoch']
var -- vars_others:  seqc_0_loc  ---  ['seqc_0_shape']
var -- vars_others:  seqc_0_loc  ---  ['seqc_nstrokes_beh']
var -- vars_others:  seqc_0_loc  ---  ['epoch', 'seqc_0_shape']
var -- vars_others:  seqc_0_loc  ---  ['epoch', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_loc  ---  ['seqc_0_shape', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_loc  ---  ['epoch', 'seqc_0_shape', 'seqc_nstrokes_beh']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_0_loc-vs-varothers-levothers-levvar.txt
var -- vars_others:  seqc_0_shape  ---  ['epoch']
var -- vars_others:  seqc_0_shape  ---  ['seqc_0_loc']
var -- vars_others:  seqc_0_shape  ---  ['seqc_nstrokes_beh']
var -- vars_others:  seqc_0_shape  ---  ['epoch', 'seqc_0_loc']
var -- vars_others:  seqc_0_shape  ---  ['epoch', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_shape  ---  ['seqc_0_loc', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_shape  ---  ['epoch', 'seqc_0_loc', 'seqc_nstrokes_beh']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_0_shape-vs-varothers-levothers-levvar.txt
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch']
var -- vars_others:  seqc_nstrokes_beh  ---  ['seqc_0_loc']
var -- vars_others:  seqc_nstrokes_beh  ---  ['seqc_0_shape']
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch', 'seqc_0_loc']
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch', 'seqc_0_shape']
var -- vars_others:  seqc_nstrokes_beh  ---  ['seqc_0_loc', 'seqc_0_shape']
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch', 'seqc_0_loc', 'seqc_0_shape']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_nstrokes_beh-vs-varothers-levothers-levvar.txt
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['00_fixcue_-600_to_-50', '00_fixcue_260_to_600', '00_fixcue_40_to_240', '00_fixcue_50_to_600', '03_samp_-600_to_-40', '03_samp_260_to_600', '03_samp_40_to_240', '03_samp_50_to_600', '04_go_cue_-600_to_-40', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
Running grouping_print_n_samples...
DOing these! ...
list_events ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['00_fixcue_-600_to_-50', '00_fixcue_260_to_600', '00_fixcue_40_to_240', '00_fixcue_50_to_600', '03_samp_-600_to_-40', '03_samp_260_to_600', '03_samp_40_to_240', '03_samp_50_to_600', '04_go_cue_-600_to_-40', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  1 38
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_-600_to_-50
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_40_to_240
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_40_to_240
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_260_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_260_to_600
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-40
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_40_to_240
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_40_to_240
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_260_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_260_to_600
 
Updated ParamsGlobals for event 04_go_cue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  04_go_cue_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
04_go_cue_-600_to_-40
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.25
POST_DUR_CALC  =  0.35
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-250_to_350
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-250_to_350
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff
starting sites:  340
starting sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
For percentile 10, using this threshold: 3.0896500159178713
sites_good:  340
sites_bad:  38
Updates self.Sites
ending sites:  340
ending sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables
var -- vars_others:  character  ---  ['epoch']
var -- vars_others:  character  ---  ['epochset']
var -- vars_others:  character  ---  ['epoch', 'epochset']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-character-vs-varothers-levothers-levvar.txt
var -- vars_others:  epoch  ---  ['character']
var -- vars_others:  epoch  ---  ['epochset']
var -- vars_others:  epoch  ---  ['character', 'epochset']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epoch-vs-varothers-levothers-levvar.txt
var -- vars_others:  epochset  ---  ['character']
var -- vars_others:  epochset  ---  ['epoch']
var -- vars_others:  epochset  ---  ['character', 'epoch']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epochset-vs-varothers-levothers-levvar.txt
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_character-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['00_fixcue_-600_to_-50', '00_fixcue_260_to_600', '00_fixcue_40_to_240', '03_samp_-600_to_-40', '03_samp_260_to_600', '03_samp_40_to_240', '04_go_cue_-600_to_-40', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
Running grouping_print_n_samples...
DOing these! ...
list_events ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['00_fixcue_-600_to_-50', '00_fixcue_260_to_600', '00_fixcue_40_to_240', '03_samp_-600_to_-40', '03_samp_260_to_600', '03_samp_40_to_240', '04_go_cue_-600_to_-40', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-250_to_350', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  1 14
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_-600_to_-50
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_40_to_240
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_40_to_240
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_260_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
00_fixcue_260_to_600
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-40
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_40_to_240
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_40_to_240
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_260_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_260_to_600
 
Updated ParamsGlobals for event 04_go_cue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  04_go_cue_-600_to_-40
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
04_go_cue_-600_to_-40
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.25
POST_DUR_CALC  =  0.35
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-250_to_350
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-250_to_350
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  character ['epoch', 'epochset']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff
starting sites:  340
starting sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
For percentile 10, using this threshold: 3.0896500159178713
sites_good:  340
sites_bad:  38
Updates self.Sites
ending sites:  340
ending sites:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 74, 75, 76, 77, 78, 79, 80, 92, 94, 96, 97, 98, 101, 102, 108, 111, 112, 114, 115, 116, 119, 120, 121, 122, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 169, 170, 173, 174, 177, 178, 179, 180, 181, 184, 186, 188, 189, 191, 192, 193, 194, 195, 197, 199, 200, 202, 203, 205, 207, 210, 211, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 229, 233, 235, 237, 240, 241, 243, 244, 245, 248, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 267, 269, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 284, 285, 286, 288, 290, 294, 296, 299, 300, 302, 303, 304, 310, 312, 316, 317, 318, 320, 326, 327, 328, 329, 331, 332, 333, 343, 344, 345, 346, 347, 349, 351, 352, 355, 356, 358, 360, 362, 368, 370, 371, 374, 375, 376, 379, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 425, 429, 430, 433, 434, 435, 436, 437, 439, 440, 441, 443, 445, 447, 449, 450, 451, 452, 453, 454, 455, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 509]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables
var -- vars_others:  seqc_0_loc_shape  ---  ['epoch']
var -- vars_others:  seqc_0_loc_shape  ---  ['epochset']
var -- vars_others:  seqc_0_loc_shape  ---  ['epoch', 'epochset']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_0_loc_shape-vs-varothers-levothers-levvar.txt
var -- vars_others:  epoch  ---  ['seqc_0_loc_shape']
var -- vars_others:  epoch  ---  ['epochset']
var -- vars_others:  epoch  ---  ['seqc_0_loc_shape', 'epochset']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epoch-vs-varothers-levothers-levvar.txt
var -- vars_others:  epochset  ---  ['seqc_0_loc_shape']
var -- vars_others:  epochset  ---  ['epoch']
var -- vars_others:  epochset  ---  ['seqc_0_loc_shape', 'epoch']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epochset-vs-varothers-levothers-levvar.txt
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
df_var doesnt exist...!
COMPUTING df_var!!!
Running grouping_print_n_samples...
DOing these! ...
list_events ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
[]
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  1 38
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_-600_to_-50
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_40_to_240
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 00_fixcue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  00_fixcue_260_to_600
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-40
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.04
POST_DUR_CALC  =  0.24
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_40_to_240
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.26
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_260_to_600
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 04_go_cue to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.04
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  04_go_cue_-600_to_-40
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.25
POST_DUR_CALC  =  0.35
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-250_to_350
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  7
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['00_fixcue', '00_fixcue', '00_fixcue', '03_samp', '03_samp', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.04, 0.26, -0.6, 0.04, 0.26, -0.6, -0.6, -0.25, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.24, 0.6, -0.04, 0.24, 0.6, -0.04, -0.05, 0.35, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
site : 60
site : 80
site : 120
site : 140
site : 160
site : 180
site : 200
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 440
site : 460
site : 480
site : 500
SAving:  /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
SAving:  /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/list_eventwindow_event.pkl
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/modulation
** Plotting summarystats
Saving at: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-221014-0/rulesw/var_by_varsother/VAR_seqc_0_loc_shape-OV_epoch_epochset/SV_r2_maxtime_1way_mshuff/modulation
./_analy_anova_script.sh: line 4: 17211 Killed                  python analy_anova_plot.py $@ n
