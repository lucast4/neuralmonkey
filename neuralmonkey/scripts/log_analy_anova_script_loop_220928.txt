Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
session:  0
1
Beh Sessions that exist on this date:  {220928: [(1, 'grammardircolor2')]}
taking this beh session: 1
Loading these beh expts: ['grammardircolor2']
Loading these beh sessions: [1]
Loading this neural session: 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
{'filename_components_hyphened': ['Pancho', '220928', '153315'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/220928'], 'basedirs_filenames': ['220928', 'Pancho-220928-153315'], 'filename_final_ext': 'Pancho-220928-153315', 'filename_final_noext': 'Pancho-220928-153315'}
FOund this path for spikes:  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/Pancho-220928-153315
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
final_dir_name  --  Pancho-220928-153315
time  --  153315
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/220928.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
-  1 64.70642265762471
o  2 92.75227027557685
o  3 92.98346476404103
-  4 69.26300597781224
o  5 87.25131996000691
o  6 89.47348011043788
o  7 123.48862681173287
o  8 82.36742131823412
o  9 210.58342301743832
o  10 97.8338766882688
o  11 139.7793052085557
o  12 78.09594426531484
o  13 226.68211889801114
o  14 125.96902141507182
o  15 177.50962307076344
o  16 169.0339794851474
o  17 91.40468793030692
o  18 114.7112663919659
o  19 447.84476403279695
-  20 49.657786596331974
o  21 111.04103222186241
o  22 80.79252047164519
-  23 53.960284735499044
o  24 76.87724899829735
o  25 87.94239639971866
o  26 416.6047760693619
o  27 175.16858508911247
o  28 78.79534329460148
o  29 209.9478275293116
o  30 88.47719042327736
o  31 176.89209718514522
o  32 95.2496299257903
o  33 87.4359746693015
o  34 114.01539566523861
-  35 38.93969556403031
o  36 373.2891541595018
o  37 99.44842975203701
o  38 252.53823987135115
o  39 94.81235612025557
o  40 270.29760878907985
o  41 102.97177115304093
o  42 240.148420927699
o  43 143.49302710044867
o  44 111.54485238637135
o  45 753.6843257890603
-  46 54.73407165935636
o  47 293.75944046092775
-  48 47.234390653264924
o  49 115.02418574890638
-  50 53.828532283988714
o  51 119.71501750642446
o  52 107.76414954235045
o  53 204.65652060189296
o  54 86.29427946202486
o  55 946.7906213292976
o  56 78.05517264386555
o  57 150.34317616250829
o  58 110.98779724632763
o  59 217.7508589785519
o  60 79.65383838411645
o  61 163.05304001054537
o  62 148.27777735896674
o  63 221.9230968334003
o  64 72.19433454630017
o  65 83.68833072772688
o  66 84.26878408671874
o  67 86.67612308622593
o  68 179.71822423141347
o  69 79.18578737111966
-  70 63.76291332805913
o  71 80.82257871768017
o  72 98.02535221837316
o  73 82.0325838508761
o  74 153.31031394906017
o  75 71.76126008841857
o  76 81.3908591756538
o  77 84.9097253104328
o  78 118.48957762751516
-  79 60.581868340287016
o  80 106.58424551947446
-  81 68.56237777558249
-  82 61.71513968765576
-  83 68.51139770521044
o  84 87.87795961366429
o  85 72.70916986784341
o  86 89.30877284717246
-  87 53.10114065347064
o  88 90.88787800284081
-  89 59.558932880231126
-  90 65.11061085344572
-  91 68.6133183438779
-  92 53.04963410161747
-  93 64.20662420022272
o  94 75.01366663086841
-  95 60.78659094111654
o  96 73.4387118520339
o  97 178.2916104760665
o  98 102.90327328281333
o  99 93.5625205554068
o  100 173.08397384706478
o  101 72.84249597570461
o  102 88.21222013753118
o  103 70.11250577140594
-  104 61.2734181474696
o  105 139.87347959570872
-  106 69.81735512964569
-  107 68.2651792479971
-  108 68.15289861893983
-  109 53.368861382725775
-  110 65.87789363329594
-  111 69.92766914309591
o  112 75.88835379688399
o  113 77.6405479099722
o  114 75.19167905313043
o  115 71.40504654565294
o  116 168.65421747240146
-  117 53.00111040470851
o  118 103.27954960464932
o  119 92.68173800763262
-  120 59.6586011134239
o  121 179.19744066805032
o  122 131.9150475032378
o  123 97.06718544852906
o  124 109.67907436106853
-  125 47.437565477022034
-  126 68.83772244578503
-  127 65.388841772671
-  128 50.46087513981149
-  129 46.832394374234354
o  130 145.0988710021157
o  131 79.03208207144435
o  132 233.64404025717513
o  133 200.91175213695985
o  134 302.4420934936441
o  135 88.02134319124198
o  136 118.24251260979983
-  137 65.6893409090676
o  138 312.0881642975485
o  139 92.95397947517722
o  140 116.82239945014548
o  141 91.03375999612653
o  142 147.8633350129349
o  143 107.28840027904667
o  144 300.95771258459354
-  145 57.21322932065123
o  146 98.16200614444388
-  147 48.78191968945475
o  148 115.54929172894919
o  149 78.12888639250232
o  150 110.99772739085198
-  151 50.381534237951925
o  152 110.70270701063637
o  153 97.76821191569351
o  154 92.94143474201482
-  155 58.229312690939906
o  156 132.89394639189834
o  157 130.0816241324179
o  158 79.53065555166987
o  159 79.62019363936379
o  160 142.98818278377686
o  161 160.5381275015413
o  162 80.99151552634922
o  163 93.6586569374334
o  164 81.12202931078691
o  165 178.1333505722069
o  166 77.15672621449986
o  167 104.2846370700293
o  168 77.59290303804482
o  169 78.33415503435735
o  170 95.26062924094659
o  171 160.58715233833027
-  172 59.11464212475558
o  173 302.92405111386125
o  174 85.65809316262767
o  175 77.62037397004508
o  176 74.98871117545787
o  177 222.86958611468526
o  178 81.5341366042363
o  179 435.1815414617571
o  180 70.74635909027002
o  181 142.65732002822978
o  182 72.90446142409318
-  183 64.14923213132916
o  184 110.3527281857254
-  185 30.054779596584666
o  186 115.34744849825354
o  187 128.84215224602863
o  188 84.42468385949833
o  189 90.90663451776442
o  190 74.16401356135052
o  191 95.53207402413663
o  192 103.12068173596309
o  193 131.66434499793354
o  194 88.04311039365767
o  195 207.20225650798577
-  196 68.07234328833087
o  197 133.8152733526463
-  198 53.93579706543043
o  199 78.46291114229044
o  200 81.97217108677278
o  201 85.5512937608602
o  202 108.1723629331069
o  203 123.81149655029022
o  204 80.32044559427996
o  205 86.82384255454015
o  206 76.41619977366372
-  207 67.7237184770418
-  208 61.167078090212
-  209 63.195706309375964
o  210 123.45549142202371
o  211 96.4718819914995
o  212 71.77623824869507
o  213 89.10523274097687
o  214 100.44040867968488
o  215 118.17732941353353
o  216 84.06214390969936
-  217 61.6567975086311
o  218 105.74604921252585
o  219 106.81335786801999
o  220 78.49109469999094
-  221 61.10210839568153
-  222 52.2313693808169
o  223 74.03296417067268
o  224 103.35655927900935
o  225 84.04796706945899
o  226 89.85248125551024
o  227 145.6834260790172
-  228 52.60554684262661
o  229 84.49694747881098
-  230 49.13364944595353
o  231 79.49768116558334
-  232 52.1185604773625
o  233 143.6608554295489
o  234 72.29740782746272
o  235 104.67656997133618
-  236 63.131440890396696
o  237 101.25035094158693
-  238 62.96179658008363
-  239 51.19097132478637
o  240 77.76394418246545
o  241 72.54932549773075
o  242 71.04550620627201
o  243 73.83732680824637
o  244 83.97921068303098
o  245 84.06324189110283
o  246 104.0870561599911
o  247 84.88588913675159
o  248 94.75390219915752
o  249 75.88419450221154
-  250 50.00922093612369
o  251 70.4389281440373
o  252 97.95898013640162
o  253 175.02433829051344
o  254 82.02839564840997
o  255 139.20723589743466
o  256 100.70542607460229
o  257 311.4895677480847
o  258 125.26118683127515
o  259 125.44329314069486
o  260 141.52202859264935
o  261 140.04112594798355
o  262 89.41074813444767
o  263 79.4152901102976
o  264 130.63124346386786
o  265 154.62044123637972
o  266 75.01319868708734
o  267 119.9071072430417
o  268 70.81739241994656
o  269 95.16384362498248
o  270 70.11469017291584
-  271 60.86349757705539
o  272 95.99388981484044
o  273 185.45435283880352
o  274 103.90069910442203
o  275 77.42014144920044
o  276 84.7522450480717
o  277 146.95640444944547
o  278 146.73066520986276
o  279 103.81185689916578
o  280 74.44124346517596
o  281 115.64619752235387
o  282 145.49198129060252
o  283 98.96043097292987
o  284 97.91206939714499
o  285 96.60898017004276
-  286 65.21801829764755
o  287 71.44784723878472
o  288 86.91016916832102
o  289 80.92527339887039
o  290 78.16593568825593
-  291 66.81128890901255
-  292 42.021122935314814
o  293 72.69283650314338
o  294 100.7429762855119
-  295 68.9599840159066
o  296 82.58989647358224
o  297 162.63214559160787
o  298 83.46919990479064
-  299 53.931600851382065
o  300 212.9820822071823
-  301 60.7696962621988
o  302 104.14842459371495
-  303 68.31988248436906
o  304 74.99424404757181
-  305 49.328322429683524
o  306 89.05300794706673
o  307 77.95189849150682
-  308 68.79345239426914
-  309 68.82744779396383
o  310 72.42636055915663
-  311 61.15283642861304
o  312 88.31401101281777
o  313 73.40385212437816
o  314 95.30700365459904
-  315 59.63662528061134
o  316 160.70910337867153
o  317 129.00249007137435
o  318 99.05040648753656
o  319 80.94998998165029
o  320 84.53144323323076
-  321 68.52941626990268
-  322 56.80429384152796
-  323 64.53067376773393
o  324 72.7230508539441
o  325 162.31152425371403
-  326 67.38508596713245
o  327 139.8114766794385
-  328 58.81366645852777
o  329 86.17924160674269
o  330 84.57640936131183
o  331 82.32530393648412
o  332 117.31530657254916
o  333 97.06515973811547
o  334 85.06081646666426
-  335 47.740035040905376
-  336 61.22718492151289
-  337 55.059852061753375
-  338 60.10670033574652
o  339 72.90246378654535
-  340 49.29952697481015
-  341 51.264257180396086
-  342 69.15010553736525
o  343 86.27320256185703
o  344 86.09559350369037
o  345 89.52798671391055
o  346 91.43236513752126
o  347 96.2235271886138
o  348 80.3995195148309
o  349 88.92431120757816
o  350 70.1305227371455
o  351 101.25020210907624
o  352 94.81420776267296
o  353 73.3077169088665
-  354 64.56092992089049
o  355 104.84158433620031
o  356 76.88982681286907
-  357 42.059168536984366
-  358 67.21424605802122
-  359 53.27329731297578
o  360 78.59124881883709
-  361 68.38212114476704
o  362 86.09002540352203
-  363 62.66461397735666
-  364 69.3786456251432
-  365 39.510085638179774
o  366 79.14926672357998
o  367 70.95394721022623
o  368 87.83812399707624
o  369 95.4014966869903
o  370 148.59646618502848
o  371 96.7957346063332
o  372 86.16201577246594
o  373 132.27116515490746
o  374 73.5582551907637
o  375 97.98031357040252
o  376 70.50606271509864
-  377 61.28323193725798
-  378 60.79151784496266
-  379 60.65144593001728
o  380 81.58152702209145
o  381 93.28561782721555
o  382 75.9573509684567
-  383 59.53836147475738
o  384 98.72555376712958
o  385 298.25407381232577
o  386 195.85277560320986
o  387 169.8466751864667
o  388 109.35690294607112
o  389 318.8819452367461
o  390 184.17704042324576
-  391 54.071809312904406
-  392 67.79237236608523
o  393 133.46610360307375
o  394 180.9430258570598
o  395 156.31163397205037
o  396 160.7104916970542
o  397 90.58061222409324
o  398 78.85048135376542
o  399 110.55448189529052
o  400 108.68443196837397
o  401 112.50275932084496
o  402 113.58006965469686
o  403 117.46186524538552
o  404 124.48780743830594
o  405 149.8258706122404
o  406 94.40066236343631
o  407 147.11958322327828
o  408 117.9327223116814
-  409 54.89740634073248
o  410 213.27796530647393
o  411 81.45343183504444
o  412 84.21224833948109
o  413 136.39433683930775
o  414 190.77437504288773
o  415 120.68962235314687
o  416 111.66471520307289
o  417 146.46454573463296
o  418 75.00036501222894
o  419 75.21696061882358
o  420 83.80826668760801
o  421 117.69559041063621
o  422 99.7360207839538
o  423 78.61780415262005
-  424 68.94655506686577
o  425 88.5436290596833
-  426 64.36682000131573
o  427 71.55469818098942
-  428 28.212085071196906
o  429 124.52132755480457
o  430 101.84957953076606
-  431 66.93976228466762
-  432 52.881732085999694
o  433 106.69577588857031
o  434 130.9298455623093
o  435 170.47612519585837
o  436 131.71840621698328
o  437 87.10855526089972
o  438 134.20232062740448
o  439 104.92413322005442
o  440 92.78015694672368
o  441 175.3746003476125
o  442 100.02609128745637
o  443 501.23521828413976
-  444 36.66405991903727
o  445 100.11728952147527
-  446 37.384899395250976
o  447 101.88020440632044
-  448 36.11403769817143
o  449 161.9757741697594
o  450 257.2029351650684
o  451 223.5455472440911
o  452 119.69384430833051
-  453 65.87787093154057
o  454 134.8030442752788
o  455 115.43287530231825
o  456 84.51470802311248
o  457 128.20436267933326
o  458 81.13602649274145
o  459 188.23608437091002
o  460 94.29288628401774
o  461 169.25470616936406
o  462 119.53139059220076
o  463 120.1045524408183
o  464 187.12270665869437
o  465 142.28063809309424
-  466 50.97633914132039
o  467 124.22206399475158
o  468 116.0529452487879
o  469 224.2817015135907
o  470 98.43403320501658
o  471 142.87604487778137
o  472 159.36372880886674
o  473 126.20476400252088
o  474 74.17119399477743
o  475 93.56361953548931
o  476 683.2504270876818
o  477 180.361723481803
o  478 74.73789642959423
o  479 73.44913066517456
o  480 83.28709419973843
o  481 87.9012291600419
-  482 36.044248681556205
o  483 95.2462259511074
o  484 96.22346932365748
o  485 222.22970279179856
o  486 138.14737097727206
o  487 80.1440922468787
o  488 149.33039029641947
o  489 156.28607435278164
o  490 113.19155907205192
o  491 87.32750147752964
o  492 162.5678783866475
o  493 125.72227014823748
o  494 228.89403629116424
o  495 81.94993868965503
o  496 131.40108817032865
o  497 140.29399833691775
o  498 103.73141085869013
o  499 106.03680761790228
o  500 160.27223346145303
o  501 142.0537028637121
o  502 315.65862303479423
o  503 90.40350064454215
o  504 75.87051421969383
o  505 92.9254588719639
o  506 116.17387874323049
-  507 34.59806584451653
o  508 85.41960496870516
o  509 107.19129142777507
o  510 73.29381670423318
-  511 40.833964239817014
o  512 113.72788482192084
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Trials.pkl
** SKIPPING EXTRACTION, since was able to load snippets, for: 
(animal, DATE, which_level, ANALY_VER, session)
Pancho 220928 trial rulesw 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
session:  0
1
Beh Sessions that exist on this date:  {220928: [(1, 'grammardircolor2')]}
taking this beh session: 1
Loading these beh expts: ['grammardircolor2']
Loading these beh sessions: [1]
Loading this neural session: 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
{'filename_components_hyphened': ['Pancho', '220928', '153315'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/220928'], 'basedirs_filenames': ['220928', 'Pancho-220928-153315'], 'filename_final_ext': 'Pancho-220928-153315', 'filename_final_noext': 'Pancho-220928-153315'}
FOund this path for spikes:  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/Pancho-220928-153315
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
final_dir_name  --  Pancho-220928-153315
time  --  153315
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/220928.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
-  1 64.70642265762471
o  2 92.75227027557685
o  3 92.98346476404103
-  4 69.26300597781224
o  5 87.25131996000691
o  6 89.47348011043788
o  7 123.48862681173287
o  8 82.36742131823412
o  9 210.58342301743832
o  10 97.8338766882688
o  11 139.7793052085557
o  12 78.09594426531484
o  13 226.68211889801114
o  14 125.96902141507182
o  15 177.50962307076344
o  16 169.0339794851474
o  17 91.40468793030692
o  18 114.7112663919659
o  19 447.84476403279695
-  20 49.657786596331974
o  21 111.04103222186241
o  22 80.79252047164519
-  23 53.960284735499044
o  24 76.87724899829735
o  25 87.94239639971866
o  26 416.6047760693619
o  27 175.16858508911247
o  28 78.79534329460148
o  29 209.9478275293116
o  30 88.47719042327736
o  31 176.89209718514522
o  32 95.2496299257903
o  33 87.4359746693015
o  34 114.01539566523861
-  35 38.93969556403031
o  36 373.2891541595018
o  37 99.44842975203701
o  38 252.53823987135115
o  39 94.81235612025557
o  40 270.29760878907985
o  41 102.97177115304093
o  42 240.148420927699
o  43 143.49302710044867
o  44 111.54485238637135
o  45 753.6843257890603
-  46 54.73407165935636
o  47 293.75944046092775
-  48 47.234390653264924
o  49 115.02418574890638
-  50 53.828532283988714
o  51 119.71501750642446
o  52 107.76414954235045
o  53 204.65652060189296
o  54 86.29427946202486
o  55 946.7906213292976
o  56 78.05517264386555
o  57 150.34317616250829
o  58 110.98779724632763
o  59 217.7508589785519
o  60 79.65383838411645
o  61 163.05304001054537
o  62 148.27777735896674
o  63 221.9230968334003
o  64 72.19433454630017
o  65 83.68833072772688
o  66 84.26878408671874
o  67 86.67612308622593
o  68 179.71822423141347
o  69 79.18578737111966
-  70 63.76291332805913
o  71 80.82257871768017
o  72 98.02535221837316
o  73 82.0325838508761
o  74 153.31031394906017
o  75 71.76126008841857
o  76 81.3908591756538
o  77 84.9097253104328
o  78 118.48957762751516
-  79 60.581868340287016
o  80 106.58424551947446
-  81 68.56237777558249
-  82 61.71513968765576
-  83 68.51139770521044
o  84 87.87795961366429
o  85 72.70916986784341
o  86 89.30877284717246
-  87 53.10114065347064
o  88 90.88787800284081
-  89 59.558932880231126
-  90 65.11061085344572
-  91 68.6133183438779
-  92 53.04963410161747
-  93 64.20662420022272
o  94 75.01366663086841
-  95 60.78659094111654
o  96 73.4387118520339
o  97 178.2916104760665
o  98 102.90327328281333
o  99 93.5625205554068
o  100 173.08397384706478
o  101 72.84249597570461
o  102 88.21222013753118
o  103 70.11250577140594
-  104 61.2734181474696
o  105 139.87347959570872
-  106 69.81735512964569
-  107 68.2651792479971
-  108 68.15289861893983
-  109 53.368861382725775
-  110 65.87789363329594
-  111 69.92766914309591
o  112 75.88835379688399
o  113 77.6405479099722
o  114 75.19167905313043
o  115 71.40504654565294
o  116 168.65421747240146
-  117 53.00111040470851
o  118 103.27954960464932
o  119 92.68173800763262
-  120 59.6586011134239
o  121 179.19744066805032
o  122 131.9150475032378
o  123 97.06718544852906
o  124 109.67907436106853
-  125 47.437565477022034
-  126 68.83772244578503
-  127 65.388841772671
-  128 50.46087513981149
-  129 46.832394374234354
o  130 145.0988710021157
o  131 79.03208207144435
o  132 233.64404025717513
o  133 200.91175213695985
o  134 302.4420934936441
o  135 88.02134319124198
o  136 118.24251260979983
-  137 65.6893409090676
o  138 312.0881642975485
o  139 92.95397947517722
o  140 116.82239945014548
o  141 91.03375999612653
o  142 147.8633350129349
o  143 107.28840027904667
o  144 300.95771258459354
-  145 57.21322932065123
o  146 98.16200614444388
-  147 48.78191968945475
o  148 115.54929172894919
o  149 78.12888639250232
o  150 110.99772739085198
-  151 50.381534237951925
o  152 110.70270701063637
o  153 97.76821191569351
o  154 92.94143474201482
-  155 58.229312690939906
o  156 132.89394639189834
o  157 130.0816241324179
o  158 79.53065555166987
o  159 79.62019363936379
o  160 142.98818278377686
o  161 160.5381275015413
o  162 80.99151552634922
o  163 93.6586569374334
o  164 81.12202931078691
o  165 178.1333505722069
o  166 77.15672621449986
o  167 104.2846370700293
o  168 77.59290303804482
o  169 78.33415503435735
o  170 95.26062924094659
o  171 160.58715233833027
-  172 59.11464212475558
o  173 302.92405111386125
o  174 85.65809316262767
o  175 77.62037397004508
o  176 74.98871117545787
o  177 222.86958611468526
o  178 81.5341366042363
o  179 435.1815414617571
o  180 70.74635909027002
o  181 142.65732002822978
o  182 72.90446142409318
-  183 64.14923213132916
o  184 110.3527281857254
-  185 30.054779596584666
o  186 115.34744849825354
o  187 128.84215224602863
o  188 84.42468385949833
o  189 90.90663451776442
o  190 74.16401356135052
o  191 95.53207402413663
o  192 103.12068173596309
o  193 131.66434499793354
o  194 88.04311039365767
o  195 207.20225650798577
-  196 68.07234328833087
o  197 133.8152733526463
-  198 53.93579706543043
o  199 78.46291114229044
o  200 81.97217108677278
o  201 85.5512937608602
o  202 108.1723629331069
o  203 123.81149655029022
o  204 80.32044559427996
o  205 86.82384255454015
o  206 76.41619977366372
-  207 67.7237184770418
-  208 61.167078090212
-  209 63.195706309375964
o  210 123.45549142202371
o  211 96.4718819914995
o  212 71.77623824869507
o  213 89.10523274097687
o  214 100.44040867968488
o  215 118.17732941353353
o  216 84.06214390969936
-  217 61.6567975086311
o  218 105.74604921252585
o  219 106.81335786801999
o  220 78.49109469999094
-  221 61.10210839568153
-  222 52.2313693808169
o  223 74.03296417067268
o  224 103.35655927900935
o  225 84.04796706945899
o  226 89.85248125551024
o  227 145.6834260790172
-  228 52.60554684262661
o  229 84.49694747881098
-  230 49.13364944595353
o  231 79.49768116558334
-  232 52.1185604773625
o  233 143.6608554295489
o  234 72.29740782746272
o  235 104.67656997133618
-  236 63.131440890396696
o  237 101.25035094158693
-  238 62.96179658008363
-  239 51.19097132478637
o  240 77.76394418246545
o  241 72.54932549773075
o  242 71.04550620627201
o  243 73.83732680824637
o  244 83.97921068303098
o  245 84.06324189110283
o  246 104.0870561599911
o  247 84.88588913675159
o  248 94.75390219915752
o  249 75.88419450221154
-  250 50.00922093612369
o  251 70.4389281440373
o  252 97.95898013640162
o  253 175.02433829051344
o  254 82.02839564840997
o  255 139.20723589743466
o  256 100.70542607460229
o  257 311.4895677480847
o  258 125.26118683127515
o  259 125.44329314069486
o  260 141.52202859264935
o  261 140.04112594798355
o  262 89.41074813444767
o  263 79.4152901102976
o  264 130.63124346386786
o  265 154.62044123637972
o  266 75.01319868708734
o  267 119.9071072430417
o  268 70.81739241994656
o  269 95.16384362498248
o  270 70.11469017291584
-  271 60.86349757705539
o  272 95.99388981484044
o  273 185.45435283880352
o  274 103.90069910442203
o  275 77.42014144920044
o  276 84.7522450480717
o  277 146.95640444944547
o  278 146.73066520986276
o  279 103.81185689916578
o  280 74.44124346517596
o  281 115.64619752235387
o  282 145.49198129060252
o  283 98.96043097292987
o  284 97.91206939714499
o  285 96.60898017004276
-  286 65.21801829764755
o  287 71.44784723878472
o  288 86.91016916832102
o  289 80.92527339887039
o  290 78.16593568825593
-  291 66.81128890901255
-  292 42.021122935314814
o  293 72.69283650314338
o  294 100.7429762855119
-  295 68.9599840159066
o  296 82.58989647358224
o  297 162.63214559160787
o  298 83.46919990479064
-  299 53.931600851382065
o  300 212.9820822071823
-  301 60.7696962621988
o  302 104.14842459371495
-  303 68.31988248436906
o  304 74.99424404757181
-  305 49.328322429683524
o  306 89.05300794706673
o  307 77.95189849150682
-  308 68.79345239426914
-  309 68.82744779396383
o  310 72.42636055915663
-  311 61.15283642861304
o  312 88.31401101281777
o  313 73.40385212437816
o  314 95.30700365459904
-  315 59.63662528061134
o  316 160.70910337867153
o  317 129.00249007137435
o  318 99.05040648753656
o  319 80.94998998165029
o  320 84.53144323323076
-  321 68.52941626990268
-  322 56.80429384152796
-  323 64.53067376773393
o  324 72.7230508539441
o  325 162.31152425371403
-  326 67.38508596713245
o  327 139.8114766794385
-  328 58.81366645852777
o  329 86.17924160674269
o  330 84.57640936131183
o  331 82.32530393648412
o  332 117.31530657254916
o  333 97.06515973811547
o  334 85.06081646666426
-  335 47.740035040905376
-  336 61.22718492151289
-  337 55.059852061753375
-  338 60.10670033574652
o  339 72.90246378654535
-  340 49.29952697481015
-  341 51.264257180396086
-  342 69.15010553736525
o  343 86.27320256185703
o  344 86.09559350369037
o  345 89.52798671391055
o  346 91.43236513752126
o  347 96.2235271886138
o  348 80.3995195148309
o  349 88.92431120757816
o  350 70.1305227371455
o  351 101.25020210907624
o  352 94.81420776267296
o  353 73.3077169088665
-  354 64.56092992089049
o  355 104.84158433620031
o  356 76.88982681286907
-  357 42.059168536984366
-  358 67.21424605802122
-  359 53.27329731297578
o  360 78.59124881883709
-  361 68.38212114476704
o  362 86.09002540352203
-  363 62.66461397735666
-  364 69.3786456251432
-  365 39.510085638179774
o  366 79.14926672357998
o  367 70.95394721022623
o  368 87.83812399707624
o  369 95.4014966869903
o  370 148.59646618502848
o  371 96.7957346063332
o  372 86.16201577246594
o  373 132.27116515490746
o  374 73.5582551907637
o  375 97.98031357040252
o  376 70.50606271509864
-  377 61.28323193725798
-  378 60.79151784496266
-  379 60.65144593001728
o  380 81.58152702209145
o  381 93.28561782721555
o  382 75.9573509684567
-  383 59.53836147475738
o  384 98.72555376712958
o  385 298.25407381232577
o  386 195.85277560320986
o  387 169.8466751864667
o  388 109.35690294607112
o  389 318.8819452367461
o  390 184.17704042324576
-  391 54.071809312904406
-  392 67.79237236608523
o  393 133.46610360307375
o  394 180.9430258570598
o  395 156.31163397205037
o  396 160.7104916970542
o  397 90.58061222409324
o  398 78.85048135376542
o  399 110.55448189529052
o  400 108.68443196837397
o  401 112.50275932084496
o  402 113.58006965469686
o  403 117.46186524538552
o  404 124.48780743830594
o  405 149.8258706122404
o  406 94.40066236343631
o  407 147.11958322327828
o  408 117.9327223116814
-  409 54.89740634073248
o  410 213.27796530647393
o  411 81.45343183504444
o  412 84.21224833948109
o  413 136.39433683930775
o  414 190.77437504288773
o  415 120.68962235314687
o  416 111.66471520307289
o  417 146.46454573463296
o  418 75.00036501222894
o  419 75.21696061882358
o  420 83.80826668760801
o  421 117.69559041063621
o  422 99.7360207839538
o  423 78.61780415262005
-  424 68.94655506686577
o  425 88.5436290596833
-  426 64.36682000131573
o  427 71.55469818098942
-  428 28.212085071196906
o  429 124.52132755480457
o  430 101.84957953076606
-  431 66.93976228466762
-  432 52.881732085999694
o  433 106.69577588857031
o  434 130.9298455623093
o  435 170.47612519585837
o  436 131.71840621698328
o  437 87.10855526089972
o  438 134.20232062740448
o  439 104.92413322005442
o  440 92.78015694672368
o  441 175.3746003476125
o  442 100.02609128745637
o  443 501.23521828413976
-  444 36.66405991903727
o  445 100.11728952147527
-  446 37.384899395250976
o  447 101.88020440632044
-  448 36.11403769817143
o  449 161.9757741697594
o  450 257.2029351650684
o  451 223.5455472440911
o  452 119.69384430833051
-  453 65.87787093154057
o  454 134.8030442752788
o  455 115.43287530231825
o  456 84.51470802311248
o  457 128.20436267933326
o  458 81.13602649274145
o  459 188.23608437091002
o  460 94.29288628401774
o  461 169.25470616936406
o  462 119.53139059220076
o  463 120.1045524408183
o  464 187.12270665869437
o  465 142.28063809309424
-  466 50.97633914132039
o  467 124.22206399475158
o  468 116.0529452487879
o  469 224.2817015135907
o  470 98.43403320501658
o  471 142.87604487778137
o  472 159.36372880886674
o  473 126.20476400252088
o  474 74.17119399477743
o  475 93.56361953548931
o  476 683.2504270876818
o  477 180.361723481803
o  478 74.73789642959423
o  479 73.44913066517456
o  480 83.28709419973843
o  481 87.9012291600419
-  482 36.044248681556205
o  483 95.2462259511074
o  484 96.22346932365748
o  485 222.22970279179856
o  486 138.14737097727206
o  487 80.1440922468787
o  488 149.33039029641947
o  489 156.28607435278164
o  490 113.19155907205192
o  491 87.32750147752964
o  492 162.5678783866475
o  493 125.72227014823748
o  494 228.89403629116424
o  495 81.94993868965503
o  496 131.40108817032865
o  497 140.29399833691775
o  498 103.73141085869013
o  499 106.03680761790228
o  500 160.27223346145303
o  501 142.0537028637121
o  502 315.65862303479423
o  503 90.40350064454215
o  504 75.87051421969383
o  505 92.9254588719639
o  506 116.17387874323049
-  507 34.59806584451653
o  508 85.41960496870516
o  509 107.19129142777507
o  510 73.29381670423318
-  511 40.833964239817014
o  512 113.72788482192084
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Trials.pkl
This many vals across loaded session
0 : 3288764
Assigning to SP.Params this item:
{'which_level': 'trial', '_list_events': ['fixcue', 'fix_touch', 'rulecue2', 'samp', 'go_cue', 'first_raise', 'on_strokeidx_0', 'off_stroke_last', 'doneb', 'post', 'reward_all'], 'list_events_uniqnames': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_features_extraction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'char_seq', 'epochset'], 'list_features_get_conjunction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'char_seq', 'epochset'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65], 'map_var_to_othervars': None, 'strokes_only_keep_single': False, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'map_var_to_levels': None}
Assigning to SP.ParamsGlobals this item:
{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.65, 'POST_DUR_CALC': 0.65, 'list_events': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65]}
stored in self.Dat[BehClass]
0
200
400
600
800
Running D.behclass_tokens_extract_datsegs
0
200
400
600
800
TODO!!! Merge this with other learning-related code
stored in self.Dat[BehClass]
0
200
400
600
800
Running D.behclass_tokens_extract_datsegs
0
200
400
600
800
trial # 0
trial # 100
trial # 200
trial # 300
trial # 400
trial # 500
trial # 600
trial # 700
trial # 800
Generated column called 'agent', which connects agent_kind-rule
n samples for conjunctions of score_name, agent_rule, agent_kind:
('binsucc', 'AnBmTR|0', 'model') :     368
('binsucc', 'AnBmTR|1', 'model') :     74
('binsucc', 'TR|0', 'model') :     237
('binsucc', 'TR|1', 'model') :     72
('binsucc', 'rndstr', 'model') :     149
TODO! _preprocess_sanity_check
Starting length of D.Dat: 900
self.Dat modified!!
Len, after remove aborts: 646
############ TAKING ONLY NO SUPERVISION TRIALS
--BEFORE REMOVE; existing supervision_stage_concise:
off|0||0         344
off|1|rank|0     196
mask|0||0         78
mask|1|rank|0     28
Name: supervision_stage_concise, dtype: int64
self.Dat modified!!
--AFTER REMOVE; existing supervision_stage_concise:
off|0||0        344
off|1|rank|0    196
Name: supervision_stage_concise, dtype: int64
Dataset final len: 540
-- Len of D, before applying this param: remove_repeated_trials, ... 540
appended col to self.Dat:
dummy
self.Dat starting legnth:  539
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  539
after: 539
-- Len of D, before applying this param: correct_sequencing_binary_score, ... 539
self.Dat starting legnth:  517
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  517
after: 517
-- Len of D, before applying this param: one_to_one_beh_task_strokes, ... 517
after: 517
Done!, new len of dataset 517
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff
starting sites:  389
starting sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 24, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 199, 200, 201, 202, 203, 204, 205, 206, 210, 211, 212, 213, 214, 215, 216, 218, 219, 220, 223, 224, 225, 226, 227, 229, 231, 233, 234, 235, 237, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 293, 294, 296, 297, 298, 300, 302, 304, 306, 307, 310, 312, 313, 314, 316, 318, 319, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 339, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 355, 356, 360, 362, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
For percentile 10, using this threshold: 3.270101735097089
sites_good:  350
sites_bad:  39
Updates self.Sites
ending sites:  350
ending sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 169, 171, 173, 174, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 218, 219, 220, 224, 225, 226, 227, 229, 231, 233, 235, 237, 240, 241, 243, 244, 245, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 284, 288, 289, 290, 296, 297, 298, 300, 302, 304, 306, 310, 312, 314, 316, 318, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 360, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_1way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['03_samp_-600_to_-50', '03_samp_50_to_600', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
DOing these! ...
list_events ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['03_samp_-600_to_-50', '03_samp_50_to_600', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
Running grouping_print_n_samples...
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  7 102
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-50
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_50_to_600
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.1
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-100_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-100_to_600
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['epochset']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff
starting sites:  350
starting sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 169, 171, 173, 174, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 218, 219, 220, 224, 225, 226, 227, 229, 231, 233, 235, 237, 240, 241, 243, 244, 245, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 284, 288, 289, 290, 296, 297, 298, 300, 302, 304, 306, 310, 312, 314, 316, 318, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 360, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
For percentile 10, using this threshold: 3.270101735097089
sites_good:  350
sites_bad:  39
Updates self.Sites
ending sites:  350
ending sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 169, 171, 173, 174, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 218, 219, 220, 224, 225, 226, 227, 229, 231, 233, 235, 237, 240, 241, 243, 244, 245, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 284, 288, 289, 290, 296, 297, 298, 300, 302, 304, 306, 310, 312, 314, 316, 318, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 360, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
df_var doesnt exist...!
COMPUTING df_var!!!
DOing these! ...
list_events ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
[]
Running grouping_print_n_samples...
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  1 63
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-50
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_50_to_600
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.1
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-100_to_600
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
site : 40
site : 60
site : 80
site : 140
site : 180
site : 200
site : 220
site : 240
site : 260
site : 300
site : 320
site : 360
site : 400
site : 420
site : 440
site : 460
site : 480
site : 500
SAving:  /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/df_var.pkl
SAving:  /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/list_eventwindow_event.pkl
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables
var -- vars_others:  epoch  ---  ['seqc_0_loc']
var -- vars_others:  epoch  ---  ['seqc_0_shape']
var -- vars_others:  epoch  ---  ['seqc_nstrokes_beh']
var -- vars_others:  epoch  ---  ['seqc_0_loc', 'seqc_0_shape']
var -- vars_others:  epoch  ---  ['seqc_0_loc', 'seqc_nstrokes_beh']
var -- vars_others:  epoch  ---  ['seqc_0_shape', 'seqc_nstrokes_beh']
var -- vars_others:  epoch  ---  ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-epoch-vs-varothers-levothers-levvar.txt
var -- vars_others:  seqc_0_loc  ---  ['epoch']
var -- vars_others:  seqc_0_loc  ---  ['seqc_0_shape']
var -- vars_others:  seqc_0_loc  ---  ['seqc_nstrokes_beh']
var -- vars_others:  seqc_0_loc  ---  ['epoch', 'seqc_0_shape']
var -- vars_others:  seqc_0_loc  ---  ['epoch', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_loc  ---  ['seqc_0_shape', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_loc  ---  ['epoch', 'seqc_0_shape', 'seqc_nstrokes_beh']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_0_loc-vs-varothers-levothers-levvar.txt
var -- vars_others:  seqc_0_shape  ---  ['epoch']
var -- vars_others:  seqc_0_shape  ---  ['seqc_0_loc']
var -- vars_others:  seqc_0_shape  ---  ['seqc_nstrokes_beh']
var -- vars_others:  seqc_0_shape  ---  ['epoch', 'seqc_0_loc']
var -- vars_others:  seqc_0_shape  ---  ['epoch', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_shape  ---  ['seqc_0_loc', 'seqc_nstrokes_beh']
var -- vars_others:  seqc_0_shape  ---  ['epoch', 'seqc_0_loc', 'seqc_nstrokes_beh']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_0_shape-vs-varothers-levothers-levvar.txt
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch']
var -- vars_others:  seqc_nstrokes_beh  ---  ['seqc_0_loc']
var -- vars_others:  seqc_nstrokes_beh  ---  ['seqc_0_shape']
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch', 'seqc_0_loc']
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch', 'seqc_0_shape']
var -- vars_others:  seqc_nstrokes_beh  ---  ['seqc_0_loc', 'seqc_0_shape']
var -- vars_others:  seqc_nstrokes_beh  ---  ['epoch', 'seqc_0_loc', 'seqc_0_shape']
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/snippets_check_conjunctions_variables/ncounts-seqc_nstrokes_beh-vs-varothers-levothers-levvar.txt
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/modulation
** Plotting summarystats
Saving at: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/modulation
Found this var:  epoch
Found this var_others:  ('seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh')
Aggregating dataframe over all othervars ...
Plotting ...
Plotting for specific single other var: seqc_0_loc...
Plotting for specific single other var: seqc_0_shape...
Plotting for specific single other var: seqc_nstrokes_beh...
** Plotting heatmaps
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/modulation_heatmap
Saving to:  /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/modulation_heatmap/brainschem-event-val-modulation_subgroups.pdf
Saving to:  /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/modulation_heatmap/brainschem-event-val-modulation_subgroups-NOMOTOR.pdf
** Plotting example strokes
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Trials.pkl
Plotting ..  ((-1, 0), 'line-8-4-0', 4)
Plotting ..  ((2, 0), 'line-8-4-0', 4)
Plotting ..  ((-1, 0), 'line-8-3-0', 4)
Plotting ..  ((-1, 1), 'line-8-4-0', 4)
Plotting ..  ((0, 1), 'line-8-4-0', 4)
Plotting ..  ((2, 1), 'line-8-4-0', 4)
Plotting ..  ((-1, 1), 'line-8-3-0', 4)
Plotting ..  ((0, 0), 'line-8-4-0', 4)
Plotting ..  ((1, 0), 'line-8-4-0', 4)
Plotting ..  ((1, 1), 'line-8-4-0', 4)
** Making plots for this event_window: 
09_post_50_to_600
Saving this event, 09_post_50_to_600, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/09_post_50_to_600
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/09_post_50_to_600/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/09_post
** Making plots for this event_window: 
10_reward_all_50_to_600
Saving this event, 10_reward_all_50_to_600, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/10_reward_all_50_to_600
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/10_reward_all_50_to_600/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/10_reward_all
** Making plots for this event_window: 
05_first_raise_-600_to_-50
Saving this event, 05_first_raise_-600_to_-50, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/05_first_raise_-600_to_-50
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/05_first_raise_-600_to_-50/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/05_first_raise
** Making plots for this event_window: 
06_on_strokeidx_0_-100_to_600
/home/lucast4/miniconda3/envs/drag2_matlab/lib/python3.8/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.5.2, the latest is 0.5.3.
Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.
  return warn(
/gorilla1/code/pythonlib/pythonlib/tools/snstools.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator
  a.set_xticklabels(a.get_xticklabels(), rotation=rotation,
Saving this event, 06_on_strokeidx_0_-100_to_600, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/06_on_strokeidx_0_-100_to_600
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/06_on_strokeidx_0_-100_to_600/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/06_on_strokeidx_0
** Making plots for this event_window: 
03_samp_50_to_600
Saving this event, 03_samp_50_to_600, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/03_samp_50_to_600
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/03_samp_50_to_600/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/03_samp
** Making plots for this event_window: 
08_doneb_-500_to_300
Saving this event, 08_doneb_-500_to_300, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/08_doneb_-500_to_300
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/08_doneb_-500_to_300/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/08_doneb
** Making plots for this event_window: 
03_samp_-600_to_-50
Saving this event, 03_samp_-600_to_-50, to /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/03_samp_-600_to_-50
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_1way_mshuff/EACH_EVENT/03_samp_-600_to_-50/modulation_v2
** Plotting summarystats
** Plotting raster + sm fr: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/rasters/03_samp
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
session:  0
1
Beh Sessions that exist on this date:  {220928: [(1, 'grammardircolor2')]}
taking this beh session: 1
Loading these beh expts: ['grammardircolor2']
Loading these beh sessions: [1]
Loading this neural session: 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
{'filename_components_hyphened': ['Pancho', '220928', '153315'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/220928'], 'basedirs_filenames': ['220928', 'Pancho-220928-153315'], 'filename_final_ext': 'Pancho-220928-153315', 'filename_final_noext': 'Pancho-220928-153315'}
FOund this path for spikes:  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/Pancho-220928-153315
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
final_dir_name  --  Pancho-220928-153315
time  --  153315
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/220928.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
-  1 64.70642265762471
o  2 92.75227027557685
o  3 92.98346476404103
-  4 69.26300597781224
o  5 87.25131996000691
o  6 89.47348011043788
o  7 123.48862681173287
o  8 82.36742131823412
o  9 210.58342301743832
o  10 97.8338766882688
o  11 139.7793052085557
o  12 78.09594426531484
o  13 226.68211889801114
o  14 125.96902141507182
o  15 177.50962307076344
o  16 169.0339794851474
o  17 91.40468793030692
o  18 114.7112663919659
o  19 447.84476403279695
-  20 49.657786596331974
o  21 111.04103222186241
o  22 80.79252047164519
-  23 53.960284735499044
o  24 76.87724899829735
o  25 87.94239639971866
o  26 416.6047760693619
o  27 175.16858508911247
o  28 78.79534329460148
o  29 209.9478275293116
o  30 88.47719042327736
o  31 176.89209718514522
o  32 95.2496299257903
o  33 87.4359746693015
o  34 114.01539566523861
-  35 38.93969556403031
o  36 373.2891541595018
o  37 99.44842975203701
o  38 252.53823987135115
o  39 94.81235612025557
o  40 270.29760878907985
o  41 102.97177115304093
o  42 240.148420927699
o  43 143.49302710044867
o  44 111.54485238637135
o  45 753.6843257890603
-  46 54.73407165935636
o  47 293.75944046092775
-  48 47.234390653264924
o  49 115.02418574890638
-  50 53.828532283988714
o  51 119.71501750642446
o  52 107.76414954235045
o  53 204.65652060189296
o  54 86.29427946202486
o  55 946.7906213292976
o  56 78.05517264386555
o  57 150.34317616250829
o  58 110.98779724632763
o  59 217.7508589785519
o  60 79.65383838411645
o  61 163.05304001054537
o  62 148.27777735896674
o  63 221.9230968334003
o  64 72.19433454630017
o  65 83.68833072772688
o  66 84.26878408671874
o  67 86.67612308622593
o  68 179.71822423141347
o  69 79.18578737111966
-  70 63.76291332805913
o  71 80.82257871768017
o  72 98.02535221837316
o  73 82.0325838508761
o  74 153.31031394906017
o  75 71.76126008841857
o  76 81.3908591756538
o  77 84.9097253104328
o  78 118.48957762751516
-  79 60.581868340287016
o  80 106.58424551947446
-  81 68.56237777558249
-  82 61.71513968765576
-  83 68.51139770521044
o  84 87.87795961366429
o  85 72.70916986784341
o  86 89.30877284717246
-  87 53.10114065347064
o  88 90.88787800284081
-  89 59.558932880231126
-  90 65.11061085344572
-  91 68.6133183438779
-  92 53.04963410161747
-  93 64.20662420022272
o  94 75.01366663086841
-  95 60.78659094111654
o  96 73.4387118520339
o  97 178.2916104760665
o  98 102.90327328281333
o  99 93.5625205554068
o  100 173.08397384706478
o  101 72.84249597570461
o  102 88.21222013753118
o  103 70.11250577140594
-  104 61.2734181474696
o  105 139.87347959570872
-  106 69.81735512964569
-  107 68.2651792479971
-  108 68.15289861893983
-  109 53.368861382725775
-  110 65.87789363329594
-  111 69.92766914309591
o  112 75.88835379688399
o  113 77.6405479099722
o  114 75.19167905313043
o  115 71.40504654565294
o  116 168.65421747240146
-  117 53.00111040470851
o  118 103.27954960464932
o  119 92.68173800763262
-  120 59.6586011134239
o  121 179.19744066805032
o  122 131.9150475032378
o  123 97.06718544852906
o  124 109.67907436106853
-  125 47.437565477022034
-  126 68.83772244578503
-  127 65.388841772671
-  128 50.46087513981149
-  129 46.832394374234354
o  130 145.0988710021157
o  131 79.03208207144435
o  132 233.64404025717513
o  133 200.91175213695985
o  134 302.4420934936441
o  135 88.02134319124198
o  136 118.24251260979983
-  137 65.6893409090676
o  138 312.0881642975485
o  139 92.95397947517722
o  140 116.82239945014548
o  141 91.03375999612653
o  142 147.8633350129349
o  143 107.28840027904667
o  144 300.95771258459354
-  145 57.21322932065123
o  146 98.16200614444388
-  147 48.78191968945475
o  148 115.54929172894919
o  149 78.12888639250232
o  150 110.99772739085198
-  151 50.381534237951925
o  152 110.70270701063637
o  153 97.76821191569351
o  154 92.94143474201482
-  155 58.229312690939906
o  156 132.89394639189834
o  157 130.0816241324179
o  158 79.53065555166987
o  159 79.62019363936379
o  160 142.98818278377686
o  161 160.5381275015413
o  162 80.99151552634922
o  163 93.6586569374334
o  164 81.12202931078691
o  165 178.1333505722069
o  166 77.15672621449986
o  167 104.2846370700293
o  168 77.59290303804482
o  169 78.33415503435735
o  170 95.26062924094659
o  171 160.58715233833027
-  172 59.11464212475558
o  173 302.92405111386125
o  174 85.65809316262767
o  175 77.62037397004508
o  176 74.98871117545787
o  177 222.86958611468526
o  178 81.5341366042363
o  179 435.1815414617571
o  180 70.74635909027002
o  181 142.65732002822978
o  182 72.90446142409318
-  183 64.14923213132916
o  184 110.3527281857254
-  185 30.054779596584666
o  186 115.34744849825354
o  187 128.84215224602863
o  188 84.42468385949833
o  189 90.90663451776442
o  190 74.16401356135052
o  191 95.53207402413663
o  192 103.12068173596309
o  193 131.66434499793354
o  194 88.04311039365767
o  195 207.20225650798577
-  196 68.07234328833087
o  197 133.8152733526463
-  198 53.93579706543043
o  199 78.46291114229044
o  200 81.97217108677278
o  201 85.5512937608602
o  202 108.1723629331069
o  203 123.81149655029022
o  204 80.32044559427996
o  205 86.82384255454015
o  206 76.41619977366372
-  207 67.7237184770418
-  208 61.167078090212
-  209 63.195706309375964
o  210 123.45549142202371
o  211 96.4718819914995
o  212 71.77623824869507
o  213 89.10523274097687
o  214 100.44040867968488
o  215 118.17732941353353
o  216 84.06214390969936
-  217 61.6567975086311
o  218 105.74604921252585
o  219 106.81335786801999
o  220 78.49109469999094
-  221 61.10210839568153
-  222 52.2313693808169
o  223 74.03296417067268
o  224 103.35655927900935
o  225 84.04796706945899
o  226 89.85248125551024
o  227 145.6834260790172
-  228 52.60554684262661
o  229 84.49694747881098
-  230 49.13364944595353
o  231 79.49768116558334
-  232 52.1185604773625
o  233 143.6608554295489
o  234 72.29740782746272
o  235 104.67656997133618
-  236 63.131440890396696
o  237 101.25035094158693
-  238 62.96179658008363
-  239 51.19097132478637
o  240 77.76394418246545
o  241 72.54932549773075
o  242 71.04550620627201
o  243 73.83732680824637
o  244 83.97921068303098
o  245 84.06324189110283
o  246 104.0870561599911
o  247 84.88588913675159
o  248 94.75390219915752
o  249 75.88419450221154
-  250 50.00922093612369
o  251 70.4389281440373
o  252 97.95898013640162
o  253 175.02433829051344
o  254 82.02839564840997
o  255 139.20723589743466
o  256 100.70542607460229
o  257 311.4895677480847
o  258 125.26118683127515
o  259 125.44329314069486
o  260 141.52202859264935
o  261 140.04112594798355
o  262 89.41074813444767
o  263 79.4152901102976
o  264 130.63124346386786
o  265 154.62044123637972
o  266 75.01319868708734
o  267 119.9071072430417
o  268 70.81739241994656
o  269 95.16384362498248
o  270 70.11469017291584
-  271 60.86349757705539
o  272 95.99388981484044
o  273 185.45435283880352
o  274 103.90069910442203
o  275 77.42014144920044
o  276 84.7522450480717
o  277 146.95640444944547
o  278 146.73066520986276
o  279 103.81185689916578
o  280 74.44124346517596
o  281 115.64619752235387
o  282 145.49198129060252
o  283 98.96043097292987
o  284 97.91206939714499
o  285 96.60898017004276
-  286 65.21801829764755
o  287 71.44784723878472
o  288 86.91016916832102
o  289 80.92527339887039
o  290 78.16593568825593
-  291 66.81128890901255
-  292 42.021122935314814
o  293 72.69283650314338
o  294 100.7429762855119
-  295 68.9599840159066
o  296 82.58989647358224
o  297 162.63214559160787
o  298 83.46919990479064
-  299 53.931600851382065
o  300 212.9820822071823
-  301 60.7696962621988
o  302 104.14842459371495
-  303 68.31988248436906
o  304 74.99424404757181
-  305 49.328322429683524
o  306 89.05300794706673
o  307 77.95189849150682
-  308 68.79345239426914
-  309 68.82744779396383
o  310 72.42636055915663
-  311 61.15283642861304
o  312 88.31401101281777
o  313 73.40385212437816
o  314 95.30700365459904
-  315 59.63662528061134
o  316 160.70910337867153
o  317 129.00249007137435
o  318 99.05040648753656
o  319 80.94998998165029
o  320 84.53144323323076
-  321 68.52941626990268
-  322 56.80429384152796
-  323 64.53067376773393
o  324 72.7230508539441
o  325 162.31152425371403
-  326 67.38508596713245
o  327 139.8114766794385
-  328 58.81366645852777
o  329 86.17924160674269
o  330 84.57640936131183
o  331 82.32530393648412
o  332 117.31530657254916
o  333 97.06515973811547
o  334 85.06081646666426
-  335 47.740035040905376
-  336 61.22718492151289
-  337 55.059852061753375
-  338 60.10670033574652
o  339 72.90246378654535
-  340 49.29952697481015
-  341 51.264257180396086
-  342 69.15010553736525
o  343 86.27320256185703
o  344 86.09559350369037
o  345 89.52798671391055
o  346 91.43236513752126
o  347 96.2235271886138
o  348 80.3995195148309
o  349 88.92431120757816
o  350 70.1305227371455
o  351 101.25020210907624
o  352 94.81420776267296
o  353 73.3077169088665
-  354 64.56092992089049
o  355 104.84158433620031
o  356 76.88982681286907
-  357 42.059168536984366
-  358 67.21424605802122
-  359 53.27329731297578
o  360 78.59124881883709
-  361 68.38212114476704
o  362 86.09002540352203
-  363 62.66461397735666
-  364 69.3786456251432
-  365 39.510085638179774
o  366 79.14926672357998
o  367 70.95394721022623
o  368 87.83812399707624
o  369 95.4014966869903
o  370 148.59646618502848
o  371 96.7957346063332
o  372 86.16201577246594
o  373 132.27116515490746
o  374 73.5582551907637
o  375 97.98031357040252
o  376 70.50606271509864
-  377 61.28323193725798
-  378 60.79151784496266
-  379 60.65144593001728
o  380 81.58152702209145
o  381 93.28561782721555
o  382 75.9573509684567
-  383 59.53836147475738
o  384 98.72555376712958
o  385 298.25407381232577
o  386 195.85277560320986
o  387 169.8466751864667
o  388 109.35690294607112
o  389 318.8819452367461
o  390 184.17704042324576
-  391 54.071809312904406
-  392 67.79237236608523
o  393 133.46610360307375
o  394 180.9430258570598
o  395 156.31163397205037
o  396 160.7104916970542
o  397 90.58061222409324
o  398 78.85048135376542
o  399 110.55448189529052
o  400 108.68443196837397
o  401 112.50275932084496
o  402 113.58006965469686
o  403 117.46186524538552
o  404 124.48780743830594
o  405 149.8258706122404
o  406 94.40066236343631
o  407 147.11958322327828
o  408 117.9327223116814
-  409 54.89740634073248
o  410 213.27796530647393
o  411 81.45343183504444
o  412 84.21224833948109
o  413 136.39433683930775
o  414 190.77437504288773
o  415 120.68962235314687
o  416 111.66471520307289
o  417 146.46454573463296
o  418 75.00036501222894
o  419 75.21696061882358
o  420 83.80826668760801
o  421 117.69559041063621
o  422 99.7360207839538
o  423 78.61780415262005
-  424 68.94655506686577
o  425 88.5436290596833
-  426 64.36682000131573
o  427 71.55469818098942
-  428 28.212085071196906
o  429 124.52132755480457
o  430 101.84957953076606
-  431 66.93976228466762
-  432 52.881732085999694
o  433 106.69577588857031
o  434 130.9298455623093
o  435 170.47612519585837
o  436 131.71840621698328
o  437 87.10855526089972
o  438 134.20232062740448
o  439 104.92413322005442
o  440 92.78015694672368
o  441 175.3746003476125
o  442 100.02609128745637
o  443 501.23521828413976
-  444 36.66405991903727
o  445 100.11728952147527
-  446 37.384899395250976
o  447 101.88020440632044
-  448 36.11403769817143
o  449 161.9757741697594
o  450 257.2029351650684
o  451 223.5455472440911
o  452 119.69384430833051
-  453 65.87787093154057
o  454 134.8030442752788
o  455 115.43287530231825
o  456 84.51470802311248
o  457 128.20436267933326
o  458 81.13602649274145
o  459 188.23608437091002
o  460 94.29288628401774
o  461 169.25470616936406
o  462 119.53139059220076
o  463 120.1045524408183
o  464 187.12270665869437
o  465 142.28063809309424
-  466 50.97633914132039
o  467 124.22206399475158
o  468 116.0529452487879
o  469 224.2817015135907
o  470 98.43403320501658
o  471 142.87604487778137
o  472 159.36372880886674
o  473 126.20476400252088
o  474 74.17119399477743
o  475 93.56361953548931
o  476 683.2504270876818
o  477 180.361723481803
o  478 74.73789642959423
o  479 73.44913066517456
o  480 83.28709419973843
o  481 87.9012291600419
-  482 36.044248681556205
o  483 95.2462259511074
o  484 96.22346932365748
o  485 222.22970279179856
o  486 138.14737097727206
o  487 80.1440922468787
o  488 149.33039029641947
o  489 156.28607435278164
o  490 113.19155907205192
o  491 87.32750147752964
o  492 162.5678783866475
o  493 125.72227014823748
o  494 228.89403629116424
o  495 81.94993868965503
o  496 131.40108817032865
o  497 140.29399833691775
o  498 103.73141085869013
o  499 106.03680761790228
o  500 160.27223346145303
o  501 142.0537028637121
o  502 315.65862303479423
o  503 90.40350064454215
o  504 75.87051421969383
o  505 92.9254588719639
o  506 116.17387874323049
-  507 34.59806584451653
o  508 85.41960496870516
o  509 107.19129142777507
o  510 73.29381670423318
-  511 40.833964239817014
o  512 113.72788482192084
== Loading TDT tank
** Loading tank data from local (previusly cached)
== Done
== Trying to load events data
Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
== Done
** MINIMAL_LOADING, therefore loading previuosly cached data
Generated self._MapperTrialcode2TrialToTrial!
Extracted into self.Dat[epoch_orig]
Extracted successfully for session:  0
Generated index mappers!
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DfScalar.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/fr_sm_times.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/DS.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Params.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/ParamsGlobals.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Sites.pkl
Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Pancho-220928-sess_0/Trials.pkl
This many vals across loaded session
0 : 3288764
Assigning to SP.Params this item:
{'which_level': 'trial', '_list_events': ['fixcue', 'fix_touch', 'rulecue2', 'samp', 'go_cue', 'first_raise', 'on_strokeidx_0', 'off_stroke_last', 'doneb', 'post', 'reward_all'], 'list_events_uniqnames': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_features_extraction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'char_seq', 'epochset'], 'list_features_get_conjunction': ['probe', 'taskgroup', 'character', 'trialcode', 'epoch', 'task_kind', 'supervision_stage_concise', 'seqc_nstrokes_beh', 'seqc_nstrokes_task', 'seqc_0_shape', 'seqc_0_loc', 'seqc_1_shape', 'seqc_1_loc', 'seqc_2_shape', 'seqc_2_loc', 'seqc_3_shape', 'seqc_3_loc', 'gridsize', 'char_seq', 'epochset'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65], 'map_var_to_othervars': None, 'strokes_only_keep_single': False, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'map_var_to_levels': None}
Assigning to SP.ParamsGlobals this item:
{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.65, 'POST_DUR_CALC': 0.65, 'list_events': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_pre_dur': [-0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65, -0.65], 'list_post_dur': [0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65]}
stored in self.Dat[BehClass]
0
200
400
600
800
Running D.behclass_tokens_extract_datsegs
0
200
400
600
800
TODO!!! Merge this with other learning-related code
stored in self.Dat[BehClass]
0
200
400
600
800
Running D.behclass_tokens_extract_datsegs
0
200
400
600
800
trial # 0
trial # 100
trial # 200
trial # 300
trial # 400
trial # 500
trial # 600
trial # 700
trial # 800
Generated column called 'agent', which connects agent_kind-rule
n samples for conjunctions of score_name, agent_rule, agent_kind:
('binsucc', 'AnBmTR|0', 'model') :     368
('binsucc', 'AnBmTR|1', 'model') :     74
('binsucc', 'TR|0', 'model') :     237
('binsucc', 'TR|1', 'model') :     72
('binsucc', 'rndstr', 'model') :     149
TODO! _preprocess_sanity_check
Starting length of D.Dat: 900
self.Dat modified!!
Len, after remove aborts: 646
############ TAKING ONLY NO SUPERVISION TRIALS
--BEFORE REMOVE; existing supervision_stage_concise:
off|0||0         344
off|1|rank|0     196
mask|0||0         78
mask|1|rank|0     28
Name: supervision_stage_concise, dtype: int64
self.Dat modified!!
--AFTER REMOVE; existing supervision_stage_concise:
off|0||0        344
off|1|rank|0    196
Name: supervision_stage_concise, dtype: int64
Dataset final len: 540
-- Len of D, before applying this param: remove_repeated_trials, ... 540
appended col to self.Dat:
dummy
self.Dat starting legnth:  539
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  539
after: 539
-- Len of D, before applying this param: correct_sequencing_binary_score, ... 539
self.Dat starting legnth:  517
Modified self.Dat, keeping only the inputted inds
self.Dat final legnth:  517
after: 517
-- Len of D, before applying this param: one_to_one_beh_task_strokes, ... 517
after: 517
Done!, new len of dataset 517
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_2way_mshuff
starting sites:  389
starting sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 24, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 113, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 199, 200, 201, 202, 203, 204, 205, 206, 210, 211, 212, 213, 214, 215, 216, 218, 219, 220, 223, 224, 225, 226, 227, 229, 231, 233, 234, 235, 237, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 293, 294, 296, 297, 298, 300, 302, 304, 306, 307, 310, 312, 313, 314, 316, 318, 319, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 339, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 355, 356, 360, 362, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
For percentile 10, using this threshold: 3.270101735097089
sites_good:  350
sites_bad:  39
Updates self.Sites
ending sites:  350
ending sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 169, 171, 173, 174, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 218, 219, 220, 224, 225, 226, 227, 229, 231, 233, 235, 237, 240, 241, 243, 244, 245, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 284, 288, 289, 290, 296, 297, 298, 300, 302, 304, 306, 310, 312, 314, 316, 318, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 360, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_2way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_2way_mshuff/df_var.pkl
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_epochset/SV_r2_maxtime_2way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['03_samp_-600_to_-50', '03_samp_50_to_600', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
DOing these! ...
list_events ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['03_samp_-600_to_-50', '03_samp_50_to_600', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
Running grouping_print_n_samples...
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  7 102
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-50
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_50_to_600
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.1
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-100_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-100_to_600
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['epochset']
Saving to: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_2way_mshuff
starting sites:  350
starting sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 169, 171, 173, 174, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 218, 219, 220, 224, 225, 226, 227, 229, 231, 233, 235, 237, 240, 241, 243, 244, 245, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 284, 288, 289, 290, 296, 297, 298, 300, 302, 304, 306, 310, 312, 314, 316, 318, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 360, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
For percentile 10, using this threshold: 3.270101735097089
sites_good:  350
sites_bad:  39
Updates self.Sites
ending sites:  350
ending sites:  [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 25, 27, 28, 29, 30, 31, 32, 33, 37, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 80, 84, 85, 86, 88, 94, 96, 97, 98, 99, 101, 102, 103, 105, 112, 114, 115, 116, 118, 119, 121, 122, 123, 124, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 152, 153, 154, 156, 157, 159, 161, 162, 163, 164, 165, 166, 167, 169, 171, 173, 174, 177, 178, 179, 180, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 203, 204, 205, 210, 211, 212, 213, 215, 216, 218, 219, 220, 224, 225, 226, 227, 229, 231, 233, 235, 237, 240, 241, 243, 244, 245, 247, 248, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 265, 267, 268, 269, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 284, 288, 289, 290, 296, 297, 298, 300, 302, 304, 306, 310, 312, 314, 316, 318, 320, 324, 325, 327, 329, 330, 331, 332, 333, 334, 343, 344, 345, 346, 348, 349, 350, 351, 352, 353, 355, 356, 360, 366, 368, 369, 370, 371, 372, 373, 374, 375, 376, 384, 385, 386, 387, 388, 389, 390, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 427, 429, 430, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 445, 447, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 512]
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.65
POST_DUR_CALC  =  0.65
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
TODO: do fr scalar computation only once! takes too much time.
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_2way_mshuff/df_var.pkl
Searching for already-done df_var at this path:
/gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_2way_mshuff/df_var.pkl
RELOADED df_var!!!
... from: /gorilla1/analyses/recordings/main/anova/bytrial/MULT_SESS/Pancho-220928-0/rulesw/var_by_varsother/VAR_epoch-OV_seqc_0_loc_seqc_0_shape_seqc_nstrokes_beh/SV_r2_maxtime_2way_mshuff/df_var.pkl
Events already done: (will skip these when recomputing)...
['03_samp_-600_to_-50', '03_samp_50_to_600', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
COMPUTING df_var!!!
DOing these! ...
list_events ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
WILL SKIP THESE EVENTS...
['03_samp_-600_to_-50', '03_samp_50_to_600', '05_first_raise_-600_to_-50', '06_on_strokeidx_0_-100_to_600', '08_doneb_-500_to_300', '09_post_50_to_600', '10_reward_all_50_to_600']
Running grouping_print_n_samples...
GOOD!, enough data, max n per grouping conjunction (nmin, nmax)  1 63
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_-600_to_-50
 
Updated ParamsGlobals for event 03_samp to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  03_samp_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
03_samp_50_to_600
 
Updated ParamsGlobals for event 05_first_raise to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.6
POST_DUR_CALC  =  -0.05
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  05_first_raise_-600_to_-50
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
05_first_raise_-600_to_-50
 
Updated ParamsGlobals for event 06_on_strokeidx_0 to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.1
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  06_on_strokeidx_0_-100_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
06_on_strokeidx_0_-100_to_600
 
Updated ParamsGlobals for event 08_doneb to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  -0.5
POST_DUR_CALC  =  0.3
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  08_doneb_-500_to_300
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
08_doneb_-500_to_300
 
Updated ParamsGlobals for event 09_post to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  09_post_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
09_post_50_to_600
 
Updated ParamsGlobals for event 10_reward_all to:
Updated self.ParamsGlobals:
n_min_trials_per_level  =  8
lenient_allow_data_if_has_n_levels  =  2
PRE_DUR_CALC  =  0.05
POST_DUR_CALC  =  0.6
list_events  =  ['03_samp', '03_samp', '05_first_raise', '06_on_strokeidx_0', '08_doneb', '09_post', '10_reward_all']
list_pre_dur  =  [-0.6, 0.05, -0.6, -0.1, -0.5, 0.05, 0.05]
list_post_dur  =  [-0.05, 0.6, -0.05, 0.6, 0.3, 0.6, 0.6]
DOING THIS EVENT:  10_reward_all_50_to_600
!!!! SKIPPING this event, since it is in events_windowed_skip that you entered:
10_reward_all_50_to_600
SKIPPING, extracted df_var is empty. Probably you have not enough data for this conjunctions, try setting DEBUG_CONJUNCTIONS=True and reading the low-level data it prints.
!! SKIPPING:  epoch ['seqc_0_loc', 'seqc_0_shape', 'seqc_nstrokes_beh']
./_analy_anova_script.sh: line 6: syntax error: unexpected end of file
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
session:  0
1
Beh Sessions that exist on this date:  {220928: [(1, 'grammardircolor2')]}
taking this beh session: 1
Loading these beh expts: ['grammardircolor2']
Loading these beh sessions: [1]
Loading this neural session: 0
Searching using this string:
/mnt/hopfield_data01/ltian/recordings/*Pancho*/*220928*/**
Found this many paths:
1
---
/mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
{'filename_components_hyphened': ['Pancho', '220928', '153315'], 'basedirs': ['/mnt/hopfield_data01/ltian/recordings/Pancho', '/mnt/hopfield_data01/ltian/recordings/Pancho/220928'], 'basedirs_filenames': ['220928', 'Pancho-220928-153315'], 'filename_final_ext': 'Pancho-220928-153315', 'filename_final_noext': 'Pancho-220928-153315'}
FOund this path for spikes:  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
== PATHS for this expt: 
raws  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315
tank  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/Pancho-220928-153315
spikes  --  /mnt/hopfield_data01/ltian/recordings/Pancho/220928/Pancho-220928-153315/spikes_tdt_quick-4.5
final_dir_name  --  Pancho-220928-153315
time  --  153315
pathbase_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315
tank_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_tank.pkl
spikes_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_spikes.pkl
datall_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/data_datall.pkl
events_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/events_photodiode.pkl
mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/mapper_st2dat.pkl
figs_local  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/figs
metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units
cached_dir  --  /gorilla1/neural_preprocess/recordings/Pancho/220928/Pancho-220928-153315/cached
Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units/220928.yaml
updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')
[_sitesdirty_update] skipping! since did not find:  sites_error_spikes
Printing whether spikes gotten (o) or not (-) because of spike peak to trough
-  1 64.70642265762471
o  2 92.75227027557685
o  3 92.98346476404103
-  4 69.26300597781224
o  5 87.25131996000691
o  6 89.47348011043788
o  7 123.48862681173287
o  8 82.36742131823412
o  9 210.58342301743832
o  10 97.8338766882688
o  11 139.7793052085557
o  12 78.09594426531484
o  13 226.68211889801114
o  14 125.96902141507182
o  15 177.50962307076344
o  16 169.0339794851474
o  17 91.40468793030692
o  18 114.7112663919659
o  19 447.84476403279695
-  20 49.657786596331974
o  21 111.04103222186241
o  22 80.79252047164519
-  23 53.960284735499044
o  24 76.87724899829735
o  25 87.94239639971866
o  26 416.6047760693619
o  27 175.16858508911247
o  28 78.79534329460148
o  29 209.9478275293116
o  30 88.47719042327736
o  31 176.89209718514522
o  32 95.2496299257903
o  33 87.4359746693015
o  34 114.01539566523861
-  35 38.93969556403031
o  36 373.2891541595018
o  37 99.44842975203701
o  38 252.53823987135115
o  39 94.81235612025557
o  40 270.29760878907985
o  41 102.97177115304093
o  42 240.148420927699
o  43 143.49302710044867
o  44 111.54485238637135
o  45 753.6843257890603
-  46 54.73407165935636
o  47 293.75944046092775
-  48 47.234390653264924
o  49 115.02418574890638
-  50 53.828532283988714
o  51 119.71501750642446
o  52 107.76414954235045
o  53 204.65652060189296
o  54 86.29427946202486
o  55 946.7906213292976
o  56 78.05517264386555
o  57 150.34317616250829
o  58 110.98779724632763
o  59 217.7508589785519
o  60 79.65383838411645
o  61 163.05304001054537
o  62 148.27777735896674
o  63 221.9230968334003
o  64 72.19433454630017
o  65 83.68833072772688
o  66 84.26878408671874
o  67 86.67612308622593
o  68 179.71822423141347
o  69 79.18578737111966
-  70 63.76291332805913
o  71 80.82257871768017
o  72 98.02535221837316
o  73 82.0325838508761
o  74 153.31031394906017
o  75 71.76126008841857
o  76 81.3908591756538
o  77 84.9097253104328
o  78 118.48957762751516
-  79 60.581868340287016
o  80 106.58424551947446
-  81 68.56237777558249
-  82 61.71513968765576
-  83 68.51139770521044
o  84 87.87795961366429
o  85 72.70916986784341
o  86 89.30877284717246
-  87 53.10114065347064
o  88 90.88787800284081
-  89 59.558932880231126
-  90 65.11061085344572
-  91 68.6133183438779
-  92 53.04963410161747
-  93 64.20662420022272
o  94 75.01366663086841
-  95 60.78659094111654
o  96 73.4387118520339
o  97 178.2916104760665
o  98 102.90327328281333
o  99 93.5625205554068
o  100 173.08397384706478
o  101 72.84249597570461
o  102 88.21222013753118
o  103 70.11250577140594
-  104 61.2734181474696
o  105 139.87347959570872
-  106 69.81735512964569
-  107 68.2651792479971
-  108 68.15289861893983
-  109 53.368861382725775
-  110 65.87789363329594
-  111 69.92766914309591
o  112 75.88835379688399
o  113 77.6405479099722
o  114 75.19167905313043
o  115 71.40504654565294
o  116 168.65421747240146
-  117 53.00111040470851
o  118 103.27954960464932
o  119 92.68173800763262
-  120 59.6586011134239
o  121 179.19744066805032
o  122 131.9150475032378
o  123 97.06718544852906
o  124 109.67907436106853
-  125 47.437565477022034
-  126 68.83772244578503
-  127 65.388841772671
-  128 50.46087513981149
-  129 46.832394374234354
o  130 145.0988710021157
o  131 79.03208207144435
o  132 233.64404025717513
o  133 200.91175213695985
o  134 302.4420934936441
o  135 88.02134319124198
o  136 118.24251260979983
-  137 65.6893409090676
o  138 312.0881642975485
o  139 92.95397947517722
o  140 116.82239945014548
o  141 91.03375999612653
o  142 147.8633350129349
o  143 107.28840027904667
o  144 300.95771258459354
-  145 57.21322932065123
o  146 98.16200614444388
-  147 48.78191968945475
o  148 115.54929172894919
o  149 78.12888639250232
o  150 110.99772739085198
-  151 50.381534237951925
o  152 110.70270701063637
o  153 97.76821191569351
o  154 92.94143474201482
-  155 58.229312690939906
o  156 132.89394639189834
o  157 130.0816241324179
o  158 79.53065555166987
o  159 79.62019363936379
o  160 142.98818278377686
o  161 160.5381275015413
o  162 80.99151552634922
o  163 93.6586569374334
o  164 81.12202931078691
o  165 178.1333505722069
o  166 77.15672621449986
o  167 104.2846370700293
o  168 77.59290303804482
o  169 78.33415503435735
o  170 95.26062924094659
o  171 160.58715233833027
-  172 59.11464212475558
o  173 302.92405111386125
o  174 85.65809316262767
o  175 77.62037397004508
o  176 74.98871117545787
o  177 222.86958611468526
o  178 81.5341366042363
o  179 435.1815414617571
o  180 70.74635909027002
o  181 142.65732002822978
o  182 72.90446142409318
-  183 64.14923213132916
o  184 110.3527281857254
-  185 30.054779596584666
o  186 115.34744849825354
o  187 128.84215224602863
o  188 84.42468385949833
o  189 90.90663451776442
o  190 74.16401356135052
o  191 95.53207402413663
o  192 103.12068173596309
o  193 131.66434499793354
o  194 88.04311039365767
o  195 207.20225650798577
-  196 68.07234328833087
o  197 133.8152733526463
-  198 53.93579706543043
o  199 78.46291114229044
o  200 81.97217108677278
o  201 85.5512937608602
o  202 108.1723629331069
o  203 123.81149655029022
o  204 80.32044559427996
o  205 86.82384255454015
o  206 76.41619977366372
-  207 67.7237184770418
-  208 61.167078090212
-  209 63.195706309375964
o  210 123.45549142202371
o  211 96.4718819914995
o  212 71.77623824869507
o  213 89.10523274097687
o  214 100.44040867968488
o  215 118.17732941353353
o  216 84.06214390969936
-  217 61.6567975086311
o  218 105.74604921252585
o  219 106.81335786801999
o  220 78.49109469999094
-  221 61.10210839568153
-  222 52.2313693808169
o  223 ./_analy_anova_script.sh: line 4: 27898 Killed                  python analy_anova_extract.py $@
