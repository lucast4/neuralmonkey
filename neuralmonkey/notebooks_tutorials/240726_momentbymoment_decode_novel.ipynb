{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DEcoding shapes, in pool where there are novel and learned shapes\n",
    "Questions:\n",
    "- Decoding for learned shapes is better\n",
    "- Learned genearlize to themselves and novel to themselves more.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50813ff4",
   "metadata": {},
   "source": [
    "# Load a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf10d0",
   "metadata": {},
   "source": [
    "To load and plot a dataset of neural activity across population, in a PopAnal class object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ac978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6345314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "DFallpa = load_handsaved_wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c98009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "animal = \"Diego\"\n",
    "date = 240522\n",
    "DFallpa = load_handsaved_wrapper(animal=animal, date=date, version=\"trial\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = DFallpa[\"pa\"].values[0].Xlabels[\"trials\"]\n",
    "dflab[\"aborted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dccf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2 - Combine two dfallpa\n",
    "# DFallpa1 = load_handsaved_wrapper(animal=\"Diego\", date=230630, version=\"trial\")\n",
    "# DFallpa2 = load_handsaved_wrapper(animal=\"Diego\", date=230630, version=\"stroke\")\n",
    "# DFallpa = pd.concat([DFallpa1, DFallpa2]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2 - Combine two dfallpa\n",
    "# animal = \"Diego\"\n",
    "# date = 231211\n",
    "# COMBINE_AREAS = True\n",
    "\n",
    "# DFallpa1 = load_handsaved_wrapper(animal=animal, date=date, version=\"trial\", combine_areas=COMBINE_AREAS, use_time=False)\n",
    "# DFallpa2 = load_handsaved_wrapper(animal=animal, date=date, version=\"stroke\", combine_areas=COMBINE_AREAS, use_time=False)\n",
    "# DFallpa = pd.concat([DFallpa1, DFallpa2]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ab1b9",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f373551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ba91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prune to chans that are common across pa for each bregion (intersection of chans)|\n",
    "dfpa_match_chans_across_pa_each_bregion(DFallpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34978a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVEDIR = f\"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/{animal}-{date}\"\n",
    "SAVEDIR = f\"/tmp/NOVEL_PRIMS/{animal}-{date}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359b451",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e793d5",
   "metadata": {},
   "source": [
    "##### Devo -- removing noisy channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daefb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfpa_concatbregion_preprocess_clean_bad_channels\n",
    "dfpa_concatbregion_preprocess_clean_bad_channels(DFallpa, PLOT=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15877d2",
   "metadata": {},
   "source": [
    "# Sqrt normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pa in DFallpa[\"pa\"]:\n",
    "    pa.X = pa.X**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94d963",
   "metadata": {},
   "source": [
    "### FR Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54ffdc",
   "metadata": {},
   "source": [
    "##### Method 1 - each PA normalize independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_preprocess_fr_normalization\n",
    "# fr_normalization_method = \"each_time_bin\"\n",
    "fr_normalization_method = \"across_time_bins\"\n",
    "plot_savedir = \"/tmp\"\n",
    "dfallpa_preprocess_fr_normalization(DFallpa, fr_normalization_method, plot_savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282ffe4",
   "metadata": {},
   "source": [
    "##### Method 2 - Concat events (for each bregion) and normalize all same way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91505d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfpa_concat_normalize_fr_split_multbregion\n",
    "# fr_normalization_method = \"each_time_bin\"\n",
    "fr_normalization_method = \"across_time_bins\"\n",
    "dfpa_concat_normalize_fr_split_multbregion(DFallpa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad140e2f",
   "metadata": {},
   "source": [
    "##### Method 3 - concat events (flexible version, only constrianed to have same n chans across PA) [works if have both trial and stroke!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfpa_concat_normalize_fr_split_multbregion_flex\n",
    "fr_mean_subtract_method = \"across_time_bins\"\n",
    "# fr_mean_subtract_method = \"each_time_bin\"\n",
    "PLOT=False\n",
    "\n",
    "pa = DFallpa[\"pa\"].values[10]\n",
    "pa.plotNeurHeat(0)\n",
    "\n",
    "dfpa_concat_normalize_fr_split_multbregion_flex(DFallpa, fr_mean_subtract_method, PLOT)\n",
    "\n",
    "pa = DFallpa[\"pa\"].values[10]\n",
    "pa.plotNeurHeat(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c9cf9",
   "metadata": {},
   "source": [
    "# Extract relevant variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap\n",
    "grouping_plot_n_samples_conjunction_heatmap(dflab, \"seqc_0_shape\", \"shape_is_novel_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd76772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_pig_decode_moment_syntaxTI import dfallpa_preprocess_condition\n",
    "# shape_var_suff = \"shapesemgrp\"|\n",
    "# loc_var_suff = \"loc_on_clust\"\n",
    "shape_var_suff = \"shape\"\n",
    "loc_var_suff = \"loc\"\n",
    "dfallpa_preprocess_condition(DFallpa, shape_var_suff, loc_var_suff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08206a",
   "metadata": {},
   "source": [
    "# Extract beh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralmonkey.analyses.decode_moment import analy_psychoprim_prepare_beh_dataset\n",
    "# DSmorphsets, map_tc_to_morph_info, map_morphset_to_basemorphinfo, map_tcmorphset_to_idxmorph, map_tcmorphset_to_info, map_morphsetidx_to_assignedbase_or_ambig, map_tc_to_morph_status = analy_psychoprim_prepare_beh_dataset(animal, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fa5de",
   "metadata": {},
   "source": [
    "# Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078863e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import train_decoder_helper, pipeline_train_test_scalar_score, pipeline_train_test_scalar_score_with_splits, pipeline_train_test_scalar_score_mult_train_dataset, test_decoder_helper, train_decoder_helper_extract_train_dataset\n",
    "from neuralmonkey.scripts.analy_pig_decode_moment_syntaxTI import get_dataset_params\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of base prim names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAVEDIR_BASE = \"/tmp/NOVELPRIMS\"\n",
    "os.makedirs(SAVEDIR_BASE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_novelprims import analy_novelprim_prepare_dataset\n",
    "analy_novelprim_prepare_dataset(DFallpa, SAVEDIR_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefabc1",
   "metadata": {},
   "source": [
    "# One decoder for each shape - visualize generalization patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over all train-test split\n",
    "if False: # Doesnt work well, takes too long.\n",
    "    cols_ignore = [\"pa_idx\", \"decoder_idx\", \"train_split_idx\"]\n",
    "    cols_float = [col for col in dfscores.columns if dfscores[col].dtype == float]\n",
    "    # cols_cat = [col for col in dfscores.columns if dfscores[col].dtype != float]\n",
    "    # cols_cat = [col for col in cols_cat if col not in cols_ignore]\n",
    "    cols_cat = ['decoder_class', 'pa_class', 'twind', 'epoch', 'trialcode']\n",
    "    from pythonlib.tools.pandastools import aggregGeneral\n",
    "    dfscores = aggregGeneral(dfscores, cols_cat, cols_float, nonnumercols=\"all\")\n",
    "    # Average over all train-test split\n",
    "    [col for col in dfscores if isinstance(dfscores[col], float)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ab301",
   "metadata": {},
   "source": [
    "### Method 1 -- not doing split by gridloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb66074",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_trials = False\n",
    "if downsample_trials:\n",
    "    # So that the lowest N doesnt pull all other categories down.\n",
    "    n_min_per_var = 10\n",
    "else:\n",
    "    n_min_per_var = 7\n",
    "        \n",
    "TWIND_TEST = (0.05, 1.2)\n",
    "\n",
    "do_upsample_balance=True\n",
    "PLOT_DECODER = False\n",
    "DO_TRAIN_TEST_SPLIT=True\n",
    "PLOT_TEST_CONCATTED = True\n",
    "\n",
    "TWIND_TRAIN = (0.05, 1.2)\n",
    "\n",
    "# Subtrract baseline?\n",
    "subtract_baseline=False\n",
    "subtract_baseline_twind=None\n",
    "include_null_data = False\n",
    "prune_labels_exist_in_train_and_test = True\n",
    "\n",
    "# - Train params\n",
    "event_train = \"03_samp\"\n",
    "twind_train = TWIND_TRAIN\n",
    "var_train = \"seqc_0_shape_pref\"\n",
    "filterdict_train = None\n",
    "\n",
    "# - Test params\n",
    "var_test = \"seqc_0_shape_pref\"\n",
    "event_test = \"03_samp\"\n",
    "which_level_test = \"trial\"\n",
    "filterdict_test = None\n",
    "# list_twind_test = [(-0.8, -0.05), TWIND_TEST]\n",
    "list_twind_test = [TWIND_TEST]\n",
    "\n",
    "# Train/test splits\n",
    "do_train_splits_nsplits=10\n",
    "score_user_test_data = False\n",
    "PLOT_TEST_SPLIT = False\n",
    "\n",
    "# Other params\n",
    "SAVEDIR = f\"{SAVEDIR_BASE}/downsample_trials={downsample_trials}-TWIND_TEST={TWIND_TEST}-dotraintestsplit={DO_TRAIN_TEST_SPLIT}\"\n",
    "\n",
    "list_bregion = DFallpa[\"bregion\"].unique().tolist()\n",
    "for bregion in list_bregion:\n",
    "    savedir = f\"{SAVEDIR}/{bregion}/decoder_training\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    print(savedir)\n",
    "\n",
    "    if not DO_TRAIN_TEST_SPLIT:\n",
    "        dfscores, Dc, PAtrain, PAtest = pipeline_train_test_scalar_score(DFallpa, bregion, var_train, event_train, \n",
    "                                                                        twind_train, filterdict_train,\n",
    "                                            var_test, event_test, list_twind_test, filterdict_test, savedir,\n",
    "                                            include_null_data=include_null_data, decoder_method_index=None,\n",
    "                                            prune_labels_exist_in_train_and_test=prune_labels_exist_in_train_and_test, PLOT=PLOT_DECODER,\n",
    "                                            which_level_test=which_level_test, n_min_per_var=n_min_per_var,\n",
    "                                            subtract_baseline=subtract_baseline, subtract_baseline_twind=subtract_baseline_twind,\n",
    "                                            do_upsample_balance=do_upsample_balance, downsample_trials=downsample_trials)\n",
    "    else:\n",
    "        dfscores, dfscores_usertest, dfscores_both, decoders, trainsets, PAtest = pipeline_train_test_scalar_score_with_splits(DFallpa, \n",
    "                                                                        bregion, var_train, event_train, \n",
    "                                                                        twind_train, filterdict_train,\n",
    "                                            var_test, event_test, list_twind_test, filterdict_test, savedir,\n",
    "                                            include_null_data=include_null_data, decoder_method_index=None,\n",
    "                                            prune_labels_exist_in_train_and_test=prune_labels_exist_in_train_and_test, \n",
    "                                            PLOT_TRAIN=PLOT_DECODER, PLOT_TEST_SPLIT=PLOT_TEST_SPLIT, PLOT_TEST_CONCATTED=PLOT_TEST_CONCATTED,\n",
    "                                            which_level_test=which_level_test, n_min_per_var=n_min_per_var,\n",
    "                                            subtract_baseline=subtract_baseline, subtract_baseline_twind=subtract_baseline_twind,\n",
    "                                            do_upsample_balance=do_upsample_balance, downsample_trials=downsample_trials,\n",
    "                                            do_train_splits_nsplits=do_train_splits_nsplits, \n",
    "                                            score_user_test_data=score_user_test_data)\n",
    "\n",
    "    ######################## PLOTS\n",
    "    from neuralmonkey.analyses.decode_moment import analy_psychoprim_score_postsamp\n",
    "    savedir = f\"{SAVEDIR}/{bregion}/PLOTS\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    print(savedir)\n",
    "    analy_novelprim_score_postsamp(dfscores, decoders, dflab, savedir)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf3071",
   "metadata": {},
   "source": [
    "### Method 2 - splitting by gridloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import pipeline_train_test_scalar_score_split_gridloc\n",
    "bregion = \"PMv\"\n",
    "dflab = pa.Xlabels[\"trials\"]\n",
    "list_loc = dflab[\"seqc_0_loc\"].unique().tolist()\n",
    "auto_prune_locations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab[\"aborted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "filterdict_train = None\n",
    "filterdict_test = None\n",
    "\n",
    "\n",
    "### OTher params.\n",
    "TWIND_TEST = (0.05, 1.2)\n",
    "TWIND_TRAIN = (0.05, 1.2)\n",
    "\n",
    "do_upsample_balance=True\n",
    "PLOT_DECODER = False\n",
    "\n",
    "# Subtrract baseline?\n",
    "subtract_baseline=False\n",
    "subtract_baseline_twind=None\n",
    "include_null_data = False\n",
    "prune_labels_exist_in_train_and_test = True\n",
    "\n",
    "# - Train params\n",
    "event_train = \"03_samp\"\n",
    "twind_train = TWIND_TRAIN\n",
    "var_train = \"seqc_0_shape_pref\"\n",
    "\n",
    "# - Test params\n",
    "var_test = \"seqc_0_shape_pref\"\n",
    "event_test = \"03_samp\"\n",
    "which_level_test = \"trial\"\n",
    "# list_twind_test = [(-0.8, -0.05), TWIND_TEST]\n",
    "list_twind_test = [TWIND_TEST]\n",
    "\n",
    "# Other params\n",
    "SAVEDIR = f\"{SAVEDIR_BASE}/downsample_trials={downsample_trials}-TWIND_TEST={TWIND_TEST}-split_by_gridloc\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_bregion = DFallpa[\"bregion\"].unique().tolist()\n",
    "for bregion in list_bregion:\n",
    "    savedir_base = f\"{SAVEDIR}/{bregion}/decoder_training\"\n",
    "    os.makedirs(savedir_base, exist_ok=True)\n",
    "    print(savedir_base)\n",
    "\n",
    "    DFSCORES, decoders, list_pa_train, list_pa_test = pipeline_train_test_scalar_score_split_gridloc(list_loc, savedir_base,\n",
    "                                                                                                     DFallpa, \n",
    "                                                                    bregion, var_train, event_train, \n",
    "                                                                    twind_train, filterdict_train,\n",
    "                                        var_test, event_test, list_twind_test, filterdict_test, \n",
    "                                        include_null_data=include_null_data, \n",
    "                                        prune_labels_exist_in_train_and_test=prune_labels_exist_in_train_and_test, \n",
    "                                        PLOT=PLOT_DECODER,\n",
    "                                        which_level_test=which_level_test, n_min_per_var=n_min_per_var,\n",
    "                                        subtract_baseline=subtract_baseline, subtract_baseline_twind=subtract_baseline_twind,\n",
    "                                        do_upsample_balance=do_upsample_balance, downsample_trials=downsample_trials,\n",
    "                                        auto_prune_locations=auto_prune_locations)\n",
    "\n",
    "    # ######################## PLOTS\n",
    "    # from neuralmonkey.analyses.decode_moment import analy_psychoprim_score_postsamp\n",
    "    # savedir = f\"{SAVEDIR}/{bregion}/PLOTS\"\n",
    "    # os.makedirs(savedir, exist_ok=True)\n",
    "    # print(savedir)\n",
    "    # analy_novelprim_score_postsamp(dfscores, decoders, dflab, savedir)                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0acf6d",
   "metadata": {},
   "source": [
    "# MULT BREGIONS - load and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER, _REGIONS_IN_ORDER_COMBINED\n",
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap, stringify_values, aggregGeneral\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import prune_dfscores_good_morphset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_BASE = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combine = True\n",
    "downsample_trials = False\n",
    "# TWIND_TEST = (0.05, 1.2)\n",
    "version = 1\n",
    "for animal, date in [(\"Pancho\", 240524), (\"Diego\", 240523)]:\n",
    "    for TWIND_TEST in [(0.05, 1.2), (0.6, 1.2)]:\n",
    "\n",
    "        if combine:\n",
    "            list_bregion = _REGIONS_IN_ORDER_COMBINED\n",
    "        else:\n",
    "            list_bregion = _REGIONS_IN_ORDER\n",
    "\n",
    "        list_dfscores = []\n",
    "        for bregion in list_bregion:\n",
    "            path = f\"{SAVEDIR_BASE}/{animal}-{date}-logistic-combine={combine}/downsample_trials={downsample_trials}-TWIND_TEST={TWIND_TEST}-version={version}/{bregion}/DFSCORES.pkl\"\n",
    "            print(\"Loading ... \", path)\n",
    "            dfscores = pd.read_pickle(path)\n",
    "            dfscores[\"bregion\"] = bregion\n",
    "            list_dfscores.append(dfscores)\n",
    "        DFSCORES = pd.concat(list_dfscores).reset_index(drop=True)\n",
    "\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE}/{animal}-{date}-logistic-combine={combine}/downsample_trials={downsample_trials}-TWIND_TEST={TWIND_TEST}-version={version}/MULT_BREGION\"\n",
    "        print(\"SAving at: \", SAVEDIR)\n",
    "\n",
    "\n",
    "\n",
    "        # ### PReprocessing\n",
    "        # assert len(DFSCORES[\"twind\"].unique())==1, \"code below assumes\"\n",
    "\n",
    "        # # Get score relative to chance (50%)\n",
    "        # score_chance = 0.5\n",
    "        # DFSCORES[\"score_adjusted\"] = (DFSCORES[\"score\"] - score_chance)/(1. - score_chance)\n",
    "\n",
    "        # ### Plots\n",
    "        # for do_agg_over_trials in [False, True]:\n",
    "        #     for morphset_get in [None, \"good_ones\"]:\n",
    "\n",
    "        #         dfscores = prune_dfscores_good_morphset(DFSCORES, morphset_get, animal, date)\n",
    "\n",
    "        #         if do_agg_over_trials:\n",
    "        #             dfscores = aggregGeneral(dfscores, \n",
    "        #                                     [\"bregion\", \"morph_set_idx|idx_within\", \"idx_within|assigned\", \"pa_class\", \"decoder_class\", \"twind\"], \n",
    "        #                                     [\"score\", \"score_adjusted\"], \n",
    "        #                                     nonnumercols=[\"trial_morph_assigned_to_which_base\"])\n",
    "        #         # (1) \n",
    "        #         huevar = \"decoder_class_semantic_good\"\n",
    "        #         xvar = \"trial_morph_assigned_to_which_base\"\n",
    "        #         xvalues = [\"base1\", \"not_ambig_base1\", \"ambig_base1\", \"ambig_base2\", \"not_ambig_base2\", \"base2\"]\n",
    "        #         huevalues = [\"base1\", \"base2\"]\n",
    "\n",
    "        #         savedir = f\"{SAVEDIR}/do_agg_over_trials={do_agg_over_trials}-morphset_get={morphset_get}/xvar={xvar}\"\n",
    "        #         os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        #         _plot(dfscores, huevar, xvar, xvalues, huevalues, savedir)\n",
    "\n",
    "        #         # (2)\n",
    "        #         huevar = \"recoded_decoder\"\n",
    "        #         xvar = \"recoded_trial_morph\"\n",
    "        #         xvalues = [\"ambig\", \"not_ambig\", \"base\"]\n",
    "        #         huevalues = [\"baseother\", \"basethis\"]\n",
    "\n",
    "        #         savedir = f\"{SAVEDIR}/do_agg_over_trials={do_agg_over_trials}-morphset_get={morphset_get}/xvar={xvar}\"\n",
    "        #         os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        #         _plot(dfscores, huevar, xvar, xvalues, huevalues, savedir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
