{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7125966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConsolidates everything from all prior tutorials, except \"euclidian stuff\" (for that see 240410_kedar_euclidia...)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Consolidates everything from all prior tutorials, except \"euclidian stuff\" (for that see 240410_kedar_euclidia...)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b248d33aff307a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d20c8",
   "metadata": {},
   "source": [
    "# Load DFallPa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14525eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Method: loading functrion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5608555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010699ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DFallpa from:  /lemur2/lucas/Dropbox/SCIENCE/FREIWALD_LAB/DATA/Xuan/DFallpa-Diego-231220-stroke-kilosort_if_exists-norm=None-combine=True-t1=-1.0-t2=1.8-quest=CHAR_BASE_stroke.pkl\n",
      "TODO: Pancho -- combine circles with the tohers\n"
     ]
    }
   ],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "# combine = False\n",
    "# question = \"CHAR_BASE_stroke\"\n",
    "# version = \"stroke\"\n",
    "\n",
    "animal = \"Diego\"\n",
    "date = 231220\n",
    "combine = True\n",
    "question = \"CHAR_BASE_stroke\"\n",
    "version = \"stroke\"\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "# combine = True\n",
    "# question = \"CHAR_BASE_trial\"\n",
    "# version = \"trial\"\n",
    "\n",
    "# animal = \"Diego\"\n",
    "# date = 231205\n",
    "# combine = True\n",
    "# question = \"CHAR_BASE_trial\"\n",
    "# version = \"trial\"\n",
    "\n",
    "DFallpa = load_handsaved_wrapper(animal, date, version=version, combine_areas=combine, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a29f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == (1) Matching chans across events\n",
      "M1  ...  59\n",
      "M1  -- n chans final:  59\n",
      "PMv  ...  64\n",
      "PMv  -- n chans final:  64\n",
      "PMd  ...  47\n",
      "PMd  -- n chans final:  47\n",
      "dlPFC  ...  29\n",
      "dlPFC  -- n chans final:  29\n",
      "vlPFC  ...  56\n",
      "vlPFC  -- n chans final:  56\n",
      "FP  ...  59\n",
      "FP  -- n chans final:  59\n",
      "SMA  ...  59\n",
      "SMA  -- n chans final:  59\n",
      "preSMA  ...  55\n",
      "preSMA  -- n chans final:  55\n",
      " == (2) Remove bad chans based on drift\n",
      "============== REMOVING DIRTY SITES:\n",
      "... bregion  M1 ... event  00_stroke\n",
      "chan 3 Replacing these trials with mean good trial: [644, 645]\n",
      "chan 6 Replacing these trials with mean good trial: [580]\n",
      "chan 9 Replacing these trials with mean good trial: [3, 48, 49]\n",
      "chan 14 Replacing these trials with mean good trial: [920, 921]\n",
      "chan 15 Replacing these trials with mean good trial: [205, 1000, 1006]\n",
      "chan 16 Replacing these trials with mean good trial: [16, 17, 161, 179, 180]\n",
      "chan 18 Replacing these trials with mean good trial: [360]\n",
      "chan 21 Replacing these trials with mean good trial: [988, 989, 990, 991, 992, 993, 994, 995]\n",
      "chan 24 Replacing these trials with mean good trial: [623, 624, 631, 648, 649, 1023, 1024]\n",
      "chan 28 Replacing these trials with mean good trial: [215, 216, 217, 218]\n",
      "chan 30 Replacing these trials with mean good trial: [988, 989, 990, 991]\n",
      "chan 36 Replacing these trials with mean good trial: [237]\n",
      "chan 38 Replacing these trials with mean good trial: [413, 414, 440, 441, 623, 624, 629, 630]\n",
      "chan 40 Replacing these trials with mean good trial: [219, 220]\n",
      "chan 44 Replacing these trials with mean good trial: [18, 19, 494]\n",
      "chan 45 Replacing these trials with mean good trial: [859, 860]\n",
      "chan 54 Replacing these trials with mean good trial: [15]\n",
      "... bregion  PMv ... event  00_stroke\n",
      "Removing these bad chans: [69, 96]\n",
      "Chans exist in PA: [65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]\n",
      "chan 65 Replacing these trials with mean good trial: [625, 626, 627, 628]\n",
      "chan 66 Replacing these trials with mean good trial: [492]\n",
      "chan 68 Replacing these trials with mean good trial: [1001, 1003, 1006]\n",
      "chan 70 Replacing these trials with mean good trial: [160, 521, 522, 523, 524]\n",
      "chan 72 Replacing these trials with mean good trial: [1006]\n",
      "chan 73 Replacing these trials with mean good trial: [1000, 1032]\n",
      "chan 77 Replacing these trials with mean good trial: [244]\n",
      "chan 78 Replacing these trials with mean good trial: [166]\n",
      "chan 79 Replacing these trials with mean good trial: [922, 923]\n",
      "chan 81 Replacing these trials with mean good trial: [166]\n",
      "chan 83 Replacing these trials with mean good trial: [896]\n",
      "chan 85 Replacing these trials with mean good trial: [262]\n",
      "chan 87 Replacing these trials with mean good trial: [1000]\n",
      "chan 95 Replacing these trials with mean good trial: [1, 2, 659, 880, 881, 882, 883, 914, 915]\n",
      "chan 98 Replacing these trials with mean good trial: [262]\n",
      "chan 103 Replacing these trials with mean good trial: [903]\n",
      "chan 104 Replacing these trials with mean good trial: [30]\n",
      "chan 107 Replacing these trials with mean good trial: [803, 804]\n",
      "chan 112 Replacing these trials with mean good trial: [332]\n",
      "chan 113 Replacing these trials with mean good trial: [360]\n",
      "chan 118 Replacing these trials with mean good trial: [1003, 1032]\n",
      "chan 119 Replacing these trials with mean good trial: [1006, 1023, 1024]\n",
      "chan 121 Replacing these trials with mean good trial: [380, 381, 419, 458, 459, 460, 461, 816, 817]\n",
      "chan 125 Replacing these trials with mean good trial: [1019, 1020]\n",
      "chan 126 Replacing these trials with mean good trial: [41, 42, 46, 47, 48, 49, 50, 51, 53, 54, 57, 58, 59, 60, 61, 62, 63, 64, 65]\n",
      "chan 127 Replacing these trials with mean good trial: [948, 949]\n",
      "chan 128 Replacing these trials with mean good trial: [605, 606, 607, 608]\n",
      "... bregion  PMd ... event  00_stroke\n",
      "Removing these bad chans: [137]\n",
      "Chans exist in PA: [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 144, 146, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 176, 177, 178, 182, 184, 185, 186, 188, 189, 191, 192]\n",
      "chan 133 Replacing these trials with mean good trial: [149, 1017, 1018]\n",
      "chan 144 Replacing these trials with mean good trial: [597, 598, 867, 868]\n",
      "chan 154 Replacing these trials with mean good trial: [15]\n",
      "chan 167 Replacing these trials with mean good trial: [896]\n",
      "chan 169 Replacing these trials with mean good trial: [438]\n",
      "chan 177 Replacing these trials with mean good trial: [29]\n",
      "chan 184 Replacing these trials with mean good trial: [166, 298, 299]\n",
      "chan 188 Replacing these trials with mean good trial: [503, 504]\n",
      "chan 191 Replacing these trials with mean good trial: [618]\n",
      "chan 192 Replacing these trials with mean good trial: [820, 821, 905, 906, 974, 975, 1019, 1020]\n",
      "... bregion  dlPFC ... event  00_stroke\n",
      "chan 225 Replacing these trials with mean good trial: [731, 732]\n",
      "chan 228 Replacing these trials with mean good trial: [0, 230, 231, 306, 307, 486, 487, 488, 489, 499, 500, 501, 502, 564, 565]\n",
      "chan 230 Replacing these trials with mean good trial: [625, 626, 627, 628, 741, 742, 824, 825, 863, 864]\n",
      "chan 233 Replacing these trials with mean good trial: [232]\n",
      "chan 236 Replacing these trials with mean good trial: [191]\n",
      "chan 238 Replacing these trials with mean good trial: [809, 892, 893]\n",
      "chan 244 Replacing these trials with mean good trial: [812, 813]\n",
      "chan 245 Replacing these trials with mean good trial: [0]\n",
      "chan 248 Replacing these trials with mean good trial: [83, 196, 197, 246, 247, 248, 249, 368, 691]\n",
      "chan 250 Replacing these trials with mean good trial: [46, 47, 527, 528]\n",
      "chan 251 Replacing these trials with mean good trial: [203, 204]\n",
      "chan 252 Replacing these trials with mean good trial: [129]\n",
      "chan 254 Replacing these trials with mean good trial: [569, 954, 955]\n",
      "chan 256 Replacing these trials with mean good trial: [1032]\n",
      "... bregion  vlPFC ... event  00_stroke\n",
      "Removing these bad chans: [274]\n",
      "Chans exist in PA: [257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 303, 305, 307, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320]\n",
      "chan 259 Replacing these trials with mean good trial: [618]\n",
      "chan 263 Replacing these trials with mean good trial: [360]\n",
      "chan 267 Replacing these trials with mean good trial: [15, 863, 864]\n",
      "chan 270 Replacing these trials with mean good trial: [934, 935]\n",
      "chan 272 Replacing these trials with mean good trial: [352, 353]\n",
      "chan 278 Replacing these trials with mean good trial: [191, 702]\n",
      "chan 281 Replacing these trials with mean good trial: [360]\n",
      "chan 282 Replacing these trials with mean good trial: [21, 1025, 1026, 1027, 1028]\n",
      "chan 283 Replacing these trials with mean good trial: [846, 847]\n",
      "chan 284 Replacing these trials with mean good trial: [904]\n",
      "chan 286 Replacing these trials with mean good trial: [360]\n",
      "chan 287 Replacing these trials with mean good trial: [360]\n",
      "chan 289 Replacing these trials with mean good trial: [360, 970, 971]\n",
      "chan 290 Replacing these trials with mean good trial: [360, 874, 875]\n",
      "chan 292 Replacing these trials with mean good trial: [119]\n",
      "chan 293 Replacing these trials with mean good trial: [597, 598]\n",
      "chan 295 Replacing these trials with mean good trial: [672, 673, 676, 677, 714, 715, 930, 931, 932, 933, 934, 935, 938, 939, 942, 943, 1002]\n",
      "chan 296 Replacing these trials with mean good trial: [934, 935]\n",
      "chan 298 Replacing these trials with mean good trial: [360]\n",
      "chan 300 Replacing these trials with mean good trial: [276, 277, 865, 866, 867, 868]\n",
      "chan 310 Replacing these trials with mean good trial: [785, 786]\n",
      "chan 313 Replacing these trials with mean good trial: [411, 412]\n",
      "chan 314 Replacing these trials with mean good trial: [143, 144, 253, 254, 692, 693, 814, 815]\n",
      "chan 315 Replacing these trials with mean good trial: [360]\n",
      "chan 316 Replacing these trials with mean good trial: [360]\n",
      "chan 317 Replacing these trials with mean good trial: [954, 955]\n",
      "chan 318 Replacing these trials with mean good trial: [360]\n",
      "chan 319 Replacing these trials with mean good trial: [494]\n",
      "chan 320 Replacing these trials with mean good trial: [196, 197, 360]\n",
      "... bregion  FP ... event  00_stroke\n",
      "Removing these bad chans: [330]\n",
      "Chans exist in PA: [321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 334, 335, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384]\n",
      "chan 321 Replacing these trials with mean good trial: [165, 810, 811]\n",
      "chan 322 Replacing these trials with mean good trial: [1019, 1020]\n",
      "chan 327 Replacing these trials with mean good trial: [619, 620, 721, 722, 754, 755]\n",
      "chan 329 Replacing these trials with mean good trial: [863, 864]\n",
      "chan 331 Replacing these trials with mean good trial: [369, 370]\n",
      "chan 335 Replacing these trials with mean good trial: [869]\n",
      "chan 342 Replacing these trials with mean good trial: [407, 408, 409, 410]\n",
      "chan 344 Replacing these trials with mean good trial: [533, 534, 601, 602, 876, 877, 954, 955]\n",
      "chan 347 Replacing these trials with mean good trial: [1015, 1016]\n",
      "chan 350 Replacing these trials with mean good trial: [108, 109, 1030, 1031]\n",
      "chan 353 Replacing these trials with mean good trial: [85, 86, 87, 88, 392, 393, 559]\n",
      "chan 355 Replacing these trials with mean good trial: [407, 408, 409, 410]\n",
      "chan 357 Replacing these trials with mean good trial: [1, 2, 18, 19, 21, 114, 115, 117, 118, 612, 613, 614, 615]\n",
      "chan 358 Replacing these trials with mean good trial: [585, 586, 799, 800]\n",
      "chan 360 Replacing these trials with mean good trial: [328, 329, 330, 331, 495, 496, 749, 750]\n",
      "chan 364 Replacing these trials with mean good trial: [596, 936, 937]\n",
      "chan 367 Replacing these trials with mean good trial: [354, 355]\n",
      "chan 368 Replacing these trials with mean good trial: [333, 334, 337, 338]\n",
      "chan 370 Replacing these trials with mean good trial: [438]\n",
      "chan 371 Replacing these trials with mean good trial: [165]\n",
      "chan 372 Replacing these trials with mean good trial: [559, 956, 957]\n",
      "chan 376 Replacing these trials with mean good trial: [251, 252, 623, 624]\n",
      "chan 378 Replacing these trials with mean good trial: [212, 213]\n",
      "chan 379 Replacing these trials with mean good trial: [956, 957]\n",
      "chan 380 Replacing these trials with mean good trial: [747, 748, 791, 792, 793, 794]\n",
      "chan 381 Replacing these trials with mean good trial: [580, 996, 997]\n",
      "chan 384 Replacing these trials with mean good trial: [337, 338, 548]\n",
      "... bregion  SMA ... event  00_stroke\n",
      "chan 389 Replacing these trials with mean good trial: [942, 943]\n",
      "chan 390 Replacing these trials with mean good trial: [58, 59]\n",
      "chan 391 Replacing these trials with mean good trial: [50, 51, 70]\n",
      "chan 392 Replacing these trials with mean good trial: [439, 970, 971]\n",
      "chan 393 Replacing these trials with mean good trial: [849]\n",
      "chan 394 Replacing these trials with mean good trial: [903, 905, 906]\n",
      "chan 397 Replacing these trials with mean good trial: [183]\n",
      "chan 400 Replacing these trials with mean good trial: [30]\n",
      "chan 401 Replacing these trials with mean good trial: [171, 172, 173, 174, 360]\n",
      "chan 403 Replacing these trials with mean good trial: [483]\n",
      "chan 410 Replacing these trials with mean good trial: [196, 197, 440, 441, 581, 582, 676, 677]\n",
      "chan 417 Replacing these trials with mean good trial: [896]\n",
      "chan 423 Replacing these trials with mean good trial: [72, 73, 120, 121, 436, 437]\n",
      "chan 428 Replacing these trials with mean good trial: [930, 931]\n",
      "chan 430 Replacing these trials with mean good trial: [849]\n",
      "chan 432 Replacing these trials with mean good trial: [998, 999, 1023, 1024, 1032]\n",
      "chan 441 Replacing these trials with mean good trial: [980, 1006]\n",
      "chan 442 Replacing these trials with mean good trial: [659, 682, 683]\n",
      "chan 446 Replacing these trials with mean good trial: [475, 476, 978]\n",
      "... bregion  preSMA ... event  00_stroke\n",
      "Removing these bad chans: [463, 467]\n",
      "Chans exist in PA: [449, 450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 471, 472, 474, 476, 477, 480, 481, 482, 483, 484, 485, 486, 487, 488, 490, 491, 492, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512]\n",
      "chan 460 Replacing these trials with mean good trial: [929]\n",
      "chan 461 Replacing these trials with mean good trial: [812, 813]\n",
      "chan 469 Replacing these trials with mean good trial: [448, 449]\n",
      "chan 471 Replacing these trials with mean good trial: [192, 193, 245, 386, 387, 388, 389, 447]\n",
      "chan 477 Replacing these trials with mean good trial: [894, 895, 948, 949]\n",
      "chan 481 Replacing these trials with mean good trial: [904, 911]\n",
      "chan 483 Replacing these trials with mean good trial: [200, 201, 618, 1000, 1032]\n",
      "chan 486 Replacing these trials with mean good trial: [483]\n",
      "chan 491 Replacing these trials with mean good trial: [52]\n",
      "chan 492 Replacing these trials with mean good trial: [122, 123, 964, 965]\n",
      "chan 496 Replacing these trials with mean good trial: [911]\n",
      "chan 499 Replacing these trials with mean good trial: [244, 809]\n",
      "chan 500 Replacing these trials with mean good trial: [530, 869]\n",
      "chan 501 Replacing these trials with mean good trial: [676, 677, 765, 766]\n",
      "chan 503 Replacing these trials with mean good trial: [618]\n",
      "chan 506 Replacing these trials with mean good trial: [23, 24, 656, 657, 721, 722]\n",
      "chan 512 Replacing these trials with mean good trial: [71, 312, 313, 464, 465]\n",
      "PA.X.shape, before and after dfallpa_preprocess_sitesdirty_single\n",
      "('M1', '00_stroke', (-1.0, 1.8)) (59, 1033, 280)  -->  (59, 1033, 280)\n",
      "('PMv', '00_stroke', (-1.0, 1.8)) (64, 1033, 280)  -->  (62, 1033, 280)\n",
      "('PMd', '00_stroke', (-1.0, 1.8)) (47, 1033, 280)  -->  (46, 1033, 280)\n",
      "('dlPFC', '00_stroke', (-1.0, 1.8)) (29, 1033, 280)  -->  (29, 1033, 280)\n",
      "('vlPFC', '00_stroke', (-1.0, 1.8)) (56, 1033, 280)  -->  (55, 1033, 280)\n",
      "('FP', '00_stroke', (-1.0, 1.8)) (59, 1033, 280)  -->  (58, 1033, 280)\n",
      "('SMA', '00_stroke', (-1.0, 1.8)) (59, 1033, 280)  -->  (59, 1033, 280)\n",
      "('preSMA', '00_stroke', (-1.0, 1.8)) (55, 1033, 280)  -->  (53, 1033, 280)\n",
      " == (3) Remove bad chans, low FR, low signal\n",
      "(59, 1033, 280)\n",
      "Keep, for  M1  ... 57 / 59\n",
      " ... keeping:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 63]\n",
      " ... excluding:  [40, 47]\n",
      "(62, 1033, 280)\n",
      "Keep, for  PMv  ... 60 / 62\n",
      " ... keeping:  [65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128]\n",
      " ... excluding:  [100, 122]\n",
      "(46, 1033, 280)\n",
      "Keep, for  PMd  ... 43 / 46\n",
      " ... keeping:  [129, 130, 131, 132, 133, 134, 135, 136, 139, 140, 144, 146, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 176, 177, 178, 182, 185, 186, 189, 191, 192]\n",
      " ... excluding:  [138, 184, 188]\n",
      "(29, 1033, 280)\n",
      "Keep, for  dlPFC  ... 23 / 29\n",
      " ... keeping:  [225, 226, 228, 229, 231, 232, 233, 234, 235, 237, 238, 239, 242, 243, 245, 246, 248, 251, 252, 253, 254, 255, 256]\n",
      " ... excluding:  [227, 230, 236, 240, 244, 250]\n",
      "(55, 1033, 280)\n",
      "Keep, for  vlPFC  ... 47 / 55\n",
      " ... keeping:  [257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 273, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 289, 292, 293, 294, 295, 296, 297, 299, 300, 303, 307, 309, 310, 312, 313, 314, 315, 316, 317, 319]\n",
      " ... excluding:  [261, 272, 290, 298, 305, 311, 318, 320]\n",
      "(58, 1033, 280)\n",
      "Keep, for  FP  ... 49 / 58\n",
      " ... keeping:  [321, 322, 324, 325, 326, 327, 328, 329, 331, 332, 335, 342, 343, 344, 345, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 366, 368, 369, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 382, 383, 384]\n",
      " ... excluding:  [323, 334, 339, 340, 341, 365, 367, 370, 381]\n",
      "(59, 1033, 280)\n",
      "Keep, for  SMA  ... 58 / 59\n",
      " ... keeping:  [385, 386, 387, 388, 389, 390, 391, 393, 394, 395, 396, 397, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 432, 433, 435, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448]\n",
      " ... excluding:  [392]\n",
      "(53, 1033, 280)\n",
      "Keep, for  preSMA  ... 47 / 53\n",
      " ... keeping:  [449, 450, 451, 452, 453, 454, 456, 458, 460, 461, 464, 465, 466, 468, 469, 471, 472, 474, 476, 477, 480, 481, 482, 483, 484, 485, 487, 488, 490, 491, 492, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 509, 510, 512]\n",
      " ... excluding:  [455, 459, 486, 494, 508, 511]\n",
      " == (4) Sqrt transform\n",
      " == (5) Normalize FR\n",
      "Running ..  M1\n",
      "Running ..  PMv\n",
      "Running ..  PMd\n",
      "Running ..  dlPFC\n",
      "Running ..  vlPFC\n",
      "Running ..  FP\n",
      "Running ..  SMA\n",
      "Running ..  preSMA\n"
     ]
    }
   ],
   "source": [
    "from neuralmonkey.metadat.analy.anova_params import params_getter_euclidian_vars\n",
    "from neuralmonkey.classes.population_mult import dfpa_concatbregion_preprocess_clean_bad_channels, dfpa_concatbregion_preprocess_wrapper\n",
    "\n",
    "# LIST_VAR, LIST_VARS_OTHERS, LIST_CONTEXT, LIST_PRUNE_MIN_N_LEVS, LIST_FILTDICT = params_getter_euclidian_vars(question)\n",
    "\n",
    "# Make a copy of all PA before normalization\n",
    "plot_clean_lowfr_chans=False\n",
    "dfpa_concatbregion_preprocess_wrapper(DFallpa, animal, date, plot_clean_lowfr_chans=plot_clean_lowfr_chans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947842bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_preprocess_sort_by_trialcode\n",
    "\n",
    "dfallpa_preprocess_sort_by_trialcode(DFallpa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]\n",
    "dflab['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040397ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "\n",
    "from neuralmonkey.classes.population_mult import dfallpa_preprocess_sitesdirty_single_just_drift\n",
    "PA = DFallpa[\"pa\"].values[0]\n",
    "dfallpa_preprocess_sitesdirty_single_just_drift(PA, animal, date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "import os\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "\n",
    "savedir = \"/tmp/PREPROCESS\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]\n",
    "dflab[\"supervision_stage_concise\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72115c7e",
   "metadata": {},
   "source": [
    "### Compare beh strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA.behavior_extract_strokes_to_dflab(trial_take_first_stroke=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac20dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "grpdict = grouping_append_and_return_inner_items_good(dflab, [shape_var, \"task_kind\"])\n",
    "\n",
    "list_shape = dflab[shape_var].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c304c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two set of strokes, score how similar they are.\n",
    "tk1 = \"prims_single\"\n",
    "tk2 = \"character\"\n",
    "for shape in list_shape:\n",
    "    key1 = (shape, tk1)\n",
    "    key2 = (shape, tk2)\n",
    "    if key1 not in grpdict.keys():\n",
    "        print(\"Key not found: \", key1)\n",
    "        continue\n",
    "    if key2 not in grpdict.keys():\n",
    "        print(\"Key not found: \", key2)\n",
    "        continue\n",
    "    \n",
    "    strokes1 = dflab.iloc[grpdict[key1]][\"strok_beh\"].tolist()\n",
    "    strokes2 = dflab.iloc[grpdict[key2]][\"strok_beh\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b27094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "ds = DatStrokes()\n",
    "Cl = ds.distgood_compute_beh_beh_strok_distances(strokes1, strokes2, label_var=shape_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc668aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88237655",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = DFallpa[\"pa\"].values[5]\n",
    "preprocess_pa(animal, date, pa, \"/tmp\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4510d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "behstrokes_extract_char_clust_sim(PA, animal, date, savedir=None, PLOT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "behstrokes_extract_char_clust_sim(PA, animal, date, \"/tmp\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9daa1a",
   "metadata": {},
   "source": [
    "# [0] Do state space and euclidian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84d7e0",
   "metadata": {},
   "source": [
    "##### First, prune PA to just good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "import seaborn as sns\n",
    "from pythonlib.tools.plottools import savefig\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/CHAR_SP_FINAL\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06978a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune to just the DFallpa for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PARAMS\n",
    "n_min_trials_per_shape = 5\n",
    "LIST_NPCS_KEEP = [4,6,2]\n",
    "PLOT_EACH_REGION = True\n",
    "\n",
    "### State space plots\n",
    "LIST_VAR = [\n",
    "    \"shape_semantic\",\n",
    "    \"shape_semantic\",\n",
    "    \"shape_semantic\",\n",
    "]\n",
    "LIST_VARS_OTHERS = [\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "]\n",
    "LIST_CONTEXT = [\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [2 for _ in range(len(LIST_VAR))]\n",
    "LIST_FILTDICT = [\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"], \"stroke_index\":[0]},\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"]},\n",
    "    {\"task_kind\":[\"prims_single\", \"prims_on_grid\"]},\n",
    "    ]\n",
    "\n",
    "for twind_analy in [(0.05, 0.25), (-0.05, 0.35), (0.1, 0.2)]:\n",
    "    for subspace_projection in [\"shape_prims_single\", \"pca\"]:\n",
    "        for prune_version in [\"sp_char_0\", \"sp_char\"]:\n",
    "            for NPCS_KEEP in LIST_NPCS_KEEP:\n",
    "                for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "                    SAVEDIR = f\"{SAVEDIR_ANALYSIS}/subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}\"\n",
    "                    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "                    assert False\n",
    "                    PLOT_STATE_SPACE = NPCS_KEEP == max(LIST_NPCS_KEEP)\n",
    "                    run(animal, date, DFallpa, SAVEDIR, subspace_projection, prune_version, NPCS_KEEP, \n",
    "                            raw_subtract_mean_each_timepoint, n_min_trials_per_shape,\n",
    "                            PLOT_EACH_REGION, PLOT_STATE_SPACE,\n",
    "                            LIST_VAR, LIST_VARS_OTHERS, LIST_FILTDICT, LIST_PRUNE_MIN_N_LEVS, twind_analy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adcb7cc",
   "metadata": {},
   "source": [
    "##### Run, a single time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import run\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_version = \"sp_char_0\"\n",
    "# # subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "# twind_analy = (-0.15, 0.2)\n",
    "# tbin_dur = 0.1\n",
    "# tbin_slide = 0.02\n",
    "\n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "twind_analy = (0.05, 0.25)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.05\n",
    "\n",
    "NPCS_KEEP = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "n_min_trials_per_shape = 5\n",
    "PLOT_EACH_REGION = False\n",
    "PLOT_STATE_SPACE = False\n",
    "LIST_VAR = [\"shape_semantic\"]\n",
    "LIST_VARS_OTHERS = [\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "]\n",
    "LIST_CONTEXT = [\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [2 for _ in range(len(LIST_VAR))]\n",
    "LIST_FILTDICT = [\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"], \"stroke_index\":[0]},\n",
    "    ]\n",
    "\n",
    "SAVEDIR = f\"/tmp/TEST-{twind_analy}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "run(animal, date, DFallpa, SAVEDIR, subspace_projection, prune_version, NPCS_KEEP, \n",
    "        raw_subtract_mean_each_timepoint, n_min_trials_per_shape,\n",
    "        PLOT_EACH_REGION, PLOT_STATE_SPACE,\n",
    "        LIST_VAR, LIST_VARS_OTHERS, LIST_FILTDICT, LIST_PRUNE_MIN_N_LEVS, twind_analy,\n",
    "        tbin_dur = tbin_dur, tbin_slide = tbin_slide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2614a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time-series of eucl distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457a5a8",
   "metadata": {},
   "source": [
    "##### Run, for a single bregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = f\"/tmp/TEST/{animal}-{date}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"preSMA_a\"    \n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "remove_drift = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c08d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_analy = (-0.15, 0.2)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d12591",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 5\n",
    "\n",
    "# Run\n",
    "PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "savedir = f\"{SAVEDIR}/preprocess\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "plot_drawings = False\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                    n_min_trials_per_shape=n_min_trials_per_shape, plot_drawings=plot_drawings,\n",
    "                    remove_chans_fr_drift=remove_drift,\n",
    "                    subspace_projection=subspace_projection, \n",
    "                           twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, keep specific chans\n",
    "# chans_keep = [1053, 1054]\n",
    "chans_keep = [1044, 1049,  1053, 1054, 1057, 1059, 1062]\n",
    "PA = PA.slice_by_dim_values_wrapper(\"chans\", chans_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7738f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    pa = PA.slice_by_dim_values_wrapper(\"times\", twind_analy)\n",
    "    pa = pa.agg_by_time_windows_binned(tbin_dur, tbin_slide)\n",
    "\n",
    "else:\n",
    "    subspace_projection = \"shape_prims_single\"\n",
    "\n",
    "    if subspace_projection == \"pca\":\n",
    "        dim_red_method = \"pca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":None,\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":None,\n",
    "        }\n",
    "    elif subspace_projection == \"shape_prims_single\":\n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_single\"]}\n",
    "        }\n",
    "    elif subspace_projection == \"shape_PIG_stroke0\":\n",
    "        # PIG (0)  \n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_on_grid\"], \"stroke_index\":[0]}\n",
    "        }\n",
    "    elif subspace_projection == \"shape_char_stroke0\":\n",
    "        # Char  \n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"character\"], \"stroke_index\":[0]}\n",
    "        }\n",
    "    else:\n",
    "        print(subspace_projection)\n",
    "        assert False\n",
    "        \n",
    "\n",
    "    ### New, cleaner method, taking all pairwise distances between trials\n",
    "    savedir = f\"{SAVEDIR}/each_region/{bregion}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    # (1) First, dim reduction\n",
    "    superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "    superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "    superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "\n",
    "    dim_red_method = \"pca\"\n",
    "    twind_analy = (-0.15, 0.3)\n",
    "    tbin_dur = 0.1\n",
    "    tbin_slice = 0.01\n",
    "    NPCS_KEEP = 10\n",
    "    _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "                                    twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "                                    NPCS_KEEP = NPCS_KEEP,\n",
    "                                    dpca_var = None, dpca_vars_group = None, dpca_filtdict=None, \n",
    "                                    dpca_proj_twind = twind_analy, \n",
    "                                    raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                    umap_n_components=None, umap_n_neighbors=None)\n",
    "    \n",
    "\n",
    "    pa = PAredu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb54f3d",
   "metadata": {},
   "source": [
    "# [2] Quick analyses of euclidian distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_group = [\"task_kind\", \"shape_semantic\"]\n",
    "version = \"traj\"\n",
    "DFDIST = pa.dataextractwrap_distance_between_groups(vars_group, version)\n",
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "# DFDIST = append_col_with_grp_index(DFDIST, [\"shape_semantic_1\", \"shape_semantic_2\"], \"shape_semantic_same\")\n",
    "# DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_same\")\n",
    "\n",
    "DFDIST[\"task_kind_same\"] = DFDIST[\"task_kind_1\"] == DFDIST[\"task_kind_2\"]\n",
    "DFDIST[\"shape_semantic_same\"] = DFDIST[\"shape_semantic_1\"] == DFDIST[\"shape_semantic_2\"]\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_12\")\n",
    "\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_same\", \"shape_semantic_same\"], \"same-task|shape\")\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"shape_semantic_same\", \"task_kind_12\"], \"same_shape|task_kind_12\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef76aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for y in [\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"]:\n",
    "    # sns.relplot(data=DFDIST, x=\"time_bin\", y=y, hue=\"same_shape|task_kind_12\", kind=\"line\", errorbar=(\"ci\", 68))\n",
    "    sns.relplot(data=DFDIST, x=\"time_bin\", y=y, hue=\"same-task|shape\", kind=\"line\", errorbar=(\"ci\", 68))\n",
    "    # fig = sns.relplot(data=DFDIST, x=\"time_bin\", y=y, kind=\"line\", hue=\"shape_novel_12\")\n",
    "    # fig = sns.relplot(data=DFDIST, x=\"time_bin\", y=y, kind=\"line\", hue=\"seqc_0_shape_12\", col=\"shape_novel_12\", legend=False, alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(data=dfres, x = \"bregion\", hue=\"shape_task_same\", y=\"dist_yue_diff\", kind=\"bar\", aspect=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function...\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "\n",
    "\n",
    "SAVEDIR = \"/tmp/TEST_TIME_RESOLV\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "bregion = \"preSMA_a\"    \n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = None\n",
    "# subspace_projection = \"shape_prims_single\"\n",
    "NPCS_KEEP = 10\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "remove_drift = False\n",
    "twind_analy = (-0.15, 0.2)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "\n",
    "euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, savedir, twind_analy,\n",
    "                            tbin_dur, tbin_slide, \n",
    "                            subspace_projection, NPCS_KEEP, \n",
    "                            n_min_trials_per_shape = 5, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # testing by hand\n",
    "    # remove = [1047, 1048, 1051, 1052, 1056, 1058]\n",
    "    # remove = [1048, 1051, 1052, 1056, 1058, 1059, 1062]\n",
    "    remove = [1052, 1056, 1057, 1062, 1073, 1075]\n",
    "    PA = DFallpa[\"pa\"].values[2]\n",
    "    hack_prune_to_these_chans = [c for c in PA.Chans if c not in remove]\n",
    "else:\n",
    "    hack_prune_to_these_chans = None\n",
    "\n",
    "# hack_prune_to_these_chans = [1049, 1053, 1054, 1057]\n",
    "# hack_prune_to_these_chans = [1043, 1044, 1047, 1053, 1054, 1057]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176494b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "import os\n",
    "# prune_version = \"sp_char_0\"\n",
    "# prune_version = None\n",
    "n_min_trials_per_shape = 5\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/TEST_TIME_RESOLV\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "\n",
    "NPCS_KEEP = 10\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "twind_analy = (-0.35, 0.5)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "# # for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "# for bregion in [\"PMv_m\"]:\n",
    "#     # for prune_version in [\"sp_char_0\", \"pig_char_0\"]:\n",
    "#     for prune_version in [\"sp_char\", \"pig_char\", \"sp_char_0\"]:\n",
    "#         # for subspace_projection in [None, \"pca\", \"shape_prims_single\", \"shape_all\"]:\n",
    "#         for subspace_projection in [None]:\n",
    "#             for remove_drift in [False]:\n",
    "#                 for remove_singleprims_unstable in [True, False]:\n",
    "\n",
    "#                     SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{bregion}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-HACK-{hack_prune_to_these_chans is not None}\"\n",
    "#                     os.makedirs(SAVEDIR, exist_ok=True)\n",
    "#                     print(SAVEDIR)\n",
    "\n",
    "\n",
    "#                     euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "#                                                 tbin_dur, tbin_slide, \n",
    "#                                                 subspace_projection, NPCS_KEEP, \n",
    "#                                                 n_min_trials_per_shape = 5, \n",
    "#                                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "#                                                 hack_prune_to_these_chans=hack_prune_to_these_chans,\n",
    "#                                                 remove_singleprims_unstable=remove_singleprims_unstable)\n",
    "\n",
    "#                     assert False\n",
    "\n",
    "# Compact version of above\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_wrapper\n",
    "euclidian_time_resolved_wrapper(animal, date, DFallpa, \"/tmp/TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 4\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "NPCS_KEEP = 6\n",
    "\n",
    "twind_analy = (-0.4, 0.5)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "    for prune_version in [\"sp_char_0\", \"pig_char_0\", \"sp_char\", \"pig_char_1plus\"]:\n",
    "        for subspace_projection in [\"task_shape_si\"]:\n",
    "            for remove_drift in [False]:\n",
    "                for raw_subtract_mean_each_timepoint in [False]:\n",
    "                    for remove_singleprims_unstable in [True]:\n",
    "                        SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{bregion}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-subtrmean={raw_subtract_mean_each_timepoint}\"\n",
    "                        os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                        print(\"SAVING AT ... \", SAVEDIR)\n",
    "                        euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "                                                    tbin_dur, tbin_slide, \n",
    "                                                    subspace_projection, NPCS_KEEP, \n",
    "                                                    n_min_trials_per_shape = n_min_trials_per_shape, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                                    remove_singleprims_unstable=remove_singleprims_unstable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540d6ed",
   "metadata": {},
   "source": [
    "# [3] Samp, Go, reach onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/SAMP\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 4\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "twind_analy = (-1, 1)\n",
    "NPCS_KEEP = 7\n",
    "events_keep = [\"03_samp\", \"05_first_raise\"]\n",
    "\n",
    "HACK = True\n",
    "for i, row in DFallpa.iterrows():\n",
    "    bregion = row[\"bregion\"]\n",
    "    which_level = row[\"which_level\"]\n",
    "    event = row[\"event\"]\n",
    "    PA = row[\"pa\"]\n",
    "\n",
    "    if HACK and bregion not in [\"PMv\"]:\n",
    "        continue\n",
    "\n",
    "    if event in events_keep:\n",
    "        for prune_version in [\"sp_char\"]:\n",
    "            for subspace_projection in [None, \"pca\", \"task_shape\"]: # NOTE: shape_prims_single not great, you lose some part of preSMA context-dependence...\n",
    "                for remove_drift in [False]:\n",
    "                    for raw_subtract_mean_each_timepoint in [False]:\n",
    "                        for remove_singleprims_unstable in [False, True]:\n",
    "                            SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{which_level}-{bregion}-{event}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-subtrmean={raw_subtract_mean_each_timepoint}\"\n",
    "                            os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                            print(\"SAVING AT ... \", SAVEDIR)\n",
    "                            euclidian_time_resolved(animal, date, PA, which_level, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "                                                        tbin_dur, tbin_slide, \n",
    "                                                        subspace_projection, NPCS_KEEP, \n",
    "                                                        n_min_trials_per_shape = n_min_trials_per_shape, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                                        remove_singleprims_unstable=remove_singleprims_unstable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a5e36",
   "metadata": {},
   "source": [
    "# Time warping based on behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each trial, get beh stroke\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8385ea",
   "metadata": {},
   "source": [
    "# Checking drift of FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each trial.\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all\n",
    "plot_heatmap_firing_rates_all(PA, savedir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f627",
   "metadata": {},
   "source": [
    "### Sanity check, change over day?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = f\"{SAVEDIR_ANALYSIS}/drift\"\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681090c8",
   "metadata": {},
   "source": [
    "##### For each region, plot sm fr across trials and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all bregions\n",
    "assert False, \"run for both chans and PCs\"\n",
    "for i, row in DFallpa.iterrows():\n",
    "    PAthis = row[\"pa\"]\n",
    "    bregion = row[\"bregion\"]\n",
    "\n",
    "    dur = 0.1\n",
    "    slide = 0.02\n",
    "    PAthis = PAthis.agg_by_time_windows_binned(dur, slide)\n",
    "\n",
    "\n",
    "    from neuralmonkey.neuralplots.population import heatmapwrapper_stratified_each_neuron\n",
    "    fig = heatmapwrapper_stratified_each_neuron(PA, \"task_kind\")\n",
    "    assert False\n",
    "    assert False\n",
    "    savefig(fig, f\"{savedir}/{bregion}.png\")\n",
    "    plt.close(\"all\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR = f\"/tmp/CHAR_HEATMAPS\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) First, dim reduction\n",
    "superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "\n",
    "dim_red_method = \"pca\"\n",
    "twind_analy = (-0.15, 0.3)\n",
    "tbin_dur = 0.1\n",
    "tbin_slice = 0.01\n",
    "NPCS_KEEP = 10\n",
    "_, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "                                twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "                                NPCS_KEEP = NPCS_KEEP,\n",
    "                                dpca_var = None, dpca_vars_group = None, dpca_filtdict=None, \n",
    "                                dpca_proj_twind = twind_analy, \n",
    "                                raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                umap_n_components=None, umap_n_neighbors=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d50581",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Script for heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16207999",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import params_subspace_projection\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all, plot_heatmap_firing_rates_all_wrapper\n",
    "\n",
    "plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR, animal, date)\n",
    "\n",
    "\n",
    "# bregion = \"FP_p\"\n",
    "# prune_version = \"sp_char_0\"\n",
    "# n_min_trials_per_shape = 5\n",
    "# raw_subtract_mean_each_timepoint = False\n",
    "# ### FINAL -- script over all\n",
    "# LIST_SS_PRUNE = [\n",
    "#     (None, False),\n",
    "#     (\"pca\", False),\n",
    "#     (\"shape_prims_single\", False),\n",
    "#     (\"pca\", True),\n",
    "#     (\"shape_prims_single\", True),\n",
    "# ]\n",
    "# PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "# ################# PREPROCESS\n",
    "# savedir = f\"{SAVEDIR}/preprocess\"\n",
    "# os.makedirs(savedir, exist_ok=True)\n",
    "# plot_drawings = False\n",
    "# PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "#                 n_min_trials_per_shape=n_min_trials_per_shape, plot_drawings=plot_drawings)\n",
    "\n",
    "# for subspace_projection, prune_chans in LIST_SS_PRUNE:\n",
    "\n",
    "#     savedir = f\"{SAVEDIR}/ss={subspace_projection}-prunedrift={prune_chans}\"\n",
    "#     os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "#     ########### PRUNE CHANS\n",
    "#     if prune_chans:\n",
    "#         # Optioanlly, remove channels with drift\n",
    "#         from neuralmonkey.classes.population_mult import dfallpa_preprocess_sitesdirty_single_just_drift\n",
    "#         PA = dfallpa_preprocess_sitesdirty_single_just_drift(PA, animal, date)\n",
    "\n",
    "#     ########### DIM REDUCTIONS\n",
    "#     if subspace_projection is not None:\n",
    "#         dim_red_method, superv_dpca_params = params_subspace_projection(subspace_projection)\n",
    "#         superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "#         superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "#         superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "#         twind_analy = (-0.2, 0.35)\n",
    "#         tbin_dur = 0.1\n",
    "#         tbin_slice = 0.01\n",
    "#         NPCS_KEEP = 10\n",
    "#         savedirthis = f\"{savedir}/dimred-{subspace_projection}\"\n",
    "#         os.makedirs(savedirthis, exist_ok=True)\n",
    "#         _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedirthis, \n",
    "#                                         twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "#                                         NPCS_KEEP = NPCS_KEEP,\n",
    "#                                         dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_filtdict=superv_dpca_filtdict, \n",
    "#                                         dpca_proj_twind = twind_analy, \n",
    "#                                         raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "#                                         umap_n_components=None, umap_n_neighbors=None)\n",
    "#     else:\n",
    "#         PAredu = PA\n",
    "\n",
    "#     savedirthis = f\"{savedir}/heatmaps-{subspace_projection}/{bregion}\"\n",
    "#     os.makedirs(savedirthis, exist_ok=True)\n",
    "#     plot_heatmap_firing_rates_all(PAredu, savedirthis)\n",
    "\n",
    "#     plt.close(\"all\")\n",
    "\n",
    "\n",
    "# # ################## [RAW]\n",
    "# # savedir = f\"{SAVEDIR}/heatmaps_raw/{bregion}\"\n",
    "# # os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# # plot_heatmap_firing_rates_all(PA, savedir)\n",
    "\n",
    "# # ################# [PCA]\n",
    "# # dim_red_method = \"pca\"\n",
    "# # twind_analy = (-0.15, 0.3)\n",
    "# # tbin_dur = 0.1\n",
    "# # tbin_slice = 0.01\n",
    "# # NPCS_KEEP = 10\n",
    "# # superv_dpca_var = None\n",
    "# # superv_dpca_vars_group = None\n",
    "# # superv_dpca_filtdict = None\n",
    "# # _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "# #                                 twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "# #                                 NPCS_KEEP = NPCS_KEEP,\n",
    "# #                                 dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_filtdict=superv_dpca_filtdict, \n",
    "# #                                 dpca_proj_twind = twind_analy, \n",
    "# #                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "# #                                 umap_n_components=None, umap_n_neighbors=None)\n",
    "\n",
    "# # savedir = f\"{SAVEDIR}/heatmaps_pca/{bregion}\"\n",
    "# # os.makedirs(savedir, exist_ok=True)\n",
    "# # plot_heatmap_firing_rates_all(PAredu, savedir)\n",
    "\n",
    "\n",
    "# # ################# [DPCA]\n",
    "# # dim_red_method = \"superv_dpca\"\n",
    "# # superv_dpca_params={\n",
    "# #     \"superv_dpca_var\":\"shape_semantic\",\n",
    "# #     \"superv_dpca_vars_group\":None,\n",
    "# #     \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_single\"]}\n",
    "# # }\n",
    "# # superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "# # superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "# # superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "# # NPCS_KEEP = 6\n",
    "# # _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "# #                                 twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "# #                                 NPCS_KEEP = NPCS_KEEP,\n",
    "# #                                 dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_filtdict=superv_dpca_filtdict, \n",
    "# #                                 dpca_proj_twind = twind_analy, \n",
    "# #                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "# #                                 umap_n_components=None, umap_n_neighbors=None)\n",
    "# # savedir = f\"{SAVEDIR}/heatmaps_dpca/{bregion}\"\n",
    "# # os.makedirs(savedir, exist_ok=True)\n",
    "# # plot_heatmap_firing_rates_all(PAredu, savedir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94190db2",
   "metadata": {},
   "source": [
    "### Accounting for drift -- exclude cases with very different PSTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff745869",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_clean_stable_single_prims_frate\n",
    "savedir = \"/tmp/test\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "preprocess_clean_stable_single_prims_frate(PA, savedir=savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab18d69",
   "metadata": {},
   "source": [
    "##### Re-apply filter based on dirty sites, being more strict about drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each grouping, get the meanPSTH, and c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae030b8",
   "metadata": {},
   "source": [
    "# MULT DATA - euclidian_time_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5204605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER, _REGIONS_IN_ORDER_COMBINED\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b801cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import  load_all_dates\n",
    "# Diego (trial, all)\n",
    "LIST_ANIMAL_DATE_COMB = [\n",
    "    (\"Diego\", 231122, True),\n",
    "    (\"Diego\", 231128, True),\n",
    "    (\"Diego\", 231129, True),\n",
    "    (\"Diego\", 231201, True),\n",
    "    (\"Diego\", 231204, True),\n",
    "    (\"Diego\", 231205, True),\n",
    "    (\"Diego\", 231211, True),\n",
    "    (\"Diego\", 231213, True),\n",
    "]\n",
    "which_level = \"trial\"\n",
    "savedir_method_old = False\n",
    "\n",
    "DFDIST = load_all_dates(LIST_ANIMAL_DATE_COMB, which_level, savedir_method_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55770662",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For plots, see analy_euclidian_chars_sp_MULT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad33c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import  plot_scalar_all\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620034e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_scalar = [-0.3, 0.1]\n",
    "DFDISTthis = DFDIST[(DFDIST[\"time_bin\"]>=twind_scalar[0]) & (DFDIST[\"time_bin\"]<=twind_scalar[1])].reset_index(drop=True)\n",
    "\n",
    "# Agg, averaging over time\n",
    "DFTHISscal = aggregGeneral(DFDISTthis, [\"animal\", \"date\", \"combine_areas\", \"event\", \"bregion\", \"metaparams\", \"same-task|shape\", \"prune_version\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                \"remove_singleprims_unstable\"], values=[\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_event_to_twind = {\n",
    "    \"03_samp\":[0.2, 1.0],\n",
    "    \"05_first_raise\":[-0.5,  0],\n",
    "}\n",
    "\n",
    "list_df = []\n",
    "for event, twind_scalar in map_event_to_twind.items():\n",
    "    dfthis = DFDIST[DFDIST[\"event\"] == event]\n",
    "    dfthis_sub = dfthis[(dfthis[\"time_bin\"]>=twind_scalar[0]-0.001) & (dfthis[\"time_bin\"]<=twind_scalar[1]+0.001)].reset_index(drop=True)\n",
    "    list_df.append(dfthis_sub)\n",
    "DFDISTthis = pd.concat(list_df).reset_index(drop=True)\n",
    "\n",
    "# Agg, averaging over time\n",
    "DFTHISscal = aggregGeneral(DFDISTthis, [\"animal\", \"date\", \"combine_areas\", \"event\", \"bregion\", \"metaparams\", \"same-task|shape\", \"prune_version\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                \"remove_singleprims_unstable\"], values=[\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.expttools import writeDictToTxt\n",
    "writeDictToTxt(map_event_to_twind, f\"{savedir}/twind_params.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67423387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final scalar plot\n",
    "DFTHISscal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/tmp/SCALAR\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFTHISscal, [\"event\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                                                \"remove_singleprims_unstable\"])\n",
    "for (event, subspace_projection, remove_drift, raw_subtract_mean_each_timepoint, remove_singleprims_unstable), inds in grpdict.items():\n",
    "    dfthis = DFTHISscal.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"same-task|shape\", \"0|1\", \"1|0\", \n",
    "                                        \"prune_version\", \"dist_yue_diff\", \"bregion\", \n",
    "                                        True, shareaxes=True, SIZE=4);\n",
    "    savefig(fig, f\"{savedir}/EVENT={event}-ss={subspace_projection}-rmvdrift={remove_drift}-subtrmean={raw_subtract_mean_each_timepoint}-rmvunstable={remove_singleprims_unstable}.pdf\")\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bafbb",
   "metadata": {},
   "source": [
    "##### Convert to scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_scalar = [-0.3, 0.1]\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import plot_scalar_all\n",
    "plot_scalar_all(DFDIST, \"/tmp\", twind_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d389c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e25171",
   "metadata": {},
   "source": [
    "### Combine all kinds of pairwise comparisons in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "savedir = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFTHISscal, [\"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                                                   \"remove_singleprims_unstable\"])\n",
    "for (subspace_projection, remove_drift, raw_subtract_mean_each_timepoint, remove_singleprims_unstable), inds in grpdict.items():\n",
    "    dfthis = DFTHISscal.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"same-task|shape\", \"0|1\", \"1|0\", \n",
    "                                        \"metaparams\", \"dist_yue_diff\", \"bregion\", \n",
    "                                        True, shareaxes=True, SIZE=4);    \n",
    "    savefig(fig, f\"{savedir}/ss={subspace_projection}-rmvdrift={remove_drift}-subtrmean={raw_subtract_mean_each_timepoint}-rmvunstable={remove_singleprims_unstable}\")\n",
    "    plt.close(\"all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38710bc",
   "metadata": {},
   "source": [
    "# Example figures (for paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa, plot_heatmap_firing_rates_all\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b557cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d154fd",
   "metadata": {},
   "source": [
    "##### [Code] plot_heatmap_firing_rates_all_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n",
    "PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "print(PA.X.shape)\n",
    "\n",
    "prune_version = \"sp_char_0\"\n",
    "n_min_trials_per_shape = 4\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "plot_drawings = False\n",
    "# twind_analy = (-0.4, 0.5)\n",
    "twind_analy = (-0.35, 0.3)\n",
    "tbin_dur=0.1\n",
    "tbin_slide=0.01\n",
    "\n",
    "NPCS_KEEP = 10\n",
    "subspace_projection = \"pca\"\n",
    "# NPCS_KEEP = None\n",
    "# subspace_projection = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "savedir = \"/tmp\"\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                    n_min_trials_per_shape=n_min_trials_per_shape, shape_var=shape_var, plot_drawings=plot_drawings,\n",
    "                    remove_chans_fr_drift=False,\n",
    "                    subspace_projection=subspace_projection, \n",
    "                        twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                        raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                        remove_singleprims_unstable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ef508",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For raw data, pick out 10 neurons\n",
    "if False:\n",
    "    from neuralmonkey.metrics.scalar import _calc_modulation_by_frsm_event_aligned_time\n",
    "    res = []\n",
    "    for i, chan in enumerate(PA.Chans):\n",
    "        frmat = PA.X[i, :, :]\n",
    "        r2 = _calc_modulation_by_frsm_event_aligned_time(frmat)\n",
    "        res.append({\n",
    "            \"r2\":r2,\n",
    "            \"chan\":chan,\n",
    "            \"indchan\":i\n",
    "        })\n",
    "    animal, date\n",
    "    df = pd.DataFrame(res)\n",
    "    import seaborn as sns\n",
    "    fig = sns.catplot(data=df, x=\"chan\", y=\"r2\", aspect=2.5, kind=\"bar\")\n",
    "    from pythonlib.tools.snstools import rotateLabel\n",
    "    rotateLabel(fig)\n",
    "    nchans = 10\n",
    "    list_indchan = df.sort_values(\"r2\", ascending=False)[\"indchan\"].tolist()[:nchans]\n",
    "    PA = PA.slice_by_dim_indices_wrapper(\"chans\", list_indchan)\n",
    "\n",
    "# for each neuron get its \n",
    "if False:\n",
    "    # too many plots\n",
    "    import os\n",
    "    savedir = \"/tmp/HEATMAPS\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    plot_heatmap_firing_rates_all(PA, savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pa = PA.norm_subtract_trial_mean_each_timepoint(\"trials\")\n",
    "# pa = PA.copy()\n",
    "\n",
    "# Pretty plot\n",
    "# (subplots, shape on y, task_kind on x)\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from neuralmonkey.neuralplots.population import heatmap_stratified_trials_grouped_by_neuron, heatmap_stratified_neuron_grouped_by_var\n",
    "\n",
    "dflab = pa.Xlabels[\"trials\"]\n",
    "\n",
    "var_row = \"shape_semantic_grp\"\n",
    "var_col = \"task_kind\"\n",
    "\n",
    "row_levels = dflab[var_row].unique().tolist()\n",
    "col_levels = dflab[var_col].unique().tolist()\n",
    "\n",
    "grpdict = grouping_append_and_return_inner_items_good(dflab, [var_row, var_col])\n",
    "\n",
    "n_rand_trials = 10\n",
    "nticks = 5\n",
    "\n",
    "SIZE = 3\n",
    "ncols = len(col_levels)\n",
    "nrows = len(row_levels)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*SIZE, nrows*SIZE))\n",
    "\n",
    "zlims = np.percentile(pa.X.flatten(), [1, 99])\n",
    "\n",
    "\n",
    "for i, row in enumerate(row_levels):\n",
    "    for j, col in enumerate(col_levels):\n",
    "        ax = axes[i][j]\n",
    "        inds = grpdict[(row, col)]\n",
    "\n",
    "        heatmap_stratified_trials_grouped_by_neuron(pa, inds, ax, n_rand_trials=n_rand_trials, zlims=zlims)\n",
    "\n",
    "        ax.set_title((row, col), fontsize=8)\n",
    "        # assert False\n",
    "\n",
    "        # times = pa.Times\n",
    "        # _inds = np.linspace(0, len(times)-1, nticks).astype(int)\n",
    "        # _vals = times[_inds]\n",
    "        # _vals = [f\"{v:.3f}\" for v in _vals]\n",
    "        # ax.set_xticks(_inds+0.5, _vals, rotation=45, fontsize=6)\n",
    "\n",
    "# savefig(fig, f\"{savedir}/grouped_by_neuron.png\")\n",
    "# plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a34533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all_wrapper\n",
    "SAVEDIR_ANALYSIS = \"/tmp/HEATMAP_WRAPPER\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR_ANALYSIS, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8313e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # too many plots\n",
    "    import os\n",
    "    savedir = \"/tmp/HEATMAPS\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    plot_heatmap_firing_rates_all(PA, savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb54ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also overlay behavioral timing\n",
    "\n",
    "# - go cue.\n",
    "# - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b834c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import load_session_helper, load_mult_session_helper\n",
    "MS = load_mult_session_helper(date, animal)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import beh_plot_event_timing_stroke\n",
    "savedir = \"/tmp\"\n",
    "PA = DFallpa.iloc[0][\"pa\"]\n",
    "beh_plot_event_timing_stroke(PA, animal, date, savedir, MS=MS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efc4f7",
   "metadata": {},
   "source": [
    "# Cross-temporal scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7466e",
   "metadata": {},
   "source": [
    "Consider each timebin for char vs. each timebin for SP.\n",
    "\n",
    "For each, get scores of diff|same, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - Reshape PA so that time is on the trial axis.\n",
    "from neuralmonkey.classes.population import concatenate_popanals\n",
    "\n",
    "list_pa = []\n",
    "list_labels = []\n",
    "for i in range(len(PAredu.Times)):\n",
    "    pathis = PAredu.slice_by_dim_indices_wrapper(\"times\", [i])\n",
    "\n",
    "    # collect infoextract_snippets_trials\n",
    "    list_pa.append(pathis)\n",
    "    list_labels.append(i)\n",
    "\n",
    "PAreduScal = concatenate_popanals(list_pa, dim=\"trials\", \n",
    "                                map_idxpa_to_value=list_labels, \n",
    "                                map_idxpa_to_value_colname=\"time_bin\",\n",
    "                                assert_otherdims_have_same_values=False,\n",
    "                                times_realign_so_first_index_is_this_time=0)\n",
    "PAreduScal.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cldist = PAreduScal.dataextract_as_distance_matrix_clusters_flex([\"task_kind\", \"shape_semantic\", \"time_bin\"], \n",
    "                                                             return_as_single_mean_over_time=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8134eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres = Cldist.rsa_distmat_score_all_pairs_of_label_groups(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e21451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres[\"task_kind_same\"] = dfres[\"task_kind_1\"] == dfres[\"task_kind_2\"]\n",
    "dfres[\"shape_semantic_same\"] = dfres[\"shape_semantic_1\"] == dfres[\"shape_semantic_2\"]\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_same\", \"task_kind_same\"], \"shape_task_same\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_pair\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_1\", \"shape_semantic_2\"], \"shape_semantic_pair\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"time_bin_1\", \"time_bin_2\"], \"time_bin_pair\")\n",
    "\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_same\", \"task_kind_1\", \"task_kind_2\"], \"ss_task_kind_pair\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c147809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f55380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subplots_heatmap(dfres, \"time_bin_1\", \"time_bin_2\", \"dist_yue_diff\", \"ss_task_kind_pair\", share_zlim=True)\n",
    "plot_subplots_heatmap(dfres, \"time_bin_1\", \"time_bin_2\", \"dist_mean\", \"ss_task_kind_pair\", share_zlim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9641ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take difference across subplots, plotting a new subplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ca7cd",
   "metadata": {},
   "source": [
    "# Check motor similarity of strokes from SP vs. CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "\n",
    "savedir = \"/tmp/PREPROCESS\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "PA = preprocess_pa(PA, savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79e0a8",
   "metadata": {},
   "source": [
    "# Collect and plot across days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_SAVE = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/MULT\"\n",
    "\n",
    "list_animal_date = [\n",
    "    # (\"Pancho\", 230112),\n",
    "    # (\"Pancho\", 230117),\n",
    "    # (\"Pancho\", 230118),\n",
    "    (\"Pancho\", 230119),\n",
    "    (\"Pancho\", 230120),\n",
    "    (\"Pancho\", 230122),\n",
    "    (\"Pancho\", 230125),\n",
    "    (\"Pancho\", 230126),\n",
    "    (\"Pancho\", 230127),\n",
    "    ]\n",
    "\n",
    "combine = True\n",
    "ANIMAL = \"Pancho\"\n",
    "\n",
    "LIST_NPCS_KEEP = [4,6,2]\n",
    "for twind_analy in [(0.05, 0.25), (-0.05, 0.35), (0.1, 0.2)]:\n",
    "    for subspace_projection in [\"shape_prims_single\", \"pca\"]:\n",
    "        for prune_version in [\"sp_char_0\", \"sp_char\"]:\n",
    "            for NPCS_KEEP in LIST_NPCS_KEEP:\n",
    "                for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "\n",
    "                    # subspace_projection = \"shape_prims_single\"\n",
    "                    # prune_version = \"sp_char_0\"\n",
    "                    # NPCS_KEEP = 4\n",
    "                    # raw_subtract_mean_each_timepoint = False\n",
    "                    # twind_analy = (0.05, 0.25)\n",
    "\n",
    "                    try:\n",
    "                        ### Load all data for this params configuration\n",
    "                        list_dfres = []\n",
    "                        for animal, date in list_animal_date:\n",
    "                            if animal==ANIMAL:\n",
    "                                # animal = \"Pancho\"\n",
    "                                # date = 230126\n",
    "\n",
    "                                SAVEDIR_ANALYSIS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/{animal}-{date}-combine={combine}\"\n",
    "                                path = f\"{SAVEDIR_ANALYSIS}/subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}/summary/DFRES.pkl\"\n",
    "                                dfres = pd.read_pickle(path)\n",
    "\n",
    "                                dfres[\"animal\"] = animal\n",
    "                                dfres[\"date\"] = date\n",
    "\n",
    "                                list_dfres.append(dfres)\n",
    "                    except FileNotFoundError as err:\n",
    "                        print(\"Skipping, did not find all data...\", path)\n",
    "                        continue\n",
    "\n",
    "                    ######### PLOTS\n",
    "                    savedir = f\"{SAVEDIR_SAVE}/{ANIMAL}/combine={combine}-subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    DFRES = pd.concat(list_dfres).reset_index(drop=True)\n",
    "\n",
    "                    from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "                    DFRES = append_col_with_grp_index(DFRES, [\"animal\", \"date\"], \"ani_dat\")\n",
    "\n",
    "                    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", \"bregion\", \n",
    "                                                                    \"dist_yue_diff\", \"ani_dat\", True, SIZE=3, shareaxes=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-1.pdf\")\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", None, \n",
    "                                                                    \"dist_yue_diff\", \"bregion\", True, SIZE=4, shareaxes=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-2.pdf\")\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", \"bregion\", \n",
    "                                                                    \"dist_yue_diff\", \"ani_dat\", False, SIZE=6, shareaxes=True,\n",
    "                                                                    color_by_var_datapt=True, alpha=0.5,\n",
    "                                                                    force_all_on_same_axis=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-3.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=DFRES, x = \"bregion\", hue=\"shape_task_same\", y=\"dist_yue_diff\", kind=\"bar\", aspect=2)\n",
    "                    savefig(fig, f\"{savedir}/catplot-1.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=DFRES, x = \"bregion\", hue=\"ani_dat\", y=\"dist_yue_diff\", kind=\"point\", aspect=2, col=\"shape_task_same\", col_wrap=2)\n",
    "                    savefig(fig, f\"{savedir}/catplot-2.pdf\")\n",
    "\n",
    "\n",
    "                    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4440d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drag2_matlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
