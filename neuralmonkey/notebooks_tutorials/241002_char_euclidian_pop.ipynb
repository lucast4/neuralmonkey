{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7125966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConsolidates everything from all prior tutorials, except \"euclidian stuff\" (for that see 240410_kedar_euclidia...)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Consolidates everything from all prior tutorials, except \"euclidian stuff\" (for that see 240410_kedar_euclidia...)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b248d33aff307a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d20c8",
   "metadata": {},
   "source": [
    "# Load DFallPa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14525eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Method: loading functrion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5608555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010699ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "# combine = False\n",
    "# question = \"CHAR_BASE_stroke\"\n",
    "# version = \"stroke\"\n",
    "\n",
    "animal = \"Diego\"\n",
    "date = 231120\n",
    "combine = True\n",
    "question = \"CHAR_BASE_stroke\"\n",
    "version = \"stroke\"\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "# combine = True\n",
    "# question = \"CHAR_BASE_trial\"\n",
    "# version = \"trial\"\n",
    "\n",
    "# animal = \"Diego\"\n",
    "# date = 231205\n",
    "# combine = True\n",
    "# question = \"CHAR_BASE_trial\"\n",
    "# version = \"trial\"\n",
    "\n",
    "DFallpa = load_handsaved_wrapper(animal, date, version=version, combine_areas=combine, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a29f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.metadat.analy.anova_params import params_getter_euclidian_vars\n",
    "from neuralmonkey.classes.population_mult import dfpa_concatbregion_preprocess_clean_bad_channels, dfpa_concatbregion_preprocess_wrapper\n",
    "\n",
    "# LIST_VAR, LIST_VARS_OTHERS, LIST_CONTEXT, LIST_PRUNE_MIN_N_LEVS, LIST_FILTDICT = params_getter_euclidian_vars(question)\n",
    "\n",
    "# Make a copy of all PA before normalization\n",
    "plot_clean_lowfr_chans=False\n",
    "dfpa_concatbregion_preprocess_wrapper(DFallpa, animal, date, plot_clean_lowfr_chans=plot_clean_lowfr_chans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947842bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_preprocess_sort_by_trialcode\n",
    "\n",
    "dfallpa_preprocess_sort_by_trialcode(DFallpa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bed64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[1]\n",
    "dflab = PA.Xlabels[\"trials\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "import os\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "\n",
    "savedir = \"/tmp/PREPROCESS\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]\n",
    "dflab[\"supervision_stage_concise\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72115c7e",
   "metadata": {},
   "source": [
    "### Compare beh strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA.behavior_extract_strokes_to_dflab(trial_take_first_stroke=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac20dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "grpdict = grouping_append_and_return_inner_items_good(dflab, [shape_var, \"task_kind\"])\n",
    "\n",
    "list_shape = dflab[shape_var].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c304c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two set of strokes, score how similar they are.\n",
    "tk1 = \"prims_single\"\n",
    "tk2 = \"character\"\n",
    "for shape in list_shape:\n",
    "    key1 = (shape, tk1)\n",
    "    key2 = (shape, tk2)\n",
    "    if key1 not in grpdict.keys():\n",
    "        print(\"Key not found: \", key1)\n",
    "        continue\n",
    "    if key2 not in grpdict.keys():\n",
    "        print(\"Key not found: \", key2)\n",
    "        continue\n",
    "    \n",
    "    strokes1 = dflab.iloc[grpdict[key1]][\"strok_beh\"].tolist()\n",
    "    strokes2 = dflab.iloc[grpdict[key2]][\"strok_beh\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b27094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "ds = DatStrokes()\n",
    "Cl = ds.distgood_compute_beh_beh_strok_distances(strokes1, strokes2, label_var=shape_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc668aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88237655",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = DFallpa[\"pa\"].values[5]\n",
    "preprocess_pa(animal, date, pa, \"/tmp\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4510d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "behstrokes_extract_char_clust_sim(PA, animal, date, savedir=None, PLOT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "behstrokes_extract_char_clust_sim(PA, animal, date, \"/tmp\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9daa1a",
   "metadata": {},
   "source": [
    "# [0] Do state space and euclidian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84d7e0",
   "metadata": {},
   "source": [
    "##### First, prune PA to just good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "import seaborn as sns\n",
    "from pythonlib.tools.plottools import savefig\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/CHAR_SP_FINAL\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06978a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune to just the DFallpa for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PARAMS\n",
    "n_min_trials_per_shape = 5\n",
    "LIST_NPCS_KEEP = [4,6,2]\n",
    "PLOT_EACH_REGION = True\n",
    "\n",
    "### State space plots\n",
    "LIST_VAR = [\n",
    "    \"shape_semantic\",\n",
    "    \"shape_semantic\",\n",
    "    \"shape_semantic\",\n",
    "]\n",
    "LIST_VARS_OTHERS = [\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "]\n",
    "LIST_CONTEXT = [\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [2 for _ in range(len(LIST_VAR))]\n",
    "LIST_FILTDICT = [\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"], \"stroke_index\":[0]},\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"]},\n",
    "    {\"task_kind\":[\"prims_single\", \"prims_on_grid\"]},\n",
    "    ]\n",
    "\n",
    "for twind_analy in [(0.05, 0.25), (-0.05, 0.35), (0.1, 0.2)]:\n",
    "    for subspace_projection in [\"shape_prims_single\", \"pca\"]:\n",
    "        for prune_version in [\"sp_char_0\", \"sp_char\"]:\n",
    "            for NPCS_KEEP in LIST_NPCS_KEEP:\n",
    "                for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "                    SAVEDIR = f\"{SAVEDIR_ANALYSIS}/subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}\"\n",
    "                    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "                    assert False\n",
    "                    PLOT_STATE_SPACE = NPCS_KEEP == max(LIST_NPCS_KEEP)\n",
    "                    run(animal, date, DFallpa, SAVEDIR, subspace_projection, prune_version, NPCS_KEEP, \n",
    "                            raw_subtract_mean_each_timepoint, n_min_trials_per_shape,\n",
    "                            PLOT_EACH_REGION, PLOT_STATE_SPACE,\n",
    "                            LIST_VAR, LIST_VARS_OTHERS, LIST_FILTDICT, LIST_PRUNE_MIN_N_LEVS, twind_analy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adcb7cc",
   "metadata": {},
   "source": [
    "##### Run, a single time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import run\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_version = \"sp_char_0\"\n",
    "# # subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "# twind_analy = (-0.15, 0.2)\n",
    "# tbin_dur = 0.1\n",
    "# tbin_slide = 0.02\n",
    "\n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "twind_analy = (0.05, 0.25)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.05\n",
    "\n",
    "NPCS_KEEP = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "n_min_trials_per_shape = 5\n",
    "PLOT_EACH_REGION = False\n",
    "PLOT_STATE_SPACE = False\n",
    "LIST_VAR = [\"shape_semantic\"]\n",
    "LIST_VARS_OTHERS = [\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "]\n",
    "LIST_CONTEXT = [\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [2 for _ in range(len(LIST_VAR))]\n",
    "LIST_FILTDICT = [\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"], \"stroke_index\":[0]},\n",
    "    ]\n",
    "\n",
    "SAVEDIR = f\"/tmp/TEST-{twind_analy}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "run(animal, date, DFallpa, SAVEDIR, subspace_projection, prune_version, NPCS_KEEP, \n",
    "        raw_subtract_mean_each_timepoint, n_min_trials_per_shape,\n",
    "        PLOT_EACH_REGION, PLOT_STATE_SPACE,\n",
    "        LIST_VAR, LIST_VARS_OTHERS, LIST_FILTDICT, LIST_PRUNE_MIN_N_LEVS, twind_analy,\n",
    "        tbin_dur = tbin_dur, tbin_slide = tbin_slide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2614a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time-series of eucl distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457a5a8",
   "metadata": {},
   "source": [
    "##### Run, for a single bregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = f\"/tmp/TEST/{animal}-{date}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"preSMA_a\"    \n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "remove_drift = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c08d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_analy = (-0.15, 0.2)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d12591",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 5\n",
    "\n",
    "# Run\n",
    "PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "savedir = f\"{SAVEDIR}/preprocess\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "plot_drawings = False\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                    n_min_trials_per_shape=n_min_trials_per_shape, plot_drawings=plot_drawings,\n",
    "                    remove_chans_fr_drift=remove_drift,\n",
    "                    subspace_projection=subspace_projection, \n",
    "                           twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, keep specific chans\n",
    "# chans_keep = [1053, 1054]\n",
    "chans_keep = [1044, 1049,  1053, 1054, 1057, 1059, 1062]\n",
    "PA = PA.slice_by_dim_values_wrapper(\"chans\", chans_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7738f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    pa = PA.slice_by_dim_values_wrapper(\"times\", twind_analy)\n",
    "    pa = pa.agg_by_time_windows_binned(tbin_dur, tbin_slide)\n",
    "\n",
    "else:\n",
    "    subspace_projection = \"shape_prims_single\"\n",
    "\n",
    "    if subspace_projection == \"pca\":\n",
    "        dim_red_method = \"pca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":None,\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":None,\n",
    "        }\n",
    "    elif subspace_projection == \"shape_prims_single\":\n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_single\"]}\n",
    "        }\n",
    "    elif subspace_projection == \"shape_PIG_stroke0\":\n",
    "        # PIG (0)  \n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_on_grid\"], \"stroke_index\":[0]}\n",
    "        }\n",
    "    elif subspace_projection == \"shape_char_stroke0\":\n",
    "        # Char  \n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"character\"], \"stroke_index\":[0]}\n",
    "        }\n",
    "    else:\n",
    "        print(subspace_projection)\n",
    "        assert False\n",
    "        \n",
    "\n",
    "    ### New, cleaner method, taking all pairwise distances between trials\n",
    "    savedir = f\"{SAVEDIR}/each_region/{bregion}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    # (1) First, dim reduction\n",
    "    superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "    superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "    superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "\n",
    "    dim_red_method = \"pca\"\n",
    "    twind_analy = (-0.15, 0.3)\n",
    "    tbin_dur = 0.1\n",
    "    tbin_slice = 0.01\n",
    "    NPCS_KEEP = 10\n",
    "    _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "                                    twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "                                    NPCS_KEEP = NPCS_KEEP,\n",
    "                                    dpca_var = None, dpca_vars_group = None, dpca_filtdict=None, \n",
    "                                    dpca_proj_twind = twind_analy, \n",
    "                                    raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                    umap_n_components=None, umap_n_neighbors=None)\n",
    "    \n",
    "\n",
    "    pa = PAredu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb54f3d",
   "metadata": {},
   "source": [
    "# [2] Quick analyses of euclidian distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_group = [\"task_kind\", \"shape_semantic\"]\n",
    "version = \"traj\"\n",
    "DFDIST = pa.dataextractwrap_distance_between_groups(vars_group, version)\n",
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "# DFDIST = append_col_with_grp_index(DFDIST, [\"shape_semantic_1\", \"shape_semantic_2\"], \"shape_semantic_same\")\n",
    "# DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_same\")\n",
    "\n",
    "DFDIST[\"task_kind_same\"] = DFDIST[\"task_kind_1\"] == DFDIST[\"task_kind_2\"]\n",
    "DFDIST[\"shape_semantic_same\"] = DFDIST[\"shape_semantic_1\"] == DFDIST[\"shape_semantic_2\"]\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_12\")\n",
    "\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_same\", \"shape_semantic_same\"], \"same-task|shape\")\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"shape_semantic_same\", \"task_kind_12\"], \"same_shape|task_kind_12\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef76aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for y in [\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"]:\n",
    "    # sns.relplot(data=DFDIST, x=\"time_bin\", y=y, hue=\"same_shape|task_kind_12\", kind=\"line\", errorbar=(\"ci\", 68))\n",
    "    sns.relplot(data=DFDIST, x=\"time_bin\", y=y, hue=\"same-task|shape\", kind=\"line\", errorbar=(\"ci\", 68))\n",
    "    # fig = sns.relplot(data=DFDIST, x=\"time_bin\", y=y, kind=\"line\", hue=\"shape_novel_12\")\n",
    "    # fig = sns.relplot(data=DFDIST, x=\"time_bin\", y=y, kind=\"line\", hue=\"seqc_0_shape_12\", col=\"shape_novel_12\", legend=False, alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(data=dfres, x = \"bregion\", hue=\"shape_task_same\", y=\"dist_yue_diff\", kind=\"bar\", aspect=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function...\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "\n",
    "\n",
    "SAVEDIR = \"/tmp/TEST_TIME_RESOLV\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "bregion = \"preSMA_a\"    \n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = None\n",
    "# subspace_projection = \"shape_prims_single\"\n",
    "NPCS_KEEP = 10\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "remove_drift = False\n",
    "twind_analy = (-0.15, 0.2)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "\n",
    "euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, savedir, twind_analy,\n",
    "                            tbin_dur, tbin_slide, \n",
    "                            subspace_projection, NPCS_KEEP, \n",
    "                            n_min_trials_per_shape = 5, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # testing by hand\n",
    "    # remove = [1047, 1048, 1051, 1052, 1056, 1058]\n",
    "    # remove = [1048, 1051, 1052, 1056, 1058, 1059, 1062]\n",
    "    remove = [1052, 1056, 1057, 1062, 1073, 1075]\n",
    "    PA = DFallpa[\"pa\"].values[2]\n",
    "    hack_prune_to_these_chans = [c for c in PA.Chans if c not in remove]\n",
    "else:\n",
    "    hack_prune_to_these_chans = None\n",
    "\n",
    "# hack_prune_to_these_chans = [1049, 1053, 1054, 1057]\n",
    "# hack_prune_to_these_chans = [1043, 1044, 1047, 1053, 1054, 1057]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176494b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "import os\n",
    "# prune_version = \"sp_char_0\"\n",
    "# prune_version = None\n",
    "n_min_trials_per_shape = 5\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/TEST_TIME_RESOLV\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "\n",
    "NPCS_KEEP = 10\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "twind_analy = (-0.35, 0.5)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "# # for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "# for bregion in [\"PMv_m\"]:\n",
    "#     # for prune_version in [\"sp_char_0\", \"pig_char_0\"]:\n",
    "#     for prune_version in [\"sp_char\", \"pig_char\", \"sp_char_0\"]:\n",
    "#         # for subspace_projection in [None, \"pca\", \"shape_prims_single\", \"shape_all\"]:\n",
    "#         for subspace_projection in [None]:\n",
    "#             for remove_drift in [False]:\n",
    "#                 for remove_singleprims_unstable in [True, False]:\n",
    "\n",
    "#                     SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{bregion}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-HACK-{hack_prune_to_these_chans is not None}\"\n",
    "#                     os.makedirs(SAVEDIR, exist_ok=True)\n",
    "#                     print(SAVEDIR)\n",
    "\n",
    "\n",
    "#                     euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "#                                                 tbin_dur, tbin_slide, \n",
    "#                                                 subspace_projection, NPCS_KEEP, \n",
    "#                                                 n_min_trials_per_shape = 5, \n",
    "#                                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "#                                                 hack_prune_to_these_chans=hack_prune_to_these_chans,\n",
    "#                                                 remove_singleprims_unstable=remove_singleprims_unstable)\n",
    "\n",
    "#                     assert False\n",
    "\n",
    "# Compact version of above\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_wrapper\n",
    "euclidian_time_resolved_wrapper(animal, date, DFallpa, \"/tmp/TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 4\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "NPCS_KEEP = 6\n",
    "\n",
    "twind_analy = (-0.4, 0.5)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "    for prune_version in [\"sp_char_0\", \"pig_char_0\", \"sp_char\", \"pig_char_1plus\"]:\n",
    "        for subspace_projection in [\"task_shape_si\"]:\n",
    "            for remove_drift in [False]:\n",
    "                for raw_subtract_mean_each_timepoint in [False]:\n",
    "                    for remove_singleprims_unstable in [True]:\n",
    "                        SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{bregion}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-subtrmean={raw_subtract_mean_each_timepoint}\"\n",
    "                        os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                        print(\"SAVING AT ... \", SAVEDIR)\n",
    "                        euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "                                                    tbin_dur, tbin_slide, \n",
    "                                                    subspace_projection, NPCS_KEEP, \n",
    "                                                    n_min_trials_per_shape = n_min_trials_per_shape, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                                    remove_singleprims_unstable=remove_singleprims_unstable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540d6ed",
   "metadata": {},
   "source": [
    "# [3] Samp, Go, reach onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/SAMP\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 4\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "twind_analy = (-1, 1)\n",
    "NPCS_KEEP = 7\n",
    "events_keep = [\"03_samp\", \"05_first_raise\"]\n",
    "\n",
    "HACK = True\n",
    "for i, row in DFallpa.iterrows():\n",
    "    bregion = row[\"bregion\"]\n",
    "    which_level = row[\"which_level\"]\n",
    "    event = row[\"event\"]\n",
    "    PA = row[\"pa\"]\n",
    "\n",
    "    if HACK and bregion not in [\"PMv\"]:\n",
    "        continue\n",
    "\n",
    "    if event in events_keep:\n",
    "        for prune_version in [\"sp_char\"]:\n",
    "            for subspace_projection in [None, \"pca\", \"task_shape\"]: # NOTE: shape_prims_single not great, you lose some part of preSMA context-dependence...\n",
    "                for remove_drift in [False]:\n",
    "                    for raw_subtract_mean_each_timepoint in [False]:\n",
    "                        for remove_singleprims_unstable in [False, True]:\n",
    "                            SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{which_level}-{bregion}-{event}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-subtrmean={raw_subtract_mean_each_timepoint}\"\n",
    "                            os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                            print(\"SAVING AT ... \", SAVEDIR)\n",
    "                            euclidian_time_resolved(animal, date, PA, which_level, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "                                                        tbin_dur, tbin_slide, \n",
    "                                                        subspace_projection, NPCS_KEEP, \n",
    "                                                        n_min_trials_per_shape = n_min_trials_per_shape, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                                        remove_singleprims_unstable=remove_singleprims_unstable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a5e36",
   "metadata": {},
   "source": [
    "# Time warping based on behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each trial, get beh stroke\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8385ea",
   "metadata": {},
   "source": [
    "# Checking drift of FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each trial.\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all\n",
    "plot_heatmap_firing_rates_all(PA, savedir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f627",
   "metadata": {},
   "source": [
    "### Sanity check, change over day?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = f\"{SAVEDIR_ANALYSIS}/drift\"\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681090c8",
   "metadata": {},
   "source": [
    "##### For each region, plot sm fr across trials and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all bregions\n",
    "assert False, \"run for both chans and PCs\"\n",
    "for i, row in DFallpa.iterrows():\n",
    "    PAthis = row[\"pa\"]\n",
    "    bregion = row[\"bregion\"]\n",
    "\n",
    "    dur = 0.1\n",
    "    slide = 0.02\n",
    "    PAthis = PAthis.agg_by_time_windows_binned(dur, slide)\n",
    "\n",
    "\n",
    "    from neuralmonkey.neuralplots.population import heatmapwrapper_stratified_each_neuron\n",
    "    fig = heatmapwrapper_stratified_each_neuron(PA, \"task_kind\")\n",
    "    assert False\n",
    "    assert False\n",
    "    savefig(fig, f\"{savedir}/{bregion}.png\")\n",
    "    plt.close(\"all\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR = f\"/tmp/CHAR_HEATMAPS\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) First, dim reduction\n",
    "superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "\n",
    "dim_red_method = \"pca\"\n",
    "twind_analy = (-0.15, 0.3)\n",
    "tbin_dur = 0.1\n",
    "tbin_slice = 0.01\n",
    "NPCS_KEEP = 10\n",
    "_, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "                                twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "                                NPCS_KEEP = NPCS_KEEP,\n",
    "                                dpca_var = None, dpca_vars_group = None, dpca_filtdict=None, \n",
    "                                dpca_proj_twind = twind_analy, \n",
    "                                raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                umap_n_components=None, umap_n_neighbors=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d50581",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Script for heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16207999",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import params_subspace_projection\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all, plot_heatmap_firing_rates_all_wrapper\n",
    "\n",
    "plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR, animal, date)\n",
    "\n",
    "\n",
    "# bregion = \"FP_p\"\n",
    "# prune_version = \"sp_char_0\"\n",
    "# n_min_trials_per_shape = 5\n",
    "# raw_subtract_mean_each_timepoint = False\n",
    "# ### FINAL -- script over all\n",
    "# LIST_SS_PRUNE = [\n",
    "#     (None, False),\n",
    "#     (\"pca\", False),\n",
    "#     (\"shape_prims_single\", False),\n",
    "#     (\"pca\", True),\n",
    "#     (\"shape_prims_single\", True),\n",
    "# ]\n",
    "# PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "# ################# PREPROCESS\n",
    "# savedir = f\"{SAVEDIR}/preprocess\"\n",
    "# os.makedirs(savedir, exist_ok=True)\n",
    "# plot_drawings = False\n",
    "# PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "#                 n_min_trials_per_shape=n_min_trials_per_shape, plot_drawings=plot_drawings)\n",
    "\n",
    "# for subspace_projection, prune_chans in LIST_SS_PRUNE:\n",
    "\n",
    "#     savedir = f\"{SAVEDIR}/ss={subspace_projection}-prunedrift={prune_chans}\"\n",
    "#     os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "#     ########### PRUNE CHANS\n",
    "#     if prune_chans:\n",
    "#         # Optioanlly, remove channels with drift\n",
    "#         from neuralmonkey.classes.population_mult import dfallpa_preprocess_sitesdirty_single_just_drift\n",
    "#         PA = dfallpa_preprocess_sitesdirty_single_just_drift(PA, animal, date)\n",
    "\n",
    "#     ########### DIM REDUCTIONS\n",
    "#     if subspace_projection is not None:\n",
    "#         dim_red_method, superv_dpca_params = params_subspace_projection(subspace_projection)\n",
    "#         superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "#         superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "#         superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "#         twind_analy = (-0.2, 0.35)\n",
    "#         tbin_dur = 0.1\n",
    "#         tbin_slice = 0.01\n",
    "#         NPCS_KEEP = 10\n",
    "#         savedirthis = f\"{savedir}/dimred-{subspace_projection}\"\n",
    "#         os.makedirs(savedirthis, exist_ok=True)\n",
    "#         _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedirthis, \n",
    "#                                         twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "#                                         NPCS_KEEP = NPCS_KEEP,\n",
    "#                                         dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_filtdict=superv_dpca_filtdict, \n",
    "#                                         dpca_proj_twind = twind_analy, \n",
    "#                                         raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "#                                         umap_n_components=None, umap_n_neighbors=None)\n",
    "#     else:\n",
    "#         PAredu = PA\n",
    "\n",
    "#     savedirthis = f\"{savedir}/heatmaps-{subspace_projection}/{bregion}\"\n",
    "#     os.makedirs(savedirthis, exist_ok=True)\n",
    "#     plot_heatmap_firing_rates_all(PAredu, savedirthis)\n",
    "\n",
    "#     plt.close(\"all\")\n",
    "\n",
    "\n",
    "# # ################## [RAW]\n",
    "# # savedir = f\"{SAVEDIR}/heatmaps_raw/{bregion}\"\n",
    "# # os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# # plot_heatmap_firing_rates_all(PA, savedir)\n",
    "\n",
    "# # ################# [PCA]\n",
    "# # dim_red_method = \"pca\"\n",
    "# # twind_analy = (-0.15, 0.3)\n",
    "# # tbin_dur = 0.1\n",
    "# # tbin_slice = 0.01\n",
    "# # NPCS_KEEP = 10\n",
    "# # superv_dpca_var = None\n",
    "# # superv_dpca_vars_group = None\n",
    "# # superv_dpca_filtdict = None\n",
    "# # _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "# #                                 twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "# #                                 NPCS_KEEP = NPCS_KEEP,\n",
    "# #                                 dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_filtdict=superv_dpca_filtdict, \n",
    "# #                                 dpca_proj_twind = twind_analy, \n",
    "# #                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "# #                                 umap_n_components=None, umap_n_neighbors=None)\n",
    "\n",
    "# # savedir = f\"{SAVEDIR}/heatmaps_pca/{bregion}\"\n",
    "# # os.makedirs(savedir, exist_ok=True)\n",
    "# # plot_heatmap_firing_rates_all(PAredu, savedir)\n",
    "\n",
    "\n",
    "# # ################# [DPCA]\n",
    "# # dim_red_method = \"superv_dpca\"\n",
    "# # superv_dpca_params={\n",
    "# #     \"superv_dpca_var\":\"shape_semantic\",\n",
    "# #     \"superv_dpca_vars_group\":None,\n",
    "# #     \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_single\"]}\n",
    "# # }\n",
    "# # superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "# # superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "# # superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "# # NPCS_KEEP = 6\n",
    "# # _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "# #                                 twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "# #                                 NPCS_KEEP = NPCS_KEEP,\n",
    "# #                                 dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_filtdict=superv_dpca_filtdict, \n",
    "# #                                 dpca_proj_twind = twind_analy, \n",
    "# #                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "# #                                 umap_n_components=None, umap_n_neighbors=None)\n",
    "# # savedir = f\"{SAVEDIR}/heatmaps_dpca/{bregion}\"\n",
    "# # os.makedirs(savedir, exist_ok=True)\n",
    "# # plot_heatmap_firing_rates_all(PAredu, savedir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94190db2",
   "metadata": {},
   "source": [
    "### Accounting for drift -- exclude cases with very different PSTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff745869",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_clean_stable_single_prims_frate\n",
    "savedir = \"/tmp/test\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "preprocess_clean_stable_single_prims_frate(PA, savedir=savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab18d69",
   "metadata": {},
   "source": [
    "##### Re-apply filter based on dirty sites, being more strict about drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each grouping, get the meanPSTH, and c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae030b8",
   "metadata": {},
   "source": [
    "# MULT DATA - euclidian_time_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5204605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER, _REGIONS_IN_ORDER_COMBINED\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b801cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import  load_all_dates\n",
    "# Diego (trial, all)\n",
    "LIST_ANIMAL_DATE_COMB = [\n",
    "    (\"Diego\", 231122, True),\n",
    "    (\"Diego\", 231128, True),\n",
    "    (\"Diego\", 231129, True),\n",
    "    (\"Diego\", 231201, True),\n",
    "    (\"Diego\", 231204, True),\n",
    "    (\"Diego\", 231205, True),\n",
    "    (\"Diego\", 231211, True),\n",
    "    (\"Diego\", 231213, True),\n",
    "]\n",
    "which_level = \"trial\"\n",
    "savedir_method_old = False\n",
    "\n",
    "DFDIST = load_all_dates(LIST_ANIMAL_DATE_COMB, which_level, savedir_method_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55770662",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For plots, see analy_euclidian_chars_sp_MULT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad33c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import  plot_scalar_all\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620034e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_scalar = [-0.3, 0.1]\n",
    "DFDISTthis = DFDIST[(DFDIST[\"time_bin\"]>=twind_scalar[0]) & (DFDIST[\"time_bin\"]<=twind_scalar[1])].reset_index(drop=True)\n",
    "\n",
    "# Agg, averaging over time\n",
    "DFTHISscal = aggregGeneral(DFDISTthis, [\"animal\", \"date\", \"combine_areas\", \"event\", \"bregion\", \"metaparams\", \"same-task|shape\", \"prune_version\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                \"remove_singleprims_unstable\"], values=[\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_event_to_twind = {\n",
    "    \"03_samp\":[0.2, 1.0],\n",
    "    \"05_first_raise\":[-0.5,  0],\n",
    "}\n",
    "\n",
    "list_df = []\n",
    "for event, twind_scalar in map_event_to_twind.items():\n",
    "    dfthis = DFDIST[DFDIST[\"event\"] == event]\n",
    "    dfthis_sub = dfthis[(dfthis[\"time_bin\"]>=twind_scalar[0]-0.001) & (dfthis[\"time_bin\"]<=twind_scalar[1]+0.001)].reset_index(drop=True)\n",
    "    list_df.append(dfthis_sub)\n",
    "DFDISTthis = pd.concat(list_df).reset_index(drop=True)\n",
    "\n",
    "# Agg, averaging over time\n",
    "DFTHISscal = aggregGeneral(DFDISTthis, [\"animal\", \"date\", \"combine_areas\", \"event\", \"bregion\", \"metaparams\", \"same-task|shape\", \"prune_version\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                \"remove_singleprims_unstable\"], values=[\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.expttools import writeDictToTxt\n",
    "writeDictToTxt(map_event_to_twind, f\"{savedir}/twind_params.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67423387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final scalar plot\n",
    "DFTHISscal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/tmp/SCALAR\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFTHISscal, [\"event\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                                                \"remove_singleprims_unstable\"])\n",
    "for (event, subspace_projection, remove_drift, raw_subtract_mean_each_timepoint, remove_singleprims_unstable), inds in grpdict.items():\n",
    "    dfthis = DFTHISscal.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"same-task|shape\", \"0|1\", \"1|0\", \n",
    "                                        \"prune_version\", \"dist_yue_diff\", \"bregion\", \n",
    "                                        True, shareaxes=True, SIZE=4);\n",
    "    savefig(fig, f\"{savedir}/EVENT={event}-ss={subspace_projection}-rmvdrift={remove_drift}-subtrmean={raw_subtract_mean_each_timepoint}-rmvunstable={remove_singleprims_unstable}.pdf\")\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bafbb",
   "metadata": {},
   "source": [
    "##### Convert to scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_scalar = [-0.3, 0.1]\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import plot_scalar_all\n",
    "plot_scalar_all(DFDIST, \"/tmp\", twind_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d389c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e25171",
   "metadata": {},
   "source": [
    "### Combine all kinds of pairwise comparisons in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "savedir = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFTHISscal, [\"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                                                   \"remove_singleprims_unstable\"])\n",
    "for (subspace_projection, remove_drift, raw_subtract_mean_each_timepoint, remove_singleprims_unstable), inds in grpdict.items():\n",
    "    dfthis = DFTHISscal.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"same-task|shape\", \"0|1\", \"1|0\", \n",
    "                                        \"metaparams\", \"dist_yue_diff\", \"bregion\", \n",
    "                                        True, shareaxes=True, SIZE=4);    \n",
    "    savefig(fig, f\"{savedir}/ss={subspace_projection}-rmvdrift={remove_drift}-subtrmean={raw_subtract_mean_each_timepoint}-rmvunstable={remove_singleprims_unstable}\")\n",
    "    plt.close(\"all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38710bc",
   "metadata": {},
   "source": [
    "# Example figures (for paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b557cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d154fd",
   "metadata": {},
   "source": [
    "### [Code] plot_heatmap_firing_rates_all_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n",
    "PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "print(PA.X.shape)\n",
    "\n",
    "prune_version = \"sp_char_0\"\n",
    "n_min_trials_per_shape = 4\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "plot_drawings = False\n",
    "# twind_analy = (-0.4, 0.5)\n",
    "twind_analy = (-0.35, 0.3)\n",
    "tbin_dur=0.1\n",
    "tbin_slide=0.01\n",
    "\n",
    "NPCS_KEEP = 10\n",
    "subspace_projection = \"pca\"\n",
    "# NPCS_KEEP = None\n",
    "# subspace_projection = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "savedir = \"/tmp\"\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                    n_min_trials_per_shape=n_min_trials_per_shape, shape_var=shape_var, plot_drawings=plot_drawings,\n",
    "                    remove_chans_fr_drift=False,\n",
    "                    subspace_projection=subspace_projection, \n",
    "                        twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                        raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                        remove_singleprims_unstable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a34533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all_wrapper\n",
    "import os\n",
    "SAVEDIR_ANALYSIS = \"/tmp/HEATMAP_WRAPPER\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR_ANALYSIS, animal, date, \n",
    "                                      DEBUG_skip_drawings=True, DEBUG_bregion=\"PMv\",\n",
    "                                      DEBUG_subspace_projection=\"pca_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b834c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import load_session_helper, load_mult_session_helper\n",
    "MS = load_mult_session_helper(date, animal)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import beh_plot_event_timing_stroke\n",
    "savedir = \"/tmp\"\n",
    "PA = DFallpa.iloc[0][\"pa\"]\n",
    "beh_plot_event_timing_stroke(PA, animal, date, savedir, MS=MS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735faca3",
   "metadata": {},
   "source": [
    "# [GOOD] Euclidian distance, using new fast method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca326e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_ANALYSIS = f\"/tmp/EUCL_FAST\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa = DFallpa.loc[[1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_fast_shuffled\n",
    "euclidian_time_resolved_fast_shuffled(DFallpa, animal, date, SAVEDIR_ANALYSIS, DO_RSA_HEATMAPS=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2320d",
   "metadata": {},
   "source": [
    "### [MULT DAYS] for: euclidian_time_resolved_fast_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidian_time_resolved_fast_shuffled_mult_scatter_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac18ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (1) Original plots, still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_kind = \"char_sp\"\n",
    "euclidian_time_resolved_fast_shuffled_mult_scatter_plots(analysis_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b547778",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (2) Stats, and scatter plots. Good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _euclidianshuff_stats_linear_load_mult_dates_postprocess, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidian_time_resolved_fast_shuffled_mult_reload, _euclidian_time_resolved_fast_shuffled_mult_scatter_plots_params\n",
    "from pythonlib.tools.pandastools import convert_var_to_categorical\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidianshuff_stats_linear_load, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n",
    "from pythonlib.tools.pandastools import convert_var_to_categorical\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from neuralmonkey.neuralplots.brainschematic import datamod_reorder_by_bregion\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from pythonlib.tools.statstools import signrank_wilcoxon_from_df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidianshuff_stats_linear_load, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n",
    "import os\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _euclidianshuff_stats_linear_load_mult_dates_postprocess, euclidianshuff_stats_linear_load, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n",
    "\n",
    "merge_pancho_ss_twinds=False\n",
    "plot_coeff = False\n",
    "analysis_kind = \"char_sp\"\n",
    "do_each_date = False\n",
    "var_effect = \"shape_semantic_grp\"\n",
    "\n",
    "# for pruning.\n",
    "list_prune_version = [\"sp_char_0\"]\n",
    "list_subspace = [\"task_shape_si\"]\n",
    "for animal in [\"Pancho\", \"Diego\"]:\n",
    "# for animal in [\"Diego\"]:\n",
    "    for var_other in [\"task_kind\"]:\n",
    "\n",
    "        list_date = _euclidian_time_resolved_fast_shuffled_mult_scatter_plots_params(analysis_kind, animal, var_other)\n",
    "        var_same_same = f\"same-{var_effect}|{var_other}\"\n",
    "\n",
    "        if do_each_date:\n",
    "            for date in list_date:\n",
    "                DFDISTS, SAVEDIR_PLOTS = euclidianshuff_stats_linear_load(animal, date, var_other, merge_pancho_ss_twinds=merge_pancho_ss_twinds)\n",
    "\n",
    "                ### ALL PLOTS\n",
    "                euclidianshuff_stats_linear_plot_wrapper(DFDISTS, SAVEDIR_PLOTS, var_other)\n",
    "\n",
    "        from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidianshuff_stats_linear_load_mult_dates\n",
    "        DFDISTS = euclidianshuff_stats_linear_load_mult_dates(animal, list_date, var_other, analysis_kind, merge_pancho_ss_twinds=merge_pancho_ss_twinds)\n",
    "\n",
    "        SAVEDIR_PLOTS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/EUCL_QUICK_SHUFFLE/MULT/stats_linear_model/{animal}-{'|'.join([str(d) for d in list_date])}-var_other={var_other}\"\n",
    "        os.makedirs(SAVEDIR_PLOTS, exist_ok=True)\n",
    "\n",
    "        # Condition\n",
    "        # For now, just plot the ones that you care about (or else takes a long time)\n",
    "        print(len(DFDISTS))\n",
    "        DFDISTS = DFDISTS[DFDISTS[\"prune_version\"].isin(list_prune_version)].reset_index(drop=True)\n",
    "        print(len(DFDISTS))\n",
    "\n",
    "        print(len(DFDISTS))\n",
    "        DFDISTS = DFDISTS[DFDISTS[\"subspace_projection\"].isin(list_subspace)].reset_index(drop=True)\n",
    "        print(len(DFDISTS))\n",
    "\n",
    "        DFDISTS = _euclidianshuff_stats_linear_load_mult_dates_postprocess(DFDISTS)\n",
    "\n",
    "\n",
    "        # Savedir\n",
    "        from pythonlib.tools.expttools import writeDictToTxt\n",
    "        writeDictToTxt({\"list_date\":list_date}, f\"{SAVEDIR_PLOTS}/list_date.txt\")\n",
    "        _, _, _, _ = euclidianshuff_stats_linear_plot_wrapper(DFDISTS, SAVEDIR_PLOTS, var_other, var_effect=var_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb9af1",
   "metadata": {},
   "source": [
    "##### Code here (devo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58726d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR_ANALYSIS = \"/tmp/CHAR_SP\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False, \"save split, or else file gets to mult GB -- run seprately for each bregion.\"\n",
    "var_effect=\"shape_semantic_grp\"\n",
    "var_conj = \"task_kind\"\n",
    "vars_group = [var_effect, var_conj]\n",
    "N_MIN_TRIALS_PER_SHAPE = 4\n",
    "TWIND_ANALY = (-0.4, 0.5)\n",
    "NPCS_KEEP = 6\n",
    "DO_RSA_HEATMAPS = False\n",
    "\n",
    "SUBSPACE_PROJ_FIT_TWIND = {\n",
    "    \"00_stroke\":[(-0.5, -0.05), (0.05, 0.5), (-0.5, 0.5), (-0.4, 0.3)],\n",
    "}\n",
    "\n",
    "# LIST_SUBSPACE_PROJECTION = [None, \"pca_proj\", \"task_shape_si\", \"shape_prims_single\"]\n",
    "LIST_SUBSPACE_PROJECTION = [\"shape_prims_single\"]\n",
    "LIST_PRUNE_VERSION = [\"sp_char\"] # GOOD\n",
    "\n",
    "N_SPLITS = 2\n",
    "\n",
    "twind_analy = TWIND_ANALY\n",
    "tbin_dur = 0.2\n",
    "tbin_slide = 0.02\n",
    "\n",
    "map_event_to_listtwind_scal = {\n",
    "    # \"00_stroke\":[(-0.5, -0.05), (0.05, 0.5), (-0.3, 0.1)],\n",
    "    # \"00_stroke\":[(-0.5, -0.05), (-0.3, 0.1), (-0.3, 0.2), (-0.4, 0.3), (-0.2, 0.3), (0.05, 0.5)],\n",
    "    \"00_stroke\":[(0.05, 0.5)],\n",
    "    }\n",
    "\n",
    "list_dfdist =[]\n",
    "for _, row in DFallpa.iterrows():\n",
    "    bregion = row[\"bregion\"]\n",
    "    which_level = row[\"which_level\"]\n",
    "    event = row[\"event\"]\n",
    "    PA = row[\"pa\"]\n",
    "\n",
    "    for prune_version in LIST_PRUNE_VERSION:\n",
    "\n",
    "        for subspace_projection in LIST_SUBSPACE_PROJECTION:\n",
    "            # plot only cleaned up data.\n",
    "            list_unstable_badstrokes = [(True, True, True)]\n",
    "                \n",
    "            # for remove_drift in [False]:\n",
    "            for remove_drift, remove_singleprims_unstable, remove_trials_with_bad_strokes in list_unstable_badstrokes:\n",
    "\n",
    "                ############################\n",
    "                if subspace_projection in [None, \"pca\"]:\n",
    "                    list_fit_twind = [twind_analy]\n",
    "                else:\n",
    "                    list_fit_twind = SUBSPACE_PROJ_FIT_TWIND[event]\n",
    "                \n",
    "                for subspace_projection_fitting_twind in list_fit_twind:\n",
    "                    \n",
    "                    # Final save dir\n",
    "                    SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{which_level}-{bregion}-{event}--prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-RmBadStrks={remove_trials_with_bad_strokes}-fit_twind={subspace_projection_fitting_twind}\"\n",
    "                    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                    print(\"SAVING AT ... \", SAVEDIR)\n",
    "\n",
    "                    if DO_RSA_HEATMAPS:\n",
    "                        # Plot pairwise distances (rsa heatmaps).\n",
    "                        # This is done separatee to below becuase it doesnt use the train-test splits.\n",
    "                        # It shold but I would have to code way to merge multple Cl, which is doable.\n",
    "                        from neuralmonkey.analyses.euclidian_distance import timevarying_compute_fast_to_scalar\n",
    "\n",
    "                        # PAthis = preprocess_pa(PA, animal, date, var_other, None, remove_drift, \n",
    "                        #                        subspace_projection, subspace_projection_fitting_twind, \n",
    "                        #                        twind_analy, tbin_dur, tbin_slide, raw_subtract_mean_each_timepoint=False,\n",
    "                        #                        skip_dim_reduction=False)\n",
    "                        \n",
    "                        PAthis = preprocess_pa(animal, date, PA, None, prune_version, \n",
    "                                            n_min_trials_per_shape=N_MIN_TRIALS_PER_SHAPE, shape_var=var_effect, plot_drawings=False,\n",
    "                                            remove_chans_fr_drift=remove_drift, subspace_projection=subspace_projection, \n",
    "                                                twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                                                raw_subtract_mean_each_timepoint=False, remove_singleprims_unstable=remove_singleprims_unstable,\n",
    "                                                remove_trials_with_bad_strokes=remove_trials_with_bad_strokes, \n",
    "                                                subspace_projection_fitting_twind=subspace_projection_fitting_twind)\n",
    "                        \n",
    "                        list_twind_scalar = map_event_to_listtwind_scal[event]\n",
    "                        for twind_scal in list_twind_scalar:\n",
    "                            savedir = f\"{SAVEDIR}/rsa_heatmap/twindscal={twind_scal}\"\n",
    "                            os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                            # Prune to scalar window\n",
    "                            pa = PAthis.slice_by_dim_values_wrapper(\"times\", twind_scal)\n",
    "\n",
    "                            # Make rsa heatmaps.\n",
    "                            timevarying_compute_fast_to_scalar(pa, vars_group, rsa_heatmap_savedir=savedir)\n",
    "\n",
    "                    # Preprocess\n",
    "                    savedir = f\"{SAVEDIR}/preprocess\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    skip_dim_reduction = True # will do so below... THis just do other preprocessing, and widowing\n",
    "                    PAthis = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                                        n_min_trials_per_shape=N_MIN_TRIALS_PER_SHAPE, shape_var=var_effect, plot_drawings=False,\n",
    "                                        remove_chans_fr_drift=remove_drift, subspace_projection=subspace_projection, \n",
    "                                            twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                                            raw_subtract_mean_each_timepoint=False, remove_singleprims_unstable=remove_singleprims_unstable,\n",
    "                                            remove_trials_with_bad_strokes=remove_trials_with_bad_strokes, \n",
    "                                            subspace_projection_fitting_twind=subspace_projection_fitting_twind,\n",
    "                                            skip_dim_reduction=skip_dim_reduction)\n",
    "\n",
    "\n",
    "                    ########### DO TRAIN-TEST SPLITS\n",
    "                    folds_dflab = PAthis.split_balanced_stratified_kfold_subsample_level_of_var(vars_group, None, None, \n",
    "                                                                                                n_splits=N_SPLITS, \n",
    "                                                                                                do_balancing_of_train_inds=False)\n",
    "\n",
    "                    for _i_dimredu, (train_inds, test_inds) in enumerate(folds_dflab):\n",
    "                        # train_inds, more inds than than test_inds\n",
    "                        train_inds = [int(i) for i in train_inds]\n",
    "                        test_inds = [int(i) for i in test_inds]\n",
    "\n",
    "                        ############# DO DIM REDUCTION\n",
    "                        from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _preprocess_pa_dim_reduction\n",
    "                        PAthisRedu = _preprocess_pa_dim_reduction(PAthis, subspace_projection, subspace_projection_fitting_twind,\n",
    "                                twind_analy, tbin_dur, tbin_slide, savedir=None, raw_subtract_mean_each_timepoint=False,\n",
    "                                inds_pa_fit=test_inds, inds_pa_final=train_inds)\n",
    "\n",
    "                        if PAthisRedu is None:\n",
    "                            print(\"SKIPPING, since PAthisRedu is None: \", SAVEDIR)\n",
    "                            assert False\n",
    "                        else:\n",
    "                            # Take different windows (for computing scalar score)\n",
    "                            # Go thru diff averaging windows (to get scalar)\n",
    "                            list_twind_scalar = map_event_to_listtwind_scal[event]\n",
    "                            for twind_scal in list_twind_scalar:\n",
    "                                \n",
    "                                pa = PAthisRedu.slice_by_dim_values_wrapper(\"times\", twind_scal)\n",
    "\n",
    "                                # ###################################### Running euclidian\n",
    "                                # from neuralmonkey.analyses.euclidian_distance import timevarying_compute_fast_to_scalar\n",
    "                                \n",
    "                                # # (1) Data\n",
    "                                # dfdist, _ = timevarying_compute_fast_to_scalar(pa, label_vars=vars_group)\n",
    "                                \n",
    "                                # dfdist[\"bregion\"] = bregion\n",
    "                                # dfdist[\"prune_version\"] = prune_version\n",
    "                                # dfdist[\"which_level\"] = which_level\n",
    "                                # dfdist[\"event\"] = event\n",
    "                                # dfdist[\"subspace_projection\"] = subspace_projection\n",
    "                                # dfdist[\"subspace_projection_fitting_twind\"] = [subspace_projection_fitting_twind for _ in range(len(dfdist))]\n",
    "                                # dfdist[\"dim_redu_fold\"] = _i_dimredu\n",
    "                                # dfdist[\"twind_scal\"] = [twind_scal for _ in range(len(dfdist))]\n",
    "                                # list_dfdist.append(dfdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PAthis.Xlabels[\"trials\"]\n",
    "\n",
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap\n",
    "grouping_plot_n_samples_conjunction_heatmap(dflab, \"shape_semantic_grp\", \"task_kind\", [\"stroke_index\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30155222",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_projection_fitting_twindsubspace_projection_fitting_twind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAthisRedu = _preprocess_pa_dim_reduction(PAthis, subspace_projection, subspace_projection_fitting_twind,\n",
    "        twind_analy, tbin_dur, tbin_slide, savedir=None, raw_subtract_mean_each_timepoint=False,\n",
    "        inds_pa_fit=test_inds, inds_pa_final=train_inds)\n",
    "print(PAthisRedu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5474aba",
   "metadata": {},
   "source": [
    "# [OLDER] Euclidian timecourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR_ANALYSIS = f\"/tmp/EUCL_TIMECOURSE\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_wrapper\n",
    "euclidian_time_resolved_wrapper(animal, date, DFallpa, SAVEDIR_ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efc4f7",
   "metadata": {},
   "source": [
    "# Cross-temporal scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7466e",
   "metadata": {},
   "source": [
    "Consider each timebin for char vs. each timebin for SP.\n",
    "\n",
    "For each, get scores of diff|same, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - Reshape PA so that time is on the trial axis.\n",
    "from neuralmonkey.classes.population import concatenate_popanals\n",
    "\n",
    "list_pa = []\n",
    "list_labels = []\n",
    "for i in range(len(PAredu.Times)):\n",
    "    pathis = PAredu.slice_by_dim_indices_wrapper(\"times\", [i])\n",
    "\n",
    "    # collect infoextract_snippets_trials\n",
    "    list_pa.append(pathis)\n",
    "    list_labels.append(i)\n",
    "\n",
    "PAreduScal = concatenate_popanals(list_pa, dim=\"trials\", \n",
    "                                map_idxpa_to_value=list_labels, \n",
    "                                map_idxpa_to_value_colname=\"time_bin\",\n",
    "                                assert_otherdims_have_same_values=False,\n",
    "                                times_realign_so_first_index_is_this_time=0)\n",
    "PAreduScal.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cldist = PAreduScal.dataextract_as_distance_matrix_clusters_flex([\"task_kind\", \"shape_semantic\", \"time_bin\"], \n",
    "                                                             return_as_single_mean_over_time=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8134eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres = Cldist.rsa_distmat_score_all_pairs_of_label_groups(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e21451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres[\"task_kind_same\"] = dfres[\"task_kind_1\"] == dfres[\"task_kind_2\"]\n",
    "dfres[\"shape_semantic_same\"] = dfres[\"shape_semantic_1\"] == dfres[\"shape_semantic_2\"]\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_same\", \"task_kind_same\"], \"shape_task_same\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_pair\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_1\", \"shape_semantic_2\"], \"shape_semantic_pair\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"time_bin_1\", \"time_bin_2\"], \"time_bin_pair\")\n",
    "\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_same\", \"task_kind_1\", \"task_kind_2\"], \"ss_task_kind_pair\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c147809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f55380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subplots_heatmap(dfres, \"time_bin_1\", \"time_bin_2\", \"dist_yue_diff\", \"ss_task_kind_pair\", share_zlim=True)\n",
    "plot_subplots_heatmap(dfres, \"time_bin_1\", \"time_bin_2\", \"dist_mean\", \"ss_task_kind_pair\", share_zlim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9641ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take difference across subplots, plotting a new subplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ca7cd",
   "metadata": {},
   "source": [
    "# Check motor similarity of strokes from SP vs. CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "\n",
    "savedir = \"/tmp/PREPROCESS\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "PA = preprocess_pa(PA, savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79e0a8",
   "metadata": {},
   "source": [
    "# Collect and plot across days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_SAVE = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/MULT\"\n",
    "\n",
    "list_animal_date = [\n",
    "    # (\"Pancho\", 230112),\n",
    "    # (\"Pancho\", 230117),\n",
    "    # (\"Pancho\", 230118),\n",
    "    (\"Pancho\", 230119),\n",
    "    (\"Pancho\", 230120),\n",
    "    (\"Pancho\", 230122),\n",
    "    (\"Pancho\", 230125),\n",
    "    (\"Pancho\", 230126),\n",
    "    (\"Pancho\", 230127),\n",
    "    ]\n",
    "\n",
    "combine = True\n",
    "ANIMAL = \"Pancho\"\n",
    "\n",
    "LIST_NPCS_KEEP = [4,6,2]\n",
    "for twind_analy in [(0.05, 0.25), (-0.05, 0.35), (0.1, 0.2)]:\n",
    "    for subspace_projection in [\"shape_prims_single\", \"pca\"]:\n",
    "        for prune_version in [\"sp_char_0\", \"sp_char\"]:\n",
    "            for NPCS_KEEP in LIST_NPCS_KEEP:\n",
    "                for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "\n",
    "                    # subspace_projection = \"shape_prims_single\"\n",
    "                    # prune_version = \"sp_char_0\"\n",
    "                    # NPCS_KEEP = 4\n",
    "                    # raw_subtract_mean_each_timepoint = False\n",
    "                    # twind_analy = (0.05, 0.25)\n",
    "\n",
    "                    try:\n",
    "                        ### Load all data for this params configuration\n",
    "                        list_dfres = []\n",
    "                        for animal, date in list_animal_date:\n",
    "                            if animal==ANIMAL:\n",
    "                                # animal = \"Pancho\"\n",
    "                                # date = 230126\n",
    "\n",
    "                                SAVEDIR_ANALYSIS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/{animal}-{date}-combine={combine}\"\n",
    "                                path = f\"{SAVEDIR_ANALYSIS}/subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}/summary/DFRES.pkl\"\n",
    "                                dfres = pd.read_pickle(path)\n",
    "\n",
    "                                dfres[\"animal\"] = animal\n",
    "                                dfres[\"date\"] = date\n",
    "\n",
    "                                list_dfres.append(dfres)\n",
    "                    except FileNotFoundError as err:\n",
    "                        print(\"Skipping, did not find all data...\", path)\n",
    "                        continue\n",
    "\n",
    "                    ######### PLOTS\n",
    "                    savedir = f\"{SAVEDIR_SAVE}/{ANIMAL}/combine={combine}-subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    DFRES = pd.concat(list_dfres).reset_index(drop=True)\n",
    "\n",
    "                    from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "                    DFRES = append_col_with_grp_index(DFRES, [\"animal\", \"date\"], \"ani_dat\")\n",
    "\n",
    "                    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", \"bregion\", \n",
    "                                                                    \"dist_yue_diff\", \"ani_dat\", True, SIZE=3, shareaxes=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-1.pdf\")\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", None, \n",
    "                                                                    \"dist_yue_diff\", \"bregion\", True, SIZE=4, shareaxes=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-2.pdf\")\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", \"bregion\", \n",
    "                                                                    \"dist_yue_diff\", \"ani_dat\", False, SIZE=6, shareaxes=True,\n",
    "                                                                    color_by_var_datapt=True, alpha=0.5,\n",
    "                                                                    force_all_on_same_axis=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-3.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=DFRES, x = \"bregion\", hue=\"shape_task_same\", y=\"dist_yue_diff\", kind=\"bar\", aspect=2)\n",
    "                    savefig(fig, f\"{savedir}/catplot-1.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=DFRES, x = \"bregion\", hue=\"ani_dat\", y=\"dist_yue_diff\", kind=\"point\", aspect=2, col=\"shape_task_same\", col_wrap=2)\n",
    "                    savefig(fig, f\"{savedir}/catplot-2.pdf\")\n",
    "\n",
    "\n",
    "                    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4440d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f403184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [MULT DAYS]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drag2_matlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
