{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Consolidates everything from all prior tutorials, except \"euclidian stuff\" (for that see 240410_kedar_euclidia...)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b248d33aff307a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d20c8",
   "metadata": {},
   "source": [
    "# Load DFallPa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14525eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Method: loading functrion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5608555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010699ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# date = 220614\n",
    "# combine = True\n",
    "# question = \"CHAR_BASE_trial\"\n",
    "# version = \"trial\"\n",
    "\n",
    "# animal = \"Pancho\"\n",
    "# date = 220624\n",
    "# combine = True\n",
    "# question = \"CHAR_BASE_stroke\"\n",
    "# version = \"stroke\"\n",
    "\n",
    "animal = \"Diego\"\n",
    "date = 231220\n",
    "# date = 231122\n",
    "combine = True\n",
    "question = \"CHAR_BASE_stroke\"\n",
    "version = \"stroke\"\n",
    "\n",
    "DFallpa = load_handsaved_wrapper(animal, date, version=version, combine_areas=combine, question=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a29f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.metadat.analy.anova_params import params_getter_euclidian_vars\n",
    "from neuralmonkey.classes.population_mult import dfpa_concatbregion_preprocess_clean_bad_channels, dfpa_concatbregion_preprocess_wrapper\n",
    "\n",
    "# LIST_VAR, LIST_VARS_OTHERS, LIST_CONTEXT, LIST_PRUNE_MIN_N_LEVS, LIST_FILTDICT = params_getter_euclidian_vars(question)\n",
    "\n",
    "# Make a copy of all PA before normalization\n",
    "dfpa_concatbregion_preprocess_wrapper(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757712e2",
   "metadata": {},
   "source": [
    "##### Do the standard preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086306d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_dfallpa_prune_chans\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_dfallpa_prune_chans_hand_coded\n",
    "SAVEDIR_ANALYSIS = \"/tmp\"\n",
    "preprocess_dfallpa_prune_chans(DFallpa, animal, date, SAVEDIR_ANALYSIS)\n",
    "preprocess_dfallpa_prune_chans_hand_coded(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4a8db",
   "metadata": {},
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ff3f2",
   "metadata": {},
   "source": [
    "##### Make sure PIG uses clean strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "dflab = PA.Xlabels[\"trials\"]\n",
    "\n",
    "from pythonlib.tools.pandastools import grouping_print_n_samples\n",
    "# grouping_print_n_samples(dflab, [\"task_kind\", \"shape_semantic\", \"clust_sim_max_colname\"])\n",
    "grouping_print_n_samples(dflab, [\"task_kind\", \"shape_semantic\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab[\"charclust_shape_seq\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28af46",
   "metadata": {},
   "source": [
    "##### Check, why bug in trialtimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73220ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "dflab = PA.Xlabels[\"trials\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af222a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab[\"trialcode_scal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c0b07",
   "metadata": {},
   "source": [
    "##### Diego, consolidate his shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339eaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "dflab = PA.Xlabels[\"trials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dflab[\"shape_semantic\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafba330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.drawmodel.tokens import MAP_SHAPESEM_TO_SHAPESEMGROUP, map_shsem_to_new_shsem\n",
    "\n",
    "# Consolidate Diego shapes\n",
    "dflab[\"shape_semantic\"] = [map_shsem_to_new_shsem[sh] if sh in map_shsem_to_new_shsem.keys() else sh for sh in dflab[\"shape_semantic\"]]\n",
    "\n",
    "# Sanity check that this doesnt change shape_sem_grp\n",
    "for _, row in dflab.iterrows():\n",
    "    assert MAP_SHAPESEM_TO_SHAPESEMGROUP[row[\"shape_semantic\"]] == row[\"shape_semantic_grp\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c1c6a",
   "metadata": {},
   "source": [
    "### Fixing code that checks for drifty neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/lemur2/lucas/Dropbox/SCIENCE/FREIWALD_LAB/DATA/Xuan/DFallpa-Diego-231218-stroke-kilosort_if_exists-norm=None-combine=True-t1=-1.0-t2=1.8-quest=CHAR_BASE_stroke.pkl\"\n",
    "path2 = \"/lemur2/lucas/Dropbox/SCIENCE/FREIWALD_LAB/DATA/Xuan/DFallpa-Diego-231218-trial-kilosort_if_exists-norm=None-combine=True-t1=-1.0-t2=1.8-quest=CHAR_BASE_trial.pkl\"\n",
    "\n",
    "import pickle\n",
    "with open(path1, \"rb\") as f:\n",
    "    dfallpa1 = pickle.load(f)\n",
    "\n",
    "with open(path2, \"rb\") as f:\n",
    "    dfallpa2 = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfallpa1[\"pa\"].values[1].Chans)\n",
    "print(dfallpa2[\"pa\"].values[1].Chans)\n",
    "print(DFallpa[\"pa\"].values[1].Chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# path = \"/lemur2/lucas/analyses/recordings/main/EXPORTED_BEH_DATA/DS/char_strokes_clusters/Diego/basis=Diego-shapes=None/231122/DS_data.pkl\"\n",
    "# path = \"/lemur2/lucas/analyses/recordings/main/EXPORTED_BEH_DATA/DS/char_strokes_clusters/Diego/basis=Diego-shapes=None/231128/DS_data.pkl\"\n",
    "path = \"/lemur2/lucas/analyses/recordings/main/EXPORTED_BEH_DATA/DS/char_strokes_clusters/Diego/basis=Diego-shapes=None/231206/DS_data.pkl\"\n",
    "with open(path, \"rb\") as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ad61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"task_kind\"]==\"character\"][\"shape_semantic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a40367",
   "metadata": {},
   "source": [
    "# Targeted dim reduction --> state space plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f4ce9",
   "metadata": {},
   "source": [
    "### (1) Compute, extract, save, make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa = DFallpa[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import targeted_dim_reduction_wrapper\n",
    "import os\n",
    "\n",
    "for var_effect in [\"shape_semantic_grp\", \"shape_semantic\"]:\n",
    "    LIST_VARIABLES = [\n",
    "        [\"task_kind\", \"stroke_index_is_first\", \"loc_on_clust_cat\", \"reach_angle_binned\", var_effect],\n",
    "    ]\n",
    "    # LIST_VARIABLES = [\n",
    "    #     [\"task_kind\", \"stroke_index_is_first\", \"loc_on_clust_cat\", var_effect],\n",
    "    #     [\"task_kind\", \"stroke_index_is_first\", var_effect],\n",
    "    #     [\"stroke_index_is_first\", var_effect],\n",
    "    # ]\n",
    "\n",
    "    for variables in LIST_VARIABLES:\n",
    "        variables_is_cat = [True for _ in range(len(variables))]\n",
    "\n",
    "        SAVEDIR_ANALYSIS = f\"/tmp/CHAR_SP_TARGETED_DR/vareff={var_effect}--varsregr={'|'.join(variables)}\"\n",
    "        os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "\n",
    "        print(SAVEDIR_ANALYSIS)\n",
    "        DFDIST = targeted_dim_reduction_wrapper(DFallpa, animal, date, SAVEDIR_ANALYSIS,\n",
    "                                                variables, variables_is_cat, var_effect = var_effect)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b7e46",
   "metadata": {},
   "source": [
    "### (2) Load mult dates, expts and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f67632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidian_time_resolved_fast_shuffled_mult_reload, _euclidian_time_resolved_fast_shuffled_mult_scatter_plots_params\n",
    "import pandas as pd\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Dataset params\n",
    "animal = \"Pancho\"\n",
    "combine = True\n",
    "\n",
    "# Plotting params\n",
    "yvar = \"dist_yue_diff\"\n",
    "MAKE_HEATMAP_DATAPTS = False # This can be slow\n",
    "\n",
    "# hard coded params\n",
    "analysis_kind = \"char_sp_00_stroke\"\n",
    "var_other = \"task_kind\"\n",
    "list_date = _euclidian_time_resolved_fast_shuffled_mult_scatter_plots_params(analysis_kind, animal, var_other)\n",
    "\n",
    "for var_effect in [\"shape_semantic_grp\", \"shape_semantic\"]:\n",
    "    # var_effect = \"shape_semantic_grp\"\n",
    "\n",
    "    LIST_VARIABLES = [\n",
    "        [\"task_kind\", \"stroke_index_is_first\", \"loc_on_clust_cat\", \"reach_angle_binned\", var_effect],\n",
    "        [\"task_kind\", \"stroke_index_is_first\", \"loc_on_clust_cat\", var_effect],\n",
    "        [\"task_kind\", \"stroke_index_is_first\", var_effect],\n",
    "        [\"stroke_index_is_first\", var_effect],\n",
    "    ]\n",
    "\n",
    "    for variables in LIST_VARIABLES:\n",
    "        variables_is_cat = [True for _ in range(len(variables))]\n",
    "\n",
    "        # variables = [\"task_kind\", \"stroke_index_is_first\", \"loc_on_clust_cat\", \"reach_angle_binned\", var_effect]\n",
    "        # variables_is_cat = [True for _ in range(len(variables))]\n",
    "\n",
    "        SAVEDIR_MULT = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/targeted_pca/MULT-combine={combine}/var={var_effect}--varsregr={'|'.join(variables)}/{animal}\"\n",
    "        os.makedirs(SAVEDIR_MULT, exist_ok=True)\n",
    "\n",
    "        ### LOAD, IF EXISTS\n",
    "        path = f\"{SAVEDIR_MULT}/DFDIST_FINAL.pkl\"\n",
    "        if os.path.exists(path):\n",
    "            DFDIST = pd.read_pickle(path)\n",
    "            with open(f\"{SAVEDIR_MULT}/DICT_DFDISTS.pkl\", \"rb\") as f:\n",
    "                DICT_DFDISTS = pickle.load(f)\n",
    "        else:\n",
    "            ### (1) load each dates data\n",
    "            list_dfdist = []\n",
    "            for date in list_date:\n",
    "                SAVEDIR_ANALYSIS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/targeted_pca/{animal}-{date}-combine={combine}/var={var_effect}--varsregr={'|'.join(variables)}\"\n",
    "\n",
    "                print(\"Loading: \", SAVEDIR_ANALYSIS)\n",
    "\n",
    "                DFDIST = pd.read_pickle(f\"{SAVEDIR_ANALYSIS}/DFDIST.pkl\")\n",
    "\n",
    "                # DFDIST = append_col_with_grp_index(DFDIST, [f\"{var_effect}_same\", \"task_kind_same\"], \"same-shape|tk\")\n",
    "                # DFDIST[\"tk_12_sorted\"] = [tuple(sorted([x[\"task_kind_1\"], x[\"task_kind_2\"]])) for _, x in DFDIST.iterrows()]\n",
    "                # DFDIST[\"sifirst_12_sorted\"] = [tuple(sorted([x[\"stroke_index_is_first_1\"], x[\"stroke_index_is_first_2\"]])) for _, x in DFDIST.iterrows()]\n",
    "                # # One plot for each pair of task kinds that are different.\n",
    "\n",
    "                # # Create a new variable -- (task_kind, stroke is first)\n",
    "                # for i in [1, 2]:\n",
    "                #     DFDIST = append_col_with_grp_index(DFDIST, [f\"task_kind_{i}\", f\"stroke_index_is_first_{i}\"], f\"tk_sifirst_{i}\")\n",
    "                # DFDIST[\"tk_sifirst_same\"] = DFDIST[\"tk_sifirst_1\"] == DFDIST[\"tk_sifirst_2\"]\n",
    "                # DFDIST[\"tk_sifirst_12\"] = DFDIST[\"tk_sifirst_1\"] + \"|\" + DFDIST[\"tk_sifirst_2\"]\n",
    "                # DFDIST[\"tk_sifirst_sorted\"] = [tuple(sorted([x[\"tk_sifirst_1\"], x[\"tk_sifirst_2\"]])) for _, x in DFDIST.iterrows()]\n",
    "\n",
    "                # Collect\n",
    "                DFDIST[\"animal\"] = animal\n",
    "                DFDIST[\"date\"] = date\n",
    "\n",
    "                list_dfdist.append(DFDIST)\n",
    "            DFDIST = pd.concat(list_dfdist).reset_index(drop=True)\n",
    "\n",
    "            ### (2) Condition the dataframe \n",
    "            # Create a new variable -- (task_kind, stroke is first)\n",
    "            for i in [1, 2]:\n",
    "                print(i)\n",
    "                DFDIST = append_col_with_grp_index(DFDIST, [f\"task_kind_{i}\", f\"stroke_index_is_first_{i}\"], f\"tk_sifirst_{i}\")\n",
    "            DFDIST[\"tk_sifirst_same\"] = DFDIST[\"tk_sifirst_1\"] == DFDIST[\"tk_sifirst_2\"]\n",
    "            DFDIST[\"tk_sifirst_12\"] = DFDIST[\"tk_sifirst_1\"] + \"|\" + DFDIST[\"tk_sifirst_2\"]\n",
    "            DFDIST[\"tk_sifirst_sorted\"] = [tuple(sorted([x[\"tk_sifirst_1\"], x[\"tk_sifirst_2\"]])) for _, x in DFDIST.iterrows()]\n",
    "            DFDIST = append_col_with_grp_index(DFDIST, [f\"{var_effect}_same\", \"task_kind_same\"], \"same-shape|tk\")\n",
    "\n",
    "            ### (3) Split into specific paired datasets\n",
    "            from neuralmonkey.scripts.analy_euclidian_chars_sp import targeted_dim_reduction_post_split_into_specicic_datasets\n",
    "            assert len(DFDIST[\"animal\"].unique())==1\n",
    "            DICT_DFDISTS = {}\n",
    "            for date in list_date:\n",
    "                dfdist = DFDIST[DFDIST[\"date\"] == date].reset_index(drop=True)\n",
    "\n",
    "                dict_dfdists = targeted_dim_reduction_post_split_into_specicic_datasets(dfdist, var_effect)\n",
    "\n",
    "                for key, df in dict_dfdists.items():\n",
    "                    print(key, \" --> n=\", len(df))\n",
    "                    if key not in DICT_DFDISTS:\n",
    "                        DICT_DFDISTS[key] = [df]\n",
    "                    else:\n",
    "                        DICT_DFDISTS[key].append(df)\n",
    "            # convert each to a df\n",
    "            DICT_DFDISTS = {k:pd.concat(list_df).reset_index(drop=True) for k, list_df in DICT_DFDISTS.items()}\n",
    "\n",
    "            ### (4) Save the final split dfdists...\n",
    "            DFDIST.to_pickle(f\"{SAVEDIR_MULT}/DFDIST_FINAL.pkl\")\n",
    "            with open(f\"{SAVEDIR_MULT}/DICT_DFDISTS.pkl\", \"wb\") as f:\n",
    "                pickle.dump(DICT_DFDISTS, f)\n",
    "\n",
    "        ### (5) Plots\n",
    "        from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap\n",
    "        import seaborn as sns\n",
    "        from pythonlib.tools.pandastools import aggregGeneral\n",
    "        from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "        from pythonlib.tools.pandastools import plot_subplots_heatmap, grouping_append_and_return_inner_items_good\n",
    "        from pythonlib.tools.plottools import savefig\n",
    "\n",
    "        # Plot an exmaple\n",
    "        # for tksipair in DICT_DFDISTS.keys(): # key = ('character|0', 'prims_single|1')\n",
    "        for tksipair in [('character|0', 'prims_single|1'), ('character|1', 'prims_single|1')]: # key = ('character|0', 'prims_single|1')\n",
    "            # for twind_scal in DFDIST[\"twind_scal\"].unique():\n",
    "            for twind_scal in [(-0.5, -0.05)]: # twind_scal = (-0.5, -0.05)\n",
    "                for npcs_keep in DFDIST[\"npcs_keep\"].unique(): # npcs_keep = 2\n",
    "                # for npcs_keep in [2, 4, 6]: # npcs_keep = 2\n",
    "                    savedir = f\"{SAVEDIR_MULT}/tksipair={tksipair}-twindscal={twind_scal}-npcs={npcs_keep}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "                    \n",
    "                    # Extract this dfdist\n",
    "                    dfdist = DICT_DFDISTS[tksipair]\n",
    "                    dfdist = dfdist[(dfdist[\"twind_scal\"] == twind_scal) & (dfdist[\"npcs_keep\"] == npcs_keep)].reset_index(drop=True)\n",
    "                    dfdist = append_col_with_grp_index(dfdist, [\"bregion\", \"same-shape|tk\"], \"br-ss|tk\")\n",
    "                    dfdist_agg = aggregGeneral(dfdist, [\"bregion\", \"date\", \"same-shape|tk\"], [\"dist_norm\", \"dist_yue_diff\"])\n",
    "\n",
    "                    # grouping_plot_n_samples_conjunction_heatmap(dfdist, \"shape_semantic_grp_1\", \"shape_semantic_grp_2\" )\n",
    "                    fig = grouping_plot_n_samples_conjunction_heatmap(dfdist, f\"{var_effect}_1\", f\"{var_effect}_2\", [\"date\"]);\n",
    "                    savefig(fig, f\"{savedir}/counts.pdf\")\n",
    "                    plt.close(\"all\")\n",
    "\n",
    "                    fig = sns.catplot(data=dfdist, x=\"bregion\", y=yvar, hue=\"same-shape|tk\", col=\"date\", col_wrap=6,\n",
    "                                kind=\"bar\", aspect=1)\n",
    "                    savefig(fig, f\"{savedir}/catplot-1.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=dfdist, x=\"bregion\", y=yvar, hue=\"same-shape|tk\", col=\"date\", col_wrap=6,\n",
    "                                alpha=0.5, jitter=True, aspect=1)\n",
    "                    savefig(fig, f\"{savedir}/catplot-2.pdf\")\n",
    "\n",
    "                    # Summary plot\n",
    "                    fig = sns.catplot(data=dfdist, x=\"bregion\", y=yvar, hue=\"same-shape|tk\", kind=\"bar\", aspect=1)\n",
    "                    savefig(fig, f\"{savedir}/catplot-summary-3.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=dfdist_agg, x=\"bregion\", y=yvar, hue=\"same-shape|tk\", kind=\"bar\", aspect=1)\n",
    "                    savefig(fig, f\"{savedir}/catplot-agg-1.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=dfdist_agg, x=\"bregion\", y=yvar, hue=\"same-shape|tk\", alpha=0.5, jitter=True, aspect=1)\n",
    "                    savefig(fig, f\"{savedir}/catplot-agg-2.pdf\")\n",
    "\n",
    "                    # Scatter\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(dfdist, \"same-shape|tk\", \"1|0\", \"0|1\", None, var_value=yvar, var_datapt=\"bregion\")\n",
    "                    savefig(fig, f\"{savedir}/scatter-1.pdf\")\n",
    "\n",
    "                    # Scatter\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(dfdist, \"same-shape|tk\", \"1|0\", \"0|1\", \"date\", var_value=yvar, var_datapt=\"bregion\")\n",
    "                    savefig(fig, f\"{savedir}/scatter-2.pdf\")\n",
    "\n",
    "                    plt.close(\"all\")\n",
    "\n",
    "                    # Plot each pair of conditions -- low-level plot\n",
    "                    if MAKE_HEATMAP_DATAPTS:\n",
    "                        grpdict = grouping_append_and_return_inner_items_good(dfdist, [\"date\"])\n",
    "                        for grp, inds in grpdict.items():\n",
    "                            dfdist_this = dfdist.iloc[inds].reset_index(drop=True)\n",
    "                            dfdist_this = dfdist_this[dfdist_this[\"same-shape|tk\"] != \"1|1\"].reset_index(drop=True) # to remove 1/4 of subplots.\n",
    "\n",
    "                            fig, axes = plot_subplots_heatmap(dfdist_this, f\"{var_effect}_1\", f\"{var_effect}_2\", yvar, \"br-ss|tk\", False, True, ncols=6)    \n",
    "\n",
    "                            savefig(fig, f\"{savedir}/heatmap_all_conditions-{grp}.pdf\")\n",
    "                            plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e7a0f",
   "metadata": {},
   "source": [
    "### Further devo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f51276",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "PA = DFallpa[\"pa\"].values[idx]\n",
    "dflab = PA.Xlabels[\"trials\"]\n",
    "\n",
    "dflab[\"gap_from_prev_angle_b\"]\n",
    "\n",
    "dflab[\"gap_from_prev_angle_binned\"].value_counts()\n",
    "assert sum(dflab[\"gap_from_prev_angle_binned\"].isna())==0\n",
    "dflab[\"reach_angle_binned\"] = [f\"angle_{x}\" for x in dflab[\"gap_from_prev_angle_binned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: how much of total variance is accounted for by this subspace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1abe6a",
   "metadata": {},
   "source": [
    "# Original method [fast shuff] but regressing out first stroke effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d05fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Only keep shapes that exist across all conditions\n",
    "    from pythonlib.tools.pandastools import append_col_with_grp_index, extract_with_levels_of_conjunction_vars_helper\n",
    "\n",
    "    VAR_SHAPE = \"shape_semantic\"\n",
    "\n",
    "    savedir = \"/tmp\"\n",
    "    PA = DFallpa[\"pa\"].values[1]\n",
    "    dflab = PA.Xlabels[\"trials\"]\n",
    "    dflab = append_col_with_grp_index(dflab, [\"task_kind\", \"stroke_index_is_first\"], \"tk_sifirst\")\n",
    "    levels_var = list(dflab[\"tk_sifirst\"].unique())\n",
    "    path_counts_pre = f\"{savedir}/counts_pre.pdf\"\n",
    "    path_counts = f\"{savedir}/counts.pdf\"\n",
    "    dfout, dict_dfthis = extract_with_levels_of_conjunction_vars_helper(dflab, \"tk_sifirst\", [VAR_SHAPE], 4, path_counts, \n",
    "                                                lenient_allow_data_if_has_n_levels=None, levels_var=levels_var, plot_counts_also_before_prune_path=path_counts_pre)\n",
    "    PA = PA.slice_by_dim_indices_wrapper(\"trials\", dfout[\"_index\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7247e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, plot state space [HEATMAP code]\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all_wrapper\n",
    "import os\n",
    "\n",
    "DEBUG_bregion = \"PMv\"\n",
    "# DEBUG_subspace_projection = \"task_shape_si\"\n",
    "# DEBUG_subspace_projection = \"task_shape\"\n",
    "var_conj = \"tk_sifirst\"\n",
    "list_twind_scal = [(-0.6, 0.4)]\n",
    "LIST_SUBSPACE_PROJECTION = [\"task_shape\"]\n",
    "# LIST_SUBSPACE_PROJECTION = [\"task_shape_si\"]\n",
    "\n",
    "heatmap_quick_version = True\n",
    "do_heatmap = False\n",
    "also_plot_clean_version = True\n",
    "\n",
    "for regress_out_first_stroke in [False, True]:\n",
    "    # for consolidate_diego_shapes_actually in [False, True]:\n",
    "    for consolidate_diego_shapes_actually in [True]:\n",
    "                \n",
    "        # regress_out_first_stroke = True\n",
    "        # LIST_PRUNE_VERSION = [None, \"sp_char\", \"sp_char_1plus\", \"sp_pig\", \"sp_pig_1plus\", \"sp_pigchar_1plus\"]\n",
    "        LIST_PRUNE_VERSION = [None, \"sp_char\", \"sp_pig\"]\n",
    "        keep_only_shapes_that_exist_across_all_contexts = True\n",
    "        # consolidate_diego_shapes_actually = True\n",
    "        SAVEDIR_ANALYSIS = f\"/tmp/CHAR_SP_HETAMAP-regress={regress_out_first_stroke}-consolidateshapes={consolidate_diego_shapes_actually}\"\n",
    "        os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "\n",
    "        plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR_ANALYSIS, animal, date, \n",
    "                                                DEBUG_skip_drawings=True, DEBUG_bregion=DEBUG_bregion,\n",
    "                                                DEBUG_subspace_projection=None,\n",
    "                                                var_conj=var_conj,\n",
    "                                                LIST_PRUNE_VERSION=LIST_PRUNE_VERSION,\n",
    "                                                list_twind_scal=list_twind_scal,\n",
    "                                                LIST_SUBSPACE_PROJECTION=LIST_SUBSPACE_PROJECTION,\n",
    "                                                heatmap_quick_version=heatmap_quick_version,\n",
    "                                                DO_HACK=regress_out_first_stroke,\n",
    "                                                do_heatmap=do_heatmap, \n",
    "                                                keep_only_shapes_that_exist_across_all_contexts=keep_only_shapes_that_exist_across_all_contexts,\n",
    "                                                consolidate_diego_shapes_actually=consolidate_diego_shapes_actually,\n",
    "                                                also_plot_clean_version=also_plot_clean_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder shapes to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9448803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop code to remove effect of frist stroke.\n",
    "PA = DFallpa[\"pa\"].values[0]\n",
    "PA.Xlabels[\"trials\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, score euclidian distance\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_fast_shuffled\n",
    "import os\n",
    "SAVEDIR_ANALYSIS = f\"/tmp/CHAR_SP_EUCL_FAST\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "\n",
    "# DFallpa = DFallpa.loc[[1],:]\n",
    "DEBUG_bregion_list = [\"PMv\", \"preSMA\"]\n",
    "HACK = True\n",
    "DO_REGRESS_HACK = True\n",
    "euclidian_time_resolved_fast_shuffled(DFallpa, animal, date, SAVEDIR_ANALYSIS, DO_RSA_HEATMAPS=True, \n",
    "                                      HACK = HACK, DEBUG_bregion_list=DEBUG_bregion_list, DO_REGRESS_HACK=DO_REGRESS_HACK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bc130",
   "metadata": {},
   "source": [
    "##### For Diego exmaple in revision, reorder shapes to match the previous plots in first submissiong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because now using shape_semantic, not shape_semantic_grp, and so the shape names are slightly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = DFallpa[\"pa\"].values[0]\n",
    "dflab = pa.Xlabels[\"trials\"]\n",
    "sorted(list(dflab[\"shape_semantic\"].unique()))\n",
    "\n",
    "shapes_in_order = ['arcdeep-DD-DD',\n",
    " 'arcdeep-RR-RR',\n",
    " 'arcdeep-UU-UU',\n",
    " 'V-DD-DD',\n",
    " 'V-RR-RR',\n",
    " 'V-UU-UU',\n",
    " 'Lcentered-DL-DL',\n",
    " 'Lcentered-DR-DR',\n",
    " 'Lcentered-UL-UL',\n",
    " 'Lcentered-UR-UR',\n",
    " 'usquare-DD-DD',\n",
    " 'usquare-RR-RR',\n",
    " 'usquare-UU-UU',\n",
    " 'zigzagSq-LL-0.0',\n",
    " 'zigzagSq-LL-1.0',\n",
    " 'zigzagSq-UU-0.0',\n",
    " 'circle-XX-XX',\n",
    " 'line-LL-LL',\n",
    " 'line-UL-UL',\n",
    " 'line-UR-UR',\n",
    " 'line-UU-UU',\n",
    " 'squiggle3-LL-0.0',\n",
    " 'squiggle3-LL-1.0',\n",
    " 'squiggle3-UU-0.0']\n",
    "\n",
    "map_shape_to_shapeordered = {}\n",
    "for i, sh in enumerate(shapes_in_order):\n",
    "    if i<10:\n",
    "        map_shape_to_shapeordered[sh] = f\"0{i}-{sh}\"\n",
    "    else:\n",
    "        map_shape_to_shapeordered[sh] = f\"{i}-{sh}\"\n",
    "\n",
    "map_shape_to_shapeordered\n",
    "# dflab[\"shape_semantic\"] = [map_shape_to_shapeordered[sh] for sh in dflab[\"shape_semantic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c83342",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f10fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Quick use computing of r2 scalar value\n",
    "PA = DFallpa[\"pa\"].values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Collect r2 over all neurosn\n",
    "var = \"shape_semantic\"\n",
    "list_r2 = []\n",
    "for idx_chan in range(len(PA.Chans)):\n",
    "    r2 = PA.metrics_scalar_compute_r2(idx_chan, var)\n",
    "    list_r2.append(r2)\n",
    "\n",
    "print(\"Range of r2: \", np.percentile(list_r2, [2.5, 50, 97.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88612f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) For a given neuron, split by var_other, and compute r2 for each of those levels.\n",
    "idx_chan = 0\n",
    "list_var = [\"shape_semantic\"]\n",
    "ms = PA.dataextract_metrics_scalar(idx_chan, list_var)\n",
    "r2 = PA.metrics_scalar_compute_r2(idx_chan, var)\n",
    "\n",
    "var = \"shape_semantic\"\n",
    "var_other = \"gridloc\"\n",
    "\n",
    "list_pa, list_levels = PA.split_by_label(\"trials\", [var_other])\n",
    "res = []\n",
    "for levo, pa in zip(list_levels, list_pa):\n",
    "    r2 = pa.metrics_scalar_compute_r2(idx_chan, var)\n",
    "    res.append({\n",
    "        \"level_other\":levo,\n",
    "        \"r2\":r2\n",
    "    })\n",
    "dfres = pd.DataFrame(res)    \n",
    "dfres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8839af5",
   "metadata": {},
   "source": [
    "##### Devo -- cleaning up strokes even more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d0fb6",
   "metadata": {},
   "source": [
    "##### Remove strokes based on motor params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5408fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_map_clustshape_to_thresh, behstrokes_extract_char_clust_sim\n",
    "\n",
    "assert DFallpa[\"event\"].unique().tolist() == [\"00_stroke\"], \"with trials, there is ambiguity bceuae of mult strokes per trials...\"\n",
    "\n",
    "### First, use one PA to extract beh and label each trial as good bad based on beh strokes.\n",
    "PA = DFallpa[\"pa\"].values[0]\n",
    "ds = behstrokes_extract_char_clust_sim(PA, animal, date, savedir=None, PLOT=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc022",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Do the pruning in preprocess_pa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f91c0",
   "metadata": {},
   "source": [
    "##### Remove char strokes that are too different from sp strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the pairwise stroke distances (previously extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "dflab = PA.Xlabels[\"trials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "var_shape = \"shape_semantic_grp\"\n",
    "SAVEDIR = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/motor_comparison/{animal}-{date}-combine={combine}-var={var_shape}\"\n",
    "nmax_for_pairs = None\n",
    "\n",
    "\n",
    "savedir = f\"{SAVEDIR}/{var_shape}/paired_distance-nmax={nmax_for_pairs}\"\n",
    "\n",
    "# for shape in dflab[var_shape].unique():\n",
    "shape = \"Lcentered-UR-UR\"\n",
    "path = f\"{savedir}/alldists-{shape}.pkl\"\n",
    "with open(path, \"rb\") as f:\n",
    "    obj = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cl = obj[\"Cl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cl.plot_heatmap_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each char stroke, get its average distance to each sp stroke\n",
    "import numpy as np\n",
    "dist_each_char = np.mean(Cl.Xinput, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(5, 12))\n",
    "ax.set_xlim([0, 1])\n",
    "ax.plot(dist_each_char, obj[\"trialcodes_char\"], \"ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: decide on a good threshold. Not that obvious, since some char may match some SP. use vareage over sp? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bf3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "# preprocess_pa(animal, date, PA, \"/tmp\", None, shape_var = var_shape, \n",
    "#                   n_min_trials_per_shape=N_MIN_TRIALS_PER_SHAPE,\n",
    "#                   plot_counts_heatmap_savepath=None, plot_drawings=True, remove_chans_fr_drift=False,\n",
    "#                   subspace_projection=None, twind_analy=None, tbin_dur=None, tbin_slide=None, NPCS_KEEP=None, \n",
    "#                   raw_subtract_mean_each_timepoint=False, remove_singleprims_unstable=False,\n",
    "#                   remove_trials_with_bad_strokes=True, subspace_projection_fitting_twind=None,\n",
    "#                   skip_dim_reduction=False, scalar_or_traj=\"traj\",\n",
    "#                   remove_trials_too_fast=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1440fa5",
   "metadata": {},
   "source": [
    "### Regressing out other variables, not just \"first stroke\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc664007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.05\n",
    "twind = twind_analy = (-1, 0.6)\n",
    "PA = PA.regress_neuron_task_variables_subtract_from_activity(tbin_dur, tbin_slide, twind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]\n",
    "dflab[\"reach_angle_class\"] = [f\"reach_{x}\" for x in dflab[\"gap_from_prev_angle_binned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878de55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "pathis = PA.slice_by_dim_indices_wrapper(\"times\", [i])\n",
    "\n",
    "var_effect = \"shape_semantic_grp\"\n",
    "variables = [\"stroke_index_is_first\", \"task_kind\", \"reach_angle_class\", var_effect]\n",
    "variables_is_cat = [True for _ in range(len(variables))]\n",
    "dfcoeff, dfbases, _, _ = pathis.regress_neuron_task_variables_all_chans(variables, \n",
    "                                                                variables_is_cat, PLOT_COEFF_HEATMAP=True)\n",
    "\n",
    "# This is the value you should subtract from all cases with first stroke = True\n",
    "a = dfcoeff[\"C(stroke_index_is_first)[T.True]\"].values[:, None] # (nchans, 1)\n",
    "\n",
    "dflab = PA.Xlabels[\"trials\"]\n",
    "b = dflab[\"stroke_index_is_first\"].astype(int).values[:, None] # (ntrials, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa61348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in dfbases[\"Xpca\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpca = dfbases[dfbases[\"var_subspace\"] == \"reach_angle_class\"][\"Xpca\"].values[0]\n",
    "explained_variance_ratio_ = dfbases[dfbases[\"var_subspace\"] == \"reach_angle_class\"][\"explained_variance_ratio_\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ebaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpca.shape\n",
    "\n",
    "Xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to get main axees for a given categorical variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcoeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe19dbd",
   "metadata": {},
   "source": [
    "# [GOOD] Careful controlling of motor behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436db966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_plot_motor_stats_similarity\n",
    "import os\n",
    "\n",
    "var_shape = \"shape_semantic_grp\"\n",
    "SAVEDIR = \"/tmp/CHAR_SP_MOTOR\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "nmax_for_pairs = None\n",
    "df_paireddist = behstrokes_preprocess_plot_motor_stats_similarity(DFallpa, animal, date, var_shape, SAVEDIR, \n",
    "                                                                  nmax_for_pairs=nmax_for_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "dflab = PA.Xlabels[\"trials\"]\n",
    "\n",
    "dflab[\"clust_sim_max_GOOD\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab[\"shape_semantic_grp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984af161",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_shape = \"shape_semantic_grp\"\n",
    "dflab[var_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_plot_motor_stats_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_print_n_samples\n",
    "grouping_print_n_samples(dflab, [\"shape_semantic_grp\", \"shape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_print_n_samples(ds.Dat, [\"shape_semantic_grp\", \"clust_sim_max_colname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ba836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.Dat[ds.Dat[\"task_kind\"] == \"character\"].reset_index(drop=True)\n",
    "grouping_print_n_samples(df, [\"shape_semantic\", \"shape_semantic\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make plots of all pairwise distances between trials, and their drawings.\n",
    "# Second, make sure the distance metric is fine\n",
    "# -- Check how I modified the trajecotry distance to be better (see line 3378 in psychometric_singleprims)\n",
    "# Third, relate the behavioral distance to the neural distance.\n",
    "# Fourth, prune data to keep just close distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8094d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [Alterantive, but decided teh above was fine] Load the original dataset\n",
    "\n",
    "from pythonlib.dataset.dataset import load_dataset_notdaily_helper, load_dataset_daily_helper\n",
    "\n",
    "D = load_dataset_daily_helper(animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab865f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.Dat[\"charclust_shape_seq_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.dataset.dataset_strokes import preprocess_dataset_to_datstrokes\n",
    "DS = preprocess_dataset_to_datstrokes(D, \"chars_clusters_without_reloading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.Dat[\"gap_from_prev_dur\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa0814",
   "metadata": {},
   "source": [
    "# Controlling for motor, see diff in neural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e7da5",
   "metadata": {},
   "source": [
    "### M1 vs. PMv, showing that PMv cares more about the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR = f\"/tmp/MOTOR\"\n",
    "# twind_analy = (-0.5, 0.)\n",
    "list_twind_analy = [(-0.5, 0.), (-0.2, 0.5)]\n",
    "list_frac = [(0, 0.5), (0, 1.)]\n",
    "list_bregion = DFallpa[\"bregion\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74432f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]\n",
    "dflab[\"index_datapt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd516020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import motor_encoding_score_wrapper\n",
    "motor_encoding_score_wrapper(DFallpa, animal, date, SAVEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only cases with enough data\n",
    "\n",
    "dfres\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Older, regression methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f736c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import convert_var_to_categorical_mult_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dist values \n",
    "dfdist_neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Devo code] Overlay regression lines.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Perform Linear Regression\n",
    "# X = df['X'].values.reshape(-1, 1)\n",
    "# Y = df['Y'].values.reshape(-1, 1)\n",
    "\n",
    "X = data[xvar].values.reshape(-1, 1)\n",
    "Y = data[yvar].values.reshape(-1, 1)\n",
    "\n",
    "# restrict beh distance to remove outliers\n",
    "xmax = np.percentile(X, [98])\n",
    "# ymax = np.percentile(Y, [98])\n",
    "# Y = Y[Y<ymax]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X, bins=100)\n",
    "ax.axvline(xmax, color=\"r\")\n",
    "\n",
    "inds_keep = X<xmax\n",
    "X = X[inds_keep, None]\n",
    "Y = Y[inds_keep, None]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Get Predictions\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "# Plot the Data and Best-Fit Line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X, Y, color='blue', label='Data Points')\n",
    "plt.plot(X, Y_pred, color='red', linewidth=2, label='Best-Fit Line')\n",
    "\n",
    "# Customize the Plot\n",
    "plt.title('Linear Regression Best-Fit Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do regression to compare same vs. diff\n",
    "\n",
    "from pythonlib.tools.pandastools import grou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c28c3",
   "metadata": {},
   "source": [
    "##### Better method -- parametrize the initial movement in the stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c49d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also get:\n",
    "# - old state space, with location, angle\n",
    "# - substrokes -- not the extractioN (which fails for chars) but the features code.\n",
    "# - Motor kinematics (regression) - /home/lucas/code/neuralmonkey/neuralmonkey/notebooks/230208_motor_kinematics.ipynb\n",
    "# - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf16323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR = \"/tmp/MOTOR_PARAMETRIZES\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c285d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the angles\n",
    "from pythonlib.tools.vectools import bin_angle_by_direction\n",
    "bin_angle_by_direction(angles, num_angle_bins=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba63aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(angles, np.sin(angles), \"xk\")\n",
    "ax.plot(angles, np.cos(angles), \"xr\")\n",
    "ax.plot(angles, np.sin(angles)**2 + np.cos(angles)**2, \"xb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d0a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_shape = \"shape_semantic\"\n",
    "\n",
    "def preprocess_and_get_motor(PA, prune_version, subspace_projection, savedir):\n",
    "    \"\"\"\n",
    "    Do dim reduction etc for PA.\n",
    "    Also extract all relevant motor features.\n",
    "\n",
    "    RETURNS:\n",
    "    - copy of PA.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### \n",
    "    n_min_trials_per_shape = 4\n",
    "    plot_drawings = False\n",
    "    # tbin_dur = 0.2\n",
    "    # tbin_slide = 0.02\n",
    "    tbin_dur = 0.1\n",
    "    tbin_slide = 0.05\n",
    "\n",
    "    # Do umap on timecourse\n",
    "    NPCS_KEEP = 8\n",
    "    raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "    subspace_projection_fitting_twind = (-0.8, 0.3)\n",
    "    # twind_analy = (-0.2, 0.1)\n",
    "    twind_analy = (-0.35, -0.05)\n",
    "    # To get scalar\n",
    "    scalar_or_traj = \"scal\"\n",
    "    PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                        n_min_trials_per_shape=n_min_trials_per_shape, shape_var=var_shape, plot_drawings=plot_drawings,\n",
    "                        remove_chans_fr_drift=False,\n",
    "                        subspace_projection=subspace_projection, \n",
    "                        twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                        raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                        subspace_projection_fitting_twind=subspace_projection_fitting_twind,\n",
    "                        remove_singleprims_unstable=True,\n",
    "                        scalar_or_traj=scalar_or_traj, \n",
    "                        consolidate_diego_shapes=False)\n",
    "\n",
    "    if True:\n",
    "        PA.behavior_strokes_kinematics_stats()\n",
    "    else:\n",
    "        # Replacing below\n",
    "        # Dim rediction of beh to good scalar plots\n",
    "        PA.behavior_extract_strokes_to_dflab()\n",
    "        dflab = PA.Xlabels[\"trials\"]\n",
    "        strokes = dflab[\"strok_beh\"].tolist()\n",
    "\n",
    "        ### MOTOR PARAMETERS\n",
    "        # get the initial velocity (angle and magnitude)\n",
    "        from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "        DS = DatStrokes()\n",
    "        \n",
    "        ### For each stroke, compute some features\n",
    "        import numpy as np\n",
    "        from pythonlib.tools.stroketools import sliceStrokes, slice_strok_by_frac_bounds\n",
    "        from pythonlib.tools.stroketools import feature_velocity_vector_angle_norm\n",
    "\n",
    "        # (1) For initial angle, take start of each stroke \n",
    "        twind = [0, 0.15] # sec\n",
    "        strokes_sliced = sliceStrokes(strokes, twind, time_is_relative_each_onset=True, assert_no_lost_strokes=True)\n",
    "\n",
    "        # - velocity vector\n",
    "        angles = [] \n",
    "        norms = []\n",
    "        for strok in strokes_sliced:\n",
    "            _, a, n = feature_velocity_vector_angle_norm(strok)\n",
    "            angles.append(a)\n",
    "            norms.append(n)\n",
    "        \n",
    "        # - circularity\n",
    "        from pythonlib.drawmodel.features import strokeCircularity\n",
    "        fraclow = 0\n",
    "        frachigh = 0.5\n",
    "        strokes_sliced = [slice_strok_by_frac_bounds(s, fraclow, frachigh) for s in strokes]\n",
    "        # strokes_sliced = slice_strok_by_frac_bounds(strokes, twind, time_is_relative_each_onset=True, assert_no_lost_strokes=True)\n",
    "\n",
    "        # circularities = strokeCircularity(strokes)\n",
    "        circularities = strokeCircularity(strokes_sliced)\n",
    "\n",
    "        # - location of onset.\n",
    "        # (reach angle, relative to on location)\n",
    "        onsets_x = [strok[0, 0] for strok in strokes]\n",
    "        onsets_y = [strok[0, 1] for strok in strokes]\n",
    "\n",
    "        # Bin the angles\n",
    "        from pythonlib.tools.vectools import bin_angle_by_direction\n",
    "        angles_binned = bin_angle_by_direction(angles, num_angle_bins=8)\n",
    "\n",
    "        # Put motor variables back into dflab\n",
    "        dflab[\"motor_angle\"] = angles\n",
    "        # dflab[\"motor_angle_sin\"] = angles\n",
    "\n",
    "        dflab[\"motor_angle_binned\"] = angles_binned\n",
    "        \n",
    "        dflab[\"gap_from_prev_angle_binned\"] = bin_angle_by_direction(dflab[\"gap_from_prev_angle\"].values, num_angle_bins=8)\n",
    "\n",
    "        dflab[\"motor_norm\"] = norms\n",
    "\n",
    "        dflab[\"motor_circ\"] = circularities\n",
    "\n",
    "        dflab[\"motor_onsetx\"] = onsets_x\n",
    "\n",
    "        dflab[\"motor_onsety\"] = onsets_y\n",
    "\n",
    "        PA.Xlabels[\"trials\"] = dflab\n",
    "\n",
    "    return PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Extract PA, preprocessed\n",
    "# bregion = \"PMv\"\n",
    "\n",
    "# prune_version = \"sp_char_0\"\n",
    "# subspace_projection = \"task_shape_si\"\n",
    "\n",
    "# prune_version = \"sp_char_0\"\n",
    "# subspace_projection = \"shape_all\"\n",
    "# var_shape = \"shape_semantic_grp\"\n",
    "\n",
    "for prune_version, subspace_projection in [\n",
    "    # (\"sp_char\", \"shape_all\"),\n",
    "    # (\"sp_pig_char\", \"shape_all\"),\n",
    "    (\"sp_char_0\", \"task_shape_si\"),\n",
    "]:\n",
    "    # prune_version = \"sp_char\"\n",
    "    # subspace_projection = \"shape_all\"\n",
    "\n",
    "    # prune_version = \"sp_pig_char\"\n",
    "    # subspace_projection = \"shape_all\"\n",
    "\n",
    "    for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "\n",
    "        savedir = f\"{SAVEDIR}/prune={prune_version}-subspace={subspace_projection}/{bregion}\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "        ### First, get beh distance between strokes. This takes a min or two so do it out here.\n",
    "        PA = extract_single_pa(DFallpa, bregion=bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "        PA = preprocess_and_get_motor(PA, prune_version, subspace_projection, savedir)\n",
    "\n",
    "        if False:\n",
    "            dflab = PA.Xlabels[\"trials\"]\n",
    "\n",
    "            # To compute any feature\n",
    "            # - strokeclass.extract\n",
    "\n",
    "            # To compute velocity\n",
    "            # - dataset_strokes.feature_com\n",
    "            # To bin and plot\n",
    "            # - substrokes.features_motor...\n",
    "            shapes = dflab[var_shape].tolist()\n",
    "            import random\n",
    "            nplot = 10\n",
    "            inds = sorted(random.sample(list(range(len(strokes))), nplot))\n",
    "\n",
    "            titles = [f\"{i}|{shapes[i]}|a={angles[i]:.2f}|n={norms[i]:.0f}|c={circularities[i]:.2f}|x={onsets_x[i]:.0f}|y={onsets_y[i]:.0f}\" for i in inds]\n",
    "            \n",
    "            # Plot exmaples\n",
    "\n",
    "            DS.plot_multiple_strok([strokes[i] for i in inds], titles=titles, overlay=False, size_per_sublot=4.8)\n",
    "            DS.plot_multiple_strok([strokes_sliced[i] for i in inds], titles=titles, overlay=False, size_per_sublot=4.8)\n",
    "            # Plot state space, coloring by all the variables, and overlaying stroke examples\n",
    "\n",
    "            # PA.plot_state_space_good_wrapper(savedir, LIST_VAR, LIST_VARS_OTHERS=None, LIST_FILTDICT=None, LIST_PRUNE_MIN_N_LEVS=None,\n",
    "            #                                       time_bin_size = 0.05, PLOT_CLEAN_VERSION = False, \n",
    "            #                                       nmin_trials_per_lev=None, list_dim_timecourse=None, list_dims=None)\n",
    "\n",
    "\n",
    "        X = PA.X\n",
    "        X = np.mean(X, axis=2).T\n",
    "        dflab = PA.Xlabels[\"trials\"]\n",
    "        from neuralmonkey.analyses.state_space_good import trajgood_plot_colorby_splotby_scalar_WRAPPER\n",
    "\n",
    "        # LIST_VAR = [\n",
    "        #     var_shape, \n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle\", \n",
    "        #     \"motor_norm\",\n",
    "        #     \"motor_circ\",\n",
    "        #     \"motor_onsetx\",\n",
    "        #     \"motor_onsety\", \n",
    "        #     \"gap_from_prev_angle\",\n",
    "        #     var_shape, \n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle\", \n",
    "        #     \"motor_norm\",\n",
    "        #     \"motor_circ\",\n",
    "        #     \"motor_onsetx\",\n",
    "        #     \"motor_onsety\", \n",
    "        #     \"gap_from_prev_angle\",\n",
    "        #     var_shape, \n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle\", \n",
    "        #     \"motor_norm\",\n",
    "        #     \"motor_circ\",\n",
    "        #     \"motor_onsetx\",\n",
    "        #     \"motor_onsety\", \n",
    "        #     \"gap_from_prev_angle\",\n",
    "        #     var_shape, \n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle\", \n",
    "        #     \"motor_norm\",\n",
    "        #     \"motor_circ\",\n",
    "        #     \"motor_onsetx\",\n",
    "        #     \"motor_onsety\",\n",
    "        #     \"gap_from_prev_angle\"]\n",
    "        # LIST_VARS_OTHERS = [\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     None,\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),\n",
    "        #     (\"task_kind\",),    \n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     (var_shape,),\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "        #     \"motor_angle_binned\",\n",
    "            \n",
    "        # ]\n",
    "\n",
    "\n",
    "        LIST_VAR = [\n",
    "            var_shape, \n",
    "            var_shape, \n",
    "            var_shape, \n",
    "            var_shape, \n",
    "            var_shape, \n",
    "            var_shape, \n",
    "            \"motor_angle_binned\",\n",
    "            \"motor_angle\", \n",
    "            \"motor_norm\",\n",
    "            \"motor_circ\",\n",
    "            \"motor_onsetx\",\n",
    "            \"motor_onsety\", \n",
    "            \"gap_from_prev_angle\",\n",
    "            ]\n",
    "        \n",
    "        LIST_VARS_OTHERS = [\n",
    "            None,\n",
    "            (\"task_kind\",),\n",
    "            (\"motor_angle_binned\", \"task_kind\"),\n",
    "            (\"motor_angle_binned\"),\n",
    "            (\"gap_from_prev_angle_binned\", \"task_kind\"),\n",
    "            (\"gap_from_prev_angle_binned\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "            (var_shape, \"task_kind\"),\n",
    "        ]\n",
    "\n",
    "        # list_dims = [(0,1), (1,2), (2,3), (3,4)]\n",
    "        list_dims = [(0,1), (1,2)]\n",
    "        for var, vars_others in zip(LIST_VAR, LIST_VARS_OTHERS):\n",
    "            trajgood_plot_colorby_splotby_scalar_WRAPPER(X, dflab, var, savedir, vars_others, list_dims)\n",
    "\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc215ec",
   "metadata": {},
   "source": [
    "##### Quick analysis of euclidian distance, using binned beh features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = \"/tmp/MOTOR_EUCLIDIAN\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_version = \"sp_pig_char\"\n",
    "subspace_projection = \"shape_all\"\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "subspace_projection_fitting_twind = (-0.4, 0.4)\n",
    "twind_analy = (-0.2, 0.1)\n",
    "var_other = \"gap_from_prev_angle_binned\"\n",
    "\n",
    "# prune_version = \"sp_char_0\"\n",
    "# subspace_projection = \"task_shape_si\"\n",
    "# shape_var = \"shape_semantic\"\n",
    "# subspace_projection_fitting_twind = (-0.8, 0.3)\n",
    "# twind_analy = (-0.35, -0.05)\n",
    "# var_other = \"motor_angle_binned\"\n",
    "\n",
    "# prune_version = \"sp_char_0\"\n",
    "# subspace_projection = \"task_shape_si\"\n",
    "# shape_var = \"shape_semantic\"\n",
    "# subspace_projection_fitting_twind = (-0.2, 0.5)\n",
    "# twind_analy = (-0.1, 0.25)\n",
    "# var_other = \"motor_angle_binned\"\n",
    "\n",
    "for bregion in [\"PMv\", \"M1\", \"PMd\", \"SMA\"]:\n",
    "    savedir = f\"{SAVEDIR}/{bregion}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    ### \n",
    "    n_min_trials_per_shape = 4\n",
    "    plot_drawings = False\n",
    "    # tbin_dur = 0.2\n",
    "    # tbin_slide = 0.02\n",
    "    tbin_dur = 0.1\n",
    "    tbin_slide = 0.05\n",
    "\n",
    "    # Do umap on timecourse\n",
    "    NPCS_KEEP = 8\n",
    "    raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "\n",
    "    ### First, get beh distance between strokes. This takes a min or two so do it out here.\n",
    "    PA = extract_single_pa(DFallpa, bregion=bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "    # To get scalar\n",
    "    scalar_or_traj = \"traj\"\n",
    "    PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                        n_min_trials_per_shape=n_min_trials_per_shape, shape_var=shape_var, plot_drawings=plot_drawings,\n",
    "                        remove_chans_fr_drift=False,\n",
    "                        subspace_projection=subspace_projection, \n",
    "                            twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                            raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                            subspace_projection_fitting_twind=subspace_projection_fitting_twind,\n",
    "                            remove_singleprims_unstable=True,\n",
    "                            scalar_or_traj=scalar_or_traj)\n",
    "\n",
    "    # Dim rediction of beh to good scalar plots\n",
    "    PA.behavior_strokes_kinematics_stats()\n",
    "    if False:\n",
    "        PA.behavior_extract_strokes_to_dflab()\n",
    "        dflab = PA.Xlabels[\"trials\"]\n",
    "        strokes = dflab[\"strok_beh\"].tolist()\n",
    "\n",
    "        ### MOTOR PARAMETERS\n",
    "        # get the initial velocity (angle and magnitude)\n",
    "        from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "        DS = DatStrokes()\n",
    "\n",
    "        ### For each stroke, compute some features\n",
    "        import numpy as np\n",
    "        from pythonlib.tools.stroketools import sliceStrokes\n",
    "        from pythonlib.tools.stroketools import feature_velocity_vector_angle_norm\n",
    "\n",
    "        # (1) For initial angle, take start of each stroke \n",
    "        twind = [0, 0.15] # sec\n",
    "        strokes_sliced = sliceStrokes(strokes, twind, time_is_relative_each_onset=True, assert_no_lost_strokes=True)\n",
    "\n",
    "        # - velocity vector\n",
    "        angles = [] \n",
    "        norms = []\n",
    "        for strok in strokes_sliced:\n",
    "            _, a, n = feature_velocity_vector_angle_norm(strok)\n",
    "            angles.append(a)\n",
    "            norms.append(n)\n",
    "\n",
    "        # - circularity\n",
    "        from pythonlib.drawmodel.features import strokeCircularity\n",
    "\n",
    "        # twind = [0, 0.15] # sec\n",
    "        # strokes_sliced = sliceStrokes(strokes, twind, time_is_relative_each_onset=True, assert_no_lost_strokes=True)\n",
    "        circularities = strokeCircularity(strokes)\n",
    "\n",
    "        # - location of onset.\n",
    "        # (reach angle, relative to on location)\n",
    "        onsets_x = [strok[0, 0] for strok in strokes]\n",
    "        onsets_y = [strok[0, 1] for strok in strokes]\n",
    "\n",
    "        # Bin the angles\n",
    "        from pythonlib.tools.vectools import bin_angle_by_direction\n",
    "        angles_binned = bin_angle_by_direction(angles, num_angle_bins=6)\n",
    "\n",
    "        # Put motor variables back into dflab\n",
    "        dflab[\"motor_angle\"] = angles\n",
    "        dflab[\"motor_angle_binned\"] = angles_binned\n",
    "        dflab[\"motor_norm\"] = norms\n",
    "        dflab[\"motor_circ\"] = circularities\n",
    "        dflab[\"motor_onsetx\"] = onsets_x\n",
    "        dflab[\"motor_onsety\"] = onsets_y\n",
    "\n",
    "        # dflab[\"gap_from_prev_angle_binned\"] = bin_angle_by_direction(dflab[\"gap_from_prev_angle\"].values, num_angle_bins=8)\n",
    "        # dflab[\"gap_from_prev_angle_binned\"] = bin_angle_by_direction(dflab[\"gap_from_prev_angle\"].values, num_angle_bins=8)\n",
    "        dflab[\"gap_from_prev_angle_binned\"] = bin_angle_by_direction(dflab[\"gap_from_prev_angle\"].values, num_angle_bins=4)\n",
    "\n",
    "\n",
    "    # #### COMPUTE EUCLIDIAN\n",
    "    # var_context_same = \"task_kind\"\n",
    "    # savedir = \"/tmp/RSA\"\n",
    "    # os.makedirs(savedir, exist_ok=True)\n",
    "    # from neuralmonkey.analyses.euclidian_distance import timevarying_compute_fast_to_scalar\n",
    "    # vars_group = [shape_var, \"motor_angle_binned\"]\n",
    "    # # vars_group = [shape_var, \"gap_from_prev_angle_binned\"]\n",
    "    # # DFDIST = PA.dataextractwrap_distance_between_groups(vars_group, \"traj_to_scal\")\n",
    "    # context_dict = {\"same\":[\"task_kind\"], \"diff\":[]}\n",
    "    # dfdist, Cldist = timevarying_compute_fast_to_scalar(PA, vars_group, savedir, var_context_same=var_context_same)\n",
    "    # import seaborn as sns\n",
    "    # sns.catplot(data=dfdist, x=\"same-shape_semantic_grp|motor_angle_binned\", y=\"dist_yue_diff\", alpha=0.5, jitter=True)\n",
    "    # sns.catplot(data=dfdist, x=\"same-shape_semantic_grp|motor_angle_binned\", y=\"dist_yue_diff\", kind=\"bar\")\n",
    "    # import seaborn as sns\n",
    "    # sns.catplot(data=dfdist, x=\"same-shape_semantic_grp|motor_angle_binned\", y=\"dist_yue_diff\", alpha=0.5, jitter=True)\n",
    "    # sns.catplot(data=dfdist, x=\"same-shape_semantic_grp|motor_angle_binned\", y=\"dist_yue_diff\", kind=\"bar\")\n",
    "\n",
    "\n",
    "    # from pythonlib.tools.vectools import bin_angle_by_direction\n",
    "    # angles_binned = bin_angle_by_direction(angles, num_angle_bins=8)\n",
    "\n",
    "    # # Put motor variables back into dflab\n",
    "    # dflab[\"motor_angle\"] = angles\n",
    "    # dflab[\"motor_angle_binned\"] = angles_binned\n",
    "    # dflab[\"motor_norm\"] = norms\n",
    "    # dflab[\"motor_circ\"] = circularities\n",
    "    # dflab[\"motor_onsetx\"] = onsets_x\n",
    "    # dflab[\"motor_onsety\"] = onsets_y\n",
    "\n",
    "\n",
    "    # dflab[\"this\"] = bin_angle_by_direction(dflab[\"gap_from_prev_angle\"].values, num_angle_bins=8)\n",
    "\n",
    "\n",
    "    #### COMPUTE EUCLIDIAN\n",
    "    # var_context_same = None\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    from neuralmonkey.analyses.euclidian_distance import timevarying_compute_fast_to_scalar\n",
    "    vars_group = [shape_var, var_other]\n",
    "    # vars_group = [\"shape_semantic_grp\", \"this\"]\n",
    "    # DFDIST = PA.dataextractwrap_distance_between_groups(vars_group, \"traj_to_scal\")\n",
    "    dfdist, Cldist = timevarying_compute_fast_to_scalar(PA, vars_group, savedir)\n",
    "\n",
    "    import seaborn as sns\n",
    "    fig = sns.catplot(data=dfdist, x=f\"same-{shape_var}|{var_other}\", y=\"dist_yue_diff\", alpha=0.5, jitter=True)\n",
    "    savefig(fig, f\"{savedir}/catplot-1.pdf\")\n",
    "    fig = sns.catplot(data=dfdist, x=f\"same-{shape_var}|{var_other}\", y=\"dist_yue_diff\", kind=\"bar\")\n",
    "    savefig(fig, f\"{savedir}/catplot-2.pdf\")\n",
    "\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A single PC space for motor\n",
    "from neuralmonkey.analyses.state_space_good import dimredgood_pca\n",
    "\n",
    "features = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa70361",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA.X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73117314",
   "metadata": {},
   "source": [
    "##### Regression -- condition on each shape, and predict a variable using the neural activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = \"/tmp/MOTOR_REGR\"\n",
    "# os.makedirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "prune_version = \"sp_pig_char\"\n",
    "subspace_projection = \"shape_all\"\n",
    "bregion = \"FP\"\n",
    "\n",
    "savedir = f\"{SAVEDIR}/prune={prune_version}-subspace={subspace_projection}/{bregion}\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "### First, get beh distance between strokes. This takes a min or two so do it out here.\n",
    "PA = extract_single_pa(DFallpa, bregion=bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "PA = preprocess_and_get_motor(PA, prune_version, subspace_projection, savedir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacae412",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from pythonlib.tools.vectools import linearize_angles_around_mean\n",
    "\n",
    "var_beh = \"motor_angle\"\n",
    "vars_grp = [var_shape]\n",
    "\n",
    "ndims = NPCS_KEEP\n",
    "\n",
    "dflab = PA.Xlabels[\"trials\"]\n",
    "X = PA.X\n",
    "assert X.shape[2]==1\n",
    "grpdict = grouping_append_and_return_inner_items_good(dflab, vars_grp)\n",
    "\n",
    "for grp, inds in grpdict.items():\n",
    "    \n",
    "    x = X[:ndims, inds, 0].T # (dims, trials)\n",
    "    y_beh = dflab.loc[inds, var_beh].values\n",
    "\n",
    "    # normalize the beh\n",
    "    if var_beh in [\"motor_angle\"]:\n",
    "        _, y_beh, _, _ = linearize_angles_around_mean(y_beh)\n",
    "    \n",
    "    # regression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86639d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab.loc[inds, var_beh].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6bfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y_beh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269efaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "# X, y = make_regression(n_samples=500, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Cross-validate the model\n",
    "cv_results = cross_validate(\n",
    "    model, x, y_beh, cv=2,  # 5-fold cross-validation\n",
    "    scoring=['r2', 'neg_mean_squared_error'],  # Specify scoring metrics\n",
    "    return_train_score=True  # Include training scores\n",
    ")\n",
    "\n",
    "# Display Results\n",
    "print(\"Cross-Validation Results:\")\n",
    "for metric in cv_results:\n",
    "    print(f\"{metric}: {cv_results[metric]}\")\n",
    "\n",
    "# Summarize Mean and Std for Test Scores\n",
    "print(\"\\nMean R2 (Test):\", np.mean(cv_results['test_r2']))\n",
    "print(\"Mean MSE (Test):\", -np.mean(cv_results['test_neg_mean_squared_error']))  # MSE is negated for interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY:\n",
    "# - good, clearly see that M1 high beh, lower shape, and PMv opposite, if do quick euclidian test, and also\n",
    "# if inspect the scatter plots.\n",
    "# --- PMd, looks like PMv. I think this would go away once include reach direction too.\n",
    "\n",
    "# TODO:\n",
    "# Plots:\n",
    "# - color angle circular.\n",
    "# - binnned reach angle (this explains PMd?)\n",
    "# - PCA on beh variables --> then bin and color in plots. \n",
    "\n",
    "# Euclidian:\n",
    "# - already did above, but was hacky. \n",
    "# - If do this, then should optimize how many bins. And maybe first do PCA of beh, and then bin.\n",
    "\n",
    "# Regression:\n",
    "# - This is best? for asking about relationship with beh, conditioned on shape.\n",
    "# - Should actually make the input data be the angles. Analogous to how the input data for euclidian distance is shape categories.\n",
    "\n",
    "# Pairwise analysis\n",
    "# - This might also be good. This is the previous method (motor_encoding_score_wrapper), but replacing the motor score there with pairwise distance (e.g., angle)\n",
    "# - TODO: make those plots and see if it looks good.\n",
    "\n",
    "# SUMMARY:\n",
    "# Actually, euclidian is very related to pairwise analysis\n",
    "# - decision for me is whether to use binned beh, or to do pairwise between raw beh, and then split into close and far.\n",
    "\n",
    "# Regression is better if you think there is systematic, simple relationship (e.g., linear or circular)    \n",
    "\n",
    "# THOUGHTS:\n",
    "# - probably first try pairwise distances ()\n",
    "\n",
    "# ALSO:\n",
    "# - some other notes on ways to analyse the pairwise distance (in macbook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0aa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99140051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc183786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD\n",
    "# Do slice\n",
    "from pythonlib.tools.stroketools import slice_strok_by_frac_bounds\n",
    "strokes = [slice_strok_by_frac_bounds(strok, frac_bounds_stroke[0], frac_bounds_stroke[1]) for strok in strokes]\n",
    "\n",
    "# Get pairwise distnace between all strokes.\n",
    "from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "ds = DatStrokes()\n",
    "labels = [tuple(x) for x in dflab.loc[:, vars_group].values.tolist()]\n",
    "\n",
    "list_distance_ver = [\"euclid_vels_2d\"]\n",
    "CldistStroke = ds.distgood_compute_beh_beh_strok_distances(strokes, strokes, list_distance_ver, labels_rows_dat=labels,\n",
    "                                                labels_cols_feats=labels, label_var=vars_group, clustclass_rsa_mode=True)\n",
    "CldistStroke._Xinput = 1-CldistStroke._Xinput # So that ranges from [0, 1] where 0 is best.\n",
    "dfdist_stroke = CldistStroke.rsa_dataextract_with_labels_as_flattened_df(keep_only_one_direction_for_each_pair=False, plot_heat=False, exclude_diagonal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, for each stroke, extract the onset (substroke)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot state space, coloring by different kinematic metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cases of (same shape, diff motor) and (diff shape, similar motor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0d29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0790e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72115c7e",
   "metadata": {},
   "source": [
    "### Compare beh strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA.behavior_extract_strokes_to_dflab(trial_take_first_stroke=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PA.Xlabels[\"trials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac20dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "grpdict = grouping_append_and_return_inner_items_good(dflab, [shape_var, \"task_kind\"])\n",
    "\n",
    "list_shape = dflab[shape_var].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c304c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two set of strokes, score how similar they are.\n",
    "tk1 = \"prims_single\"\n",
    "tk2 = \"character\"\n",
    "for shape in list_shape:\n",
    "    key1 = (shape, tk1)\n",
    "    key2 = (shape, tk2)\n",
    "    if key1 not in grpdict.keys():\n",
    "        print(\"Key not found: \", key1)\n",
    "        continue\n",
    "    if key2 not in grpdict.keys():\n",
    "        print(\"Key not found: \", key2)\n",
    "        continue\n",
    "    \n",
    "    strokes1 = dflab.iloc[grpdict[key1]][\"strok_beh\"].tolist()\n",
    "    strokes2 = dflab.iloc[grpdict[key2]][\"strok_beh\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b27094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pythonlib.dataset.dataset_strokes import DatStrokes\n",
    "ds = DatStrokes()\n",
    "Cl = ds.distgood_compute_beh_beh_strok_distances(strokes1, strokes2, label_var=shape_var)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc668aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88237655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "\n",
    "pa = DFallpa[\"pa\"].values[5]\n",
    "preprocess_pa(animal, date, pa, \"/tmp\", None, remove_trials_too_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4510d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "behstrokes_extract_char_clust_sim(PA, animal, date, savedir=None, PLOT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[0]\n",
    "behstrokes_extract_char_clust_sim(PA, animal, date, \"/tmp\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9daa1a",
   "metadata": {},
   "source": [
    "# [0] Do state space and euclidian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84d7e0",
   "metadata": {},
   "source": [
    "##### First, prune PA to just good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "import seaborn as sns\n",
    "from pythonlib.tools.plottools import savefig\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/CHAR_SP_FINAL\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06978a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune to just the DFallpa for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PARAMS\n",
    "n_min_trials_per_shape = 5\n",
    "LIST_NPCS_KEEP = [4,6,2]\n",
    "PLOT_EACH_REGION = True\n",
    "\n",
    "### State space plots\n",
    "LIST_VAR = [\n",
    "    \"shape_semantic\",\n",
    "    \"shape_semantic\",\n",
    "    \"shape_semantic\",\n",
    "]\n",
    "LIST_VARS_OTHERS = [\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "]\n",
    "LIST_CONTEXT = [\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [2 for _ in range(len(LIST_VAR))]\n",
    "LIST_FILTDICT = [\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"], \"stroke_index\":[0]},\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"]},\n",
    "    {\"task_kind\":[\"prims_single\", \"prims_on_grid\"]},\n",
    "    ]\n",
    "\n",
    "for twind_analy in [(0.05, 0.25), (-0.05, 0.35), (0.1, 0.2)]:\n",
    "    for subspace_projection in [\"shape_prims_single\", \"pca\"]:\n",
    "        for prune_version in [\"sp_char_0\", \"sp_char\"]:\n",
    "            for NPCS_KEEP in LIST_NPCS_KEEP:\n",
    "                for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "                    SAVEDIR = f\"{SAVEDIR_ANALYSIS}/subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}\"\n",
    "                    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "                    assert False\n",
    "                    PLOT_STATE_SPACE = NPCS_KEEP == max(LIST_NPCS_KEEP)\n",
    "                    run(animal, date, DFallpa, SAVEDIR, subspace_projection, prune_version, NPCS_KEEP, \n",
    "                            raw_subtract_mean_each_timepoint, n_min_trials_per_shape,\n",
    "                            PLOT_EACH_REGION, PLOT_STATE_SPACE,\n",
    "                            LIST_VAR, LIST_VARS_OTHERS, LIST_FILTDICT, LIST_PRUNE_MIN_N_LEVS, twind_analy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adcb7cc",
   "metadata": {},
   "source": [
    "##### Run, a single time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import run\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_version = \"sp_char_0\"\n",
    "# # subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "# twind_analy = (-0.15, 0.2)\n",
    "# tbin_dur = 0.1\n",
    "# tbin_slide = 0.02\n",
    "\n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "twind_analy = (0.05, 0.25)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.05\n",
    "\n",
    "NPCS_KEEP = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "n_min_trials_per_shape = 5\n",
    "PLOT_EACH_REGION = False\n",
    "PLOT_STATE_SPACE = False\n",
    "LIST_VAR = [\"shape_semantic\"]\n",
    "LIST_VARS_OTHERS = [\n",
    "    [\"task_kind\", \"stroke_index\"],\n",
    "]\n",
    "LIST_CONTEXT = [\n",
    "    {\"same\":[\"stroke_index\"], \"diff\":[\"task_kind\"]},\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [2 for _ in range(len(LIST_VAR))]\n",
    "LIST_FILTDICT = [\n",
    "    {\"task_kind\":[\"prims_single\", \"character\"], \"stroke_index\":[0]},\n",
    "    ]\n",
    "\n",
    "SAVEDIR = f\"/tmp/TEST-{twind_analy}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "run(animal, date, DFallpa, SAVEDIR, subspace_projection, prune_version, NPCS_KEEP, \n",
    "        raw_subtract_mean_each_timepoint, n_min_trials_per_shape,\n",
    "        PLOT_EACH_REGION, PLOT_STATE_SPACE,\n",
    "        LIST_VAR, LIST_VARS_OTHERS, LIST_FILTDICT, LIST_PRUNE_MIN_N_LEVS, twind_analy,\n",
    "        tbin_dur = tbin_dur, tbin_slide = tbin_slide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2614a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time-series of eucl distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457a5a8",
   "metadata": {},
   "source": [
    "##### Run, for a single bregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = f\"/tmp/TEST/{animal}-{date}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"preSMA_a\"    \n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = \"shape_prims_single\"\n",
    "# subspace_projection = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "remove_drift = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c08d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_analy = (-0.15, 0.2)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d12591",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 5\n",
    "\n",
    "# Run\n",
    "PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "\n",
    "savedir = f\"{SAVEDIR}/preprocess\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "plot_drawings = False\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                    n_min_trials_per_shape=n_min_trials_per_shape, plot_drawings=plot_drawings,\n",
    "                    remove_chans_fr_drift=remove_drift,\n",
    "                    subspace_projection=subspace_projection, \n",
    "                           twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, keep specific chans\n",
    "# chans_keep = [1053, 1054]\n",
    "chans_keep = [1044, 1049,  1053, 1054, 1057, 1059, 1062]\n",
    "PA = PA.slice_by_dim_values_wrapper(\"chans\", chans_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7738f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    pa = PA.slice_by_dim_values_wrapper(\"times\", twind_analy)\n",
    "    pa = pa.agg_by_time_windows_binned(tbin_dur, tbin_slide)\n",
    "\n",
    "else:\n",
    "    subspace_projection = \"shape_prims_single\"\n",
    "\n",
    "    if subspace_projection == \"pca\":\n",
    "        dim_red_method = \"pca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":None,\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":None,\n",
    "        }\n",
    "    elif subspace_projection == \"shape_prims_single\":\n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_single\"]}\n",
    "        }\n",
    "    elif subspace_projection == \"shape_PIG_stroke0\":\n",
    "        # PIG (0)  \n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"prims_on_grid\"], \"stroke_index\":[0]}\n",
    "        }\n",
    "    elif subspace_projection == \"shape_char_stroke0\":\n",
    "        # Char  \n",
    "        dim_red_method = \"superv_dpca\"\n",
    "        superv_dpca_params={\n",
    "            \"superv_dpca_var\":\"shape_semantic\",\n",
    "            \"superv_dpca_vars_group\":None,\n",
    "            \"superv_dpca_filtdict\":{\"task_kind\":[\"character\"], \"stroke_index\":[0]}\n",
    "        }\n",
    "    else:\n",
    "        print(subspace_projection)\n",
    "        assert False\n",
    "        \n",
    "\n",
    "    ### New, cleaner method, taking all pairwise distances between trials\n",
    "    savedir = f\"{SAVEDIR}/each_region/{bregion}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    # (1) First, dim reduction\n",
    "    superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "    superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "    superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "\n",
    "    dim_red_method = \"pca\"\n",
    "    twind_analy = (-0.15, 0.3)\n",
    "    tbin_dur = 0.1\n",
    "    tbin_slice = 0.01\n",
    "    NPCS_KEEP = 10\n",
    "    _, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "                                    twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "                                    NPCS_KEEP = NPCS_KEEP,\n",
    "                                    dpca_var = None, dpca_vars_group = None, dpca_filtdict=None, \n",
    "                                    dpca_proj_twind = twind_analy, \n",
    "                                    raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                    umap_n_components=None, umap_n_neighbors=None)\n",
    "    \n",
    "\n",
    "    pa = PAredu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb54f3d",
   "metadata": {},
   "source": [
    "# [2] Quick analyses of euclidian distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_group = [\"task_kind\", \"shape_semantic\"]\n",
    "version = \"traj\"\n",
    "DFDIST = pa.dataextractwrap_distance_between_groups(vars_group, version)\n",
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "# DFDIST = append_col_with_grp_index(DFDIST, [\"shape_semantic_1\", \"shape_semantic_2\"], \"shape_semantic_same\")\n",
    "# DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_same\")\n",
    "\n",
    "DFDIST[\"task_kind_same\"] = DFDIST[\"task_kind_1\"] == DFDIST[\"task_kind_2\"]\n",
    "DFDIST[\"shape_semantic_same\"] = DFDIST[\"shape_semantic_1\"] == DFDIST[\"shape_semantic_2\"]\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_12\")\n",
    "\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"task_kind_same\", \"shape_semantic_same\"], \"same-task|shape\")\n",
    "\n",
    "DFDIST = append_col_with_grp_index(DFDIST, [\"shape_semantic_same\", \"task_kind_12\"], \"same_shape|task_kind_12\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef76aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for y in [\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"]:\n",
    "    # sns.relplot(data=DFDIST, x=\"time_bin\", y=y, hue=\"same_shape|task_kind_12\", kind=\"line\", errorbar=(\"ci\", 68))\n",
    "    sns.relplot(data=DFDIST, x=\"time_bin\", y=y, hue=\"same-task|shape\", kind=\"line\", errorbar=(\"ci\", 68))\n",
    "    # fig = sns.relplot(data=DFDIST, x=\"time_bin\", y=y, kind=\"line\", hue=\"shape_novel_12\")\n",
    "    # fig = sns.relplot(data=DFDIST, x=\"time_bin\", y=y, kind=\"line\", hue=\"seqc_0_shape_12\", col=\"shape_novel_12\", legend=False, alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(data=dfres, x = \"bregion\", hue=\"shape_task_same\", y=\"dist_yue_diff\", kind=\"bar\", aspect=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function...\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "\n",
    "\n",
    "SAVEDIR = \"/tmp/TEST_TIME_RESOLV\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "\n",
    "bregion = \"preSMA_a\"    \n",
    "prune_version = \"sp_char_0\"\n",
    "subspace_projection = None\n",
    "# subspace_projection = \"shape_prims_single\"\n",
    "NPCS_KEEP = 10\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "remove_drift = False\n",
    "twind_analy = (-0.15, 0.2)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "\n",
    "euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, savedir, twind_analy,\n",
    "                            tbin_dur, tbin_slide, \n",
    "                            subspace_projection, NPCS_KEEP, \n",
    "                            n_min_trials_per_shape = 5, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # testing by hand\n",
    "    # remove = [1047, 1048, 1051, 1052, 1056, 1058]\n",
    "    # remove = [1048, 1051, 1052, 1056, 1058, 1059, 1062]\n",
    "    remove = [1052, 1056, 1057, 1062, 1073, 1075]\n",
    "    PA = DFallpa[\"pa\"].values[2]\n",
    "    hack_prune_to_these_chans = [c for c in PA.Chans if c not in remove]\n",
    "else:\n",
    "    hack_prune_to_these_chans = None\n",
    "\n",
    "# hack_prune_to_these_chans = [1049, 1053, 1054, 1057]\n",
    "# hack_prune_to_these_chans = [1043, 1044, 1047, 1053, 1054, 1057]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176494b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "import os\n",
    "# prune_version = \"sp_char_0\"\n",
    "# prune_version = None\n",
    "n_min_trials_per_shape = 5\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/TEST_TIME_RESOLV\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "\n",
    "NPCS_KEEP = 10\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "twind_analy = (-0.35, 0.5)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "# # for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "# for bregion in [\"PMv_m\"]:\n",
    "#     # for prune_version in [\"sp_char_0\", \"pig_char_0\"]:\n",
    "#     for prune_version in [\"sp_char\", \"pig_char\", \"sp_char_0\"]:\n",
    "#         # for subspace_projection in [None, \"pca\", \"shape_prims_single\", \"shape_all\"]:\n",
    "#         for subspace_projection in [None]:\n",
    "#             for remove_drift in [False]:\n",
    "#                 for remove_singleprims_unstable in [True, False]:\n",
    "\n",
    "#                     SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{bregion}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-HACK-{hack_prune_to_these_chans is not None}\"\n",
    "#                     os.makedirs(SAVEDIR, exist_ok=True)\n",
    "#                     print(SAVEDIR)\n",
    "\n",
    "\n",
    "#                     euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "#                                                 tbin_dur, tbin_slide, \n",
    "#                                                 subspace_projection, NPCS_KEEP, \n",
    "#                                                 n_min_trials_per_shape = 5, \n",
    "#                                                 raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "#                                                 hack_prune_to_these_chans=hack_prune_to_these_chans,\n",
    "#                                                 remove_singleprims_unstable=remove_singleprims_unstable)\n",
    "\n",
    "#                     assert False\n",
    "\n",
    "# Compact version of above\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_wrapper\n",
    "euclidian_time_resolved_wrapper(animal, date, DFallpa, \"/tmp/TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 4\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "NPCS_KEEP = 6\n",
    "\n",
    "twind_analy = (-0.4, 0.5)\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "\n",
    "for bregion in DFallpa[\"bregion\"].unique().tolist():\n",
    "    for prune_version in [\"sp_char_0\", \"pig_char_0\", \"sp_char\", \"pig_char_1plus\"]:\n",
    "        for subspace_projection in [\"task_shape_si\"]:\n",
    "            for remove_drift in [False]:\n",
    "                for raw_subtract_mean_each_timepoint in [False]:\n",
    "                    for remove_singleprims_unstable in [True]:\n",
    "                        SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{bregion}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-subtrmean={raw_subtract_mean_each_timepoint}\"\n",
    "                        os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                        print(\"SAVING AT ... \", SAVEDIR)\n",
    "                        euclidian_time_resolved(animal, date, DFallpa, bregion, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "                                                    tbin_dur, tbin_slide, \n",
    "                                                    subspace_projection, NPCS_KEEP, \n",
    "                                                    n_min_trials_per_shape = n_min_trials_per_shape, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                                    remove_singleprims_unstable=remove_singleprims_unstable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540d6ed",
   "metadata": {},
   "source": [
    "# [3] Samp, Go, reach onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved\n",
    "\n",
    "SAVEDIR_ANALYSIS = \"/tmp/SAMP\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFallpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min_trials_per_shape = 4\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "tbin_dur = 0.1\n",
    "tbin_slide = 0.02\n",
    "twind_analy = (-1, 1)\n",
    "NPCS_KEEP = 7\n",
    "events_keep = [\"03_samp\", \"05_first_raise\"]\n",
    "\n",
    "HACK = True\n",
    "for i, row in DFallpa.iterrows():\n",
    "    bregion = row[\"bregion\"]\n",
    "    which_level = row[\"which_level\"]\n",
    "    event = row[\"event\"]\n",
    "    PA = row[\"pa\"]\n",
    "\n",
    "    if HACK and bregion not in [\"PMv\"]:\n",
    "        continue\n",
    "\n",
    "    if event in events_keep:\n",
    "        for prune_version in [\"sp_char\"]:\n",
    "            for subspace_projection in [None, \"pca\", \"task_shape\"]: # NOTE: shape_prims_single not great, you lose some part of preSMA context-dependence...\n",
    "                for remove_drift in [False]:\n",
    "                    for raw_subtract_mean_each_timepoint in [False]:\n",
    "                        for remove_singleprims_unstable in [False, True]:\n",
    "                            SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{which_level}-{bregion}-{event}-prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-subtrmean={raw_subtract_mean_each_timepoint}\"\n",
    "                            os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                            print(\"SAVING AT ... \", SAVEDIR)\n",
    "                            euclidian_time_resolved(animal, date, PA, which_level, prune_version, remove_drift, SAVEDIR, twind_analy,\n",
    "                                                        tbin_dur, tbin_slide, \n",
    "                                                        subspace_projection, NPCS_KEEP, \n",
    "                                                        n_min_trials_per_shape = n_min_trials_per_shape, raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                                        remove_singleprims_unstable=remove_singleprims_unstable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a5e36",
   "metadata": {},
   "source": [
    "# Time warping based on behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each trial, get beh stroke\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8385ea",
   "metadata": {},
   "source": [
    "# Checking drift of FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each trial.\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all\n",
    "plot_heatmap_firing_rates_all(PA, savedir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f627",
   "metadata": {},
   "source": [
    "### Sanity check, change over day?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = f\"{SAVEDIR_ANALYSIS}/drift\"\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681090c8",
   "metadata": {},
   "source": [
    "##### For each region, plot sm fr across trials and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all bregions\n",
    "assert False, \"run for both chans and PCs\"\n",
    "for i, row in DFallpa.iterrows():\n",
    "    PAthis = row[\"pa\"]\n",
    "    bregion = row[\"bregion\"]\n",
    "\n",
    "    dur = 0.1\n",
    "    slide = 0.02\n",
    "    PAthis = PAthis.agg_by_time_windows_binned(dur, slide)\n",
    "\n",
    "\n",
    "    from neuralmonkey.neuralplots.population import heatmapwrapper_stratified_each_neuron\n",
    "    fig = heatmapwrapper_stratified_each_neuron(PA, \"task_kind\")\n",
    "    assert False\n",
    "    assert False\n",
    "    savefig(fig, f\"{savedir}/{bregion}.png\")\n",
    "    plt.close(\"all\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR = f\"/tmp/CHAR_HEATMAPS\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) First, dim reduction\n",
    "superv_dpca_var = superv_dpca_params['superv_dpca_var']\n",
    "superv_dpca_vars_group = superv_dpca_params['superv_dpca_vars_group']\n",
    "superv_dpca_filtdict = superv_dpca_params['superv_dpca_filtdict']\n",
    "\n",
    "\n",
    "dim_red_method = \"pca\"\n",
    "twind_analy = (-0.15, 0.3)\n",
    "tbin_dur = 0.1\n",
    "tbin_slice = 0.01\n",
    "NPCS_KEEP = 10\n",
    "_, PAredu = PA.dataextract_dimred_wrapper(\"traj\", dim_red_method, savedir, \n",
    "                                twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slice, \n",
    "                                NPCS_KEEP = NPCS_KEEP,\n",
    "                                dpca_var = None, dpca_vars_group = None, dpca_filtdict=None, \n",
    "                                dpca_proj_twind = twind_analy, \n",
    "                                raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                                umap_n_components=None, umap_n_neighbors=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d50581",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Script for heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16207999",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import params_subspace_projection\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all, plot_heatmap_firing_rates_all_wrapper\n",
    "\n",
    "plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR, animal, date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94190db2",
   "metadata": {},
   "source": [
    "### Accounting for drift -- exclude cases with very different PSTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff745869",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = DFallpa[\"pa\"].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_clean_stable_single_prims_frate\n",
    "savedir = \"/tmp/test\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "preprocess_clean_stable_single_prims_frate(PA, savedir=savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab18d69",
   "metadata": {},
   "source": [
    "##### Re-apply filter based on dirty sites, being more strict about drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each grouping, get the meanPSTH, and c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae030b8",
   "metadata": {},
   "source": [
    "# MULT DATA - euclidian_time_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5204605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER, _REGIONS_IN_ORDER_COMBINED\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b801cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import  load_all_dates\n",
    "# Diego (trial, all)\n",
    "LIST_ANIMAL_DATE_COMB = [\n",
    "    (\"Diego\", 231122, True),\n",
    "    (\"Diego\", 231128, True),\n",
    "    (\"Diego\", 231129, True),\n",
    "    (\"Diego\", 231201, True),\n",
    "    (\"Diego\", 231204, True),\n",
    "    (\"Diego\", 231205, True),\n",
    "    (\"Diego\", 231211, True),\n",
    "    (\"Diego\", 231213, True),\n",
    "]\n",
    "which_level = \"trial\"\n",
    "savedir_method_old = False\n",
    "\n",
    "DFDIST = load_all_dates(LIST_ANIMAL_DATE_COMB, which_level, savedir_method_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55770662",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For plots, see analy_euclidian_chars_sp_MULT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad33c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import  plot_scalar_all\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620034e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_scalar = [-0.3, 0.1]\n",
    "DFDISTthis = DFDIST[(DFDIST[\"time_bin\"]>=twind_scalar[0]) & (DFDIST[\"time_bin\"]<=twind_scalar[1])].reset_index(drop=True)\n",
    "\n",
    "# Agg, averaging over time\n",
    "DFTHISscal = aggregGeneral(DFDISTthis, [\"animal\", \"date\", \"combine_areas\", \"event\", \"bregion\", \"metaparams\", \"same-task|shape\", \"prune_version\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                \"remove_singleprims_unstable\"], values=[\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_event_to_twind = {\n",
    "    \"03_samp\":[0.2, 1.0],\n",
    "    \"05_first_raise\":[-0.5,  0],\n",
    "}\n",
    "\n",
    "list_df = []\n",
    "for event, twind_scalar in map_event_to_twind.items():\n",
    "    dfthis = DFDIST[DFDIST[\"event\"] == event]\n",
    "    dfthis_sub = dfthis[(dfthis[\"time_bin\"]>=twind_scalar[0]-0.001) & (dfthis[\"time_bin\"]<=twind_scalar[1]+0.001)].reset_index(drop=True)\n",
    "    list_df.append(dfthis_sub)\n",
    "DFDISTthis = pd.concat(list_df).reset_index(drop=True)\n",
    "\n",
    "# Agg, averaging over time\n",
    "DFTHISscal = aggregGeneral(DFDISTthis, [\"animal\", \"date\", \"combine_areas\", \"event\", \"bregion\", \"metaparams\", \"same-task|shape\", \"prune_version\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                \"remove_singleprims_unstable\"], values=[\"dist_mean\", \"dist_norm\", \"dist_yue_diff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f82a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.expttools import writeDictToTxt\n",
    "writeDictToTxt(map_event_to_twind, f\"{savedir}/twind_params.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67423387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final scalar plot\n",
    "DFTHISscal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/tmp/SCALAR\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFTHISscal, [\"event\", \"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                                                \"remove_singleprims_unstable\"])\n",
    "for (event, subspace_projection, remove_drift, raw_subtract_mean_each_timepoint, remove_singleprims_unstable), inds in grpdict.items():\n",
    "    dfthis = DFTHISscal.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"same-task|shape\", \"0|1\", \"1|0\", \n",
    "                                        \"prune_version\", \"dist_yue_diff\", \"bregion\", \n",
    "                                        True, shareaxes=True, SIZE=4);\n",
    "    savefig(fig, f\"{savedir}/EVENT={event}-ss={subspace_projection}-rmvdrift={remove_drift}-subtrmean={raw_subtract_mean_each_timepoint}-rmvunstable={remove_singleprims_unstable}.pdf\")\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bafbb",
   "metadata": {},
   "source": [
    "##### Convert to scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "twind_scalar = [-0.3, 0.1]\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp_MULT import plot_scalar_all\n",
    "plot_scalar_all(DFDIST, \"/tmp\", twind_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d389c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e25171",
   "metadata": {},
   "source": [
    "### Combine all kinds of pairwise comparisons in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "savedir = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFTHISscal, [\"subspace_projection\", \"remove_drift\", \"raw_subtract_mean_each_timepoint\", \n",
    "                                                                   \"remove_singleprims_unstable\"])\n",
    "for (subspace_projection, remove_drift, raw_subtract_mean_each_timepoint, remove_singleprims_unstable), inds in grpdict.items():\n",
    "    dfthis = DFTHISscal.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"same-task|shape\", \"0|1\", \"1|0\", \n",
    "                                        \"metaparams\", \"dist_yue_diff\", \"bregion\", \n",
    "                                        True, shareaxes=True, SIZE=4);    \n",
    "    savefig(fig, f\"{savedir}/ss={subspace_projection}-rmvdrift={remove_drift}-subtrmean={raw_subtract_mean_each_timepoint}-rmvunstable={remove_singleprims_unstable}\")\n",
    "    plt.close(\"all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38710bc",
   "metadata": {},
   "source": [
    "# Example figures (for paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b557cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d154fd",
   "metadata": {},
   "source": [
    "### [Code] plot_heatmap_firing_rates_all_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n",
    "PA = extract_single_pa(DFallpa, bregion, which_level=\"stroke\", event=\"00_stroke\")\n",
    "print(PA.X.shape)\n",
    "\n",
    "prune_version = \"sp_char_0\"\n",
    "n_min_trials_per_shape = 4\n",
    "shape_var = \"shape_semantic_grp\"\n",
    "plot_drawings = False\n",
    "# twind_analy = (-0.4, 0.5)\n",
    "twind_analy = (-0.35, 0.3)\n",
    "tbin_dur=0.1\n",
    "tbin_slide=0.01\n",
    "\n",
    "NPCS_KEEP = 10\n",
    "subspace_projection = \"pca\"\n",
    "# NPCS_KEEP = None\n",
    "# subspace_projection = None\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "savedir = \"/tmp\"\n",
    "PA = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                    n_min_trials_per_shape=n_min_trials_per_shape, shape_var=shape_var, plot_drawings=plot_drawings,\n",
    "                    remove_chans_fr_drift=False,\n",
    "                    subspace_projection=subspace_projection, \n",
    "                        twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                        raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint,\n",
    "                        remove_singleprims_unstable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a34533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import plot_heatmap_firing_rates_all_wrapper\n",
    "import os\n",
    "SAVEDIR_ANALYSIS = \"/tmp/HEATMAP_WRAPPER\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "plot_heatmap_firing_rates_all_wrapper(DFallpa, SAVEDIR_ANALYSIS, animal, date, \n",
    "                                      DEBUG_skip_drawings=True, DEBUG_bregion=\"PMv\",\n",
    "                                      DEBUG_subspace_projection=\"pca_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import behstrokes_preprocess_assign_col_bad_strokes, preprocess_pa\n",
    "behstrokes_preprocess_assign_col_bad_strokes(DFallpa, animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b834c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import load_session_helper, load_mult_session_helper\n",
    "MS = load_mult_session_helper(date, animal)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import beh_plot_event_timing_stroke\n",
    "savedir = \"/tmp\"\n",
    "PA = DFallpa.iloc[0][\"pa\"]\n",
    "beh_plot_event_timing_stroke(PA, animal, date, savedir, MS=MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca326e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2320d",
   "metadata": {},
   "source": [
    "### [MULT DAYS] for: euclidian_time_resolved_fast_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidian_time_resolved_fast_shuffled_mult_scatter_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac18ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (1) Original plots, still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Original paper\n",
    "    analysis_kind = \"char_sp_00_stroke\"\n",
    "    # analysis_kind = \"char_sp_05_first_raise\"\n",
    "    # analysis_kind = \"char_sp_04_go_cue\"\n",
    "else:\n",
    "    # Revision\n",
    "    # analysis_kind = \"char_sp_00_stroke_revision_noregr\"\n",
    "    analysis_kind = \"char_sp_00_stroke_revision_regr\"\n",
    "    analysis_kind = \"char_sp_00_stroke_revision_regr_seman\"\n",
    "    \n",
    "prune_min_n_trials = None\n",
    "DFDISTS, DFDISTS_AGG = euclidian_time_resolved_fast_shuffled_mult_scatter_plots(analysis_kind, just_return_df=True, \n",
    "                                                                                               DO_FURTHER_POSTPROCESSING=False,\n",
    "                                                                                               prune_min_n_trials=prune_min_n_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384f060",
   "metadata": {},
   "source": [
    "##### Random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/EUCL_QUICK_SHUFFLE_revision-VAR_SHAPE=shape_semantic_grp/Diego-231220-combine=True-wl=stroke-00_stroke-regrhack=True/list_dfdist.pkl\"\n",
    "# path = \"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/EUCL_QUICK_SHUFFLE_revision/Diego-231220-combine=True-wl=stroke-00_stroke-regrhack=True/list_dfdist.pkl\"\n",
    "import pickle\n",
    "with open(path, \"rb\") as f:\n",
    "    list_dfdist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import grouping_print_n_samples, grouping_plot_n_samples_conjunction_heatmap\n",
    "from pythonlib.tools.plottools import savefig\n",
    "savedir = \"/tmp\"\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFDISTS, [\"animal\", \"date\", \"prune_version\"])\n",
    "print(len(grpdict))\n",
    "for grp, inds in grpdict.items():\n",
    "    print(grp)\n",
    "    dfdists = DFDISTS.iloc[inds].reset_index(drop=True)\n",
    "    # grouping_print_n_samples(dfdists, [\"animal\", \"date\", \"metaparams\", \"same-shape_semantic_grp|task_kind\", \"shape_semantic_grp_1\", \"shape_semantic_grp_2\"])\n",
    "    # grouping_print_n_samples(dfdists, [\"same-shape_semantic_grp|task_kind\", \"shape_semantic_grp_1\", \"shape_semantic_grp_2\", \"animal\", \"date\"])\n",
    "    # asds\n",
    "    fig = grouping_plot_n_samples_conjunction_heatmap(dfdists, \"shape_semantic_grp_1\", \"shape_semantic_grp_2\", \n",
    "                                                [\"same-shape_semantic_grp|task_kind\"], annotate_heatmap=False,\n",
    "                                                FIGSIZE=4, n_columns=4)\n",
    "    savefig(fig, f\"{savedir}/allpairs_counts-{grp}.pdf\")\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149a78c",
   "metadata": {},
   "source": [
    "### Good, for revision (all stroke indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88982fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    import time\n",
    "    nhrs = 4\n",
    "    nsec = nhrs * 60 * 60\n",
    "    print(\"sleeping this many sec: \", nsec)\n",
    "    time.sleep(nsec)\n",
    "\n",
    "# Just plot a subset, or else will take forever\n",
    "# list_metaparams_plot_each_pair = [\n",
    "#     \"sp_char_1plus|task_shape_si|(-0.8, 0.3)|00_stroke|(-0.5, -0.05)\"\n",
    "#     ]\n",
    "list_metaparams_plot_each_pair = [\n",
    "    # \"sp_char_1plus|task_shape|(-0.8, 0.3)|00_stroke|(-0.5, -0.05)\"\n",
    "    \"sp_char_0|task_shape|(-0.8, 0.3)|00_stroke|(-0.35, -0.05)\",\n",
    "    \"sp_char_1plus|task_shape|(-0.8, 0.3)|00_stroke|(-0.35, -0.05)\",\n",
    "    \"sp_pig_1plus|task_shape|(-0.8, 0.3)|00_stroke|(-0.35, -0.05)\",\n",
    "    ]\n",
    "\n",
    "for PLOT_EACH_PAIR in [False, True]: # First False to run thru quickly\n",
    "    # for analysis_kind in [\"char_sp_00_stroke_revision_regr\", \"char_sp_00_stroke_revision_noregr\"]:\n",
    "    # for analysis_kind in [\"char_sp_00_stroke_revision_regr\"]:\n",
    "    for analysis_kind in [\"char_sp_00_stroke_revision_regr_seman\", \"char_sp_00_stroke_revision_noregr_seman\"]:\n",
    "        # for DO_FURTHER_POSTPROCESSING in [True, False]:\n",
    "        for DO_FURTHER_POSTPROCESSING in [True]: # False doesnt do much\n",
    "        # for DO_FURTHER_POSTPROCESSING in [False]: # False is quicker\n",
    "            # for prune_min_n_trials in [None, 4]:\n",
    "            for prune_min_n_trials in [None]:\n",
    "\n",
    "                # Only plot each pair one time\n",
    "                if PLOT_EACH_PAIR:\n",
    "                    a = (prune_min_n_trials is None) and (DO_FURTHER_POSTPROCESSING is True)\n",
    "                    if not a:\n",
    "                        continue\n",
    "                euclidian_time_resolved_fast_shuffled_mult_scatter_plots(analysis_kind, \n",
    "                                                                        DO_FURTHER_POSTPROCESSING=DO_FURTHER_POSTPROCESSING,\n",
    "                                                                        PLOT_EACH_PAIR=PLOT_EACH_PAIR, \n",
    "                                                                        prune_min_n_trials=prune_min_n_trials,\n",
    "                                                                        list_metaparams_plot_each_pair=list_metaparams_plot_each_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Debug -- Try replotting after removing specific shapes.\n",
    "var_effect = \"shape_semantic\"\n",
    "var_value = \"dist_yue_diff\"\n",
    "var_same_same = f\"same-{var_effect}|task_kind\"\n",
    "\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping, grouping_append_and_return_inner_items_good\n",
    "\n",
    "### Original\n",
    "if False:\n",
    "    grp_vars = [\"which_level\", \"prune_version\", \"subspace|twind\", \"event\"]\n",
    "    grpdict_dat = grouping_append_and_return_inner_items_good(DFDISTS_AGG, grp_vars)\n",
    "\n",
    "\n",
    "    dfthis_dat = DFDISTS\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis_dat, var_same_same, \"1|0\", \"0|1\", \"metaparams\", \n",
    "                                        var_value, \"bregion\", True, shareaxes=True, SIZE=3.5)\n",
    "\n",
    "### Exclude line\n",
    "# DFDISTS = DFDISTS[[x[0]!=\"line-UL-UL\" for x in DFDISTS[\"labels_1\"]]].reset_index(drop=True)\n",
    "# dfthis_dat = DFDISTS[[\"ARC\" in x[0] for x in DFDISTS[\"labels_1\"]]].reset_index(drop=True)\n",
    "# dfthis_dat = DFDISTS[[\"line\" not in x[0] for x in DFDISTS[\"labels_1\"]]].reset_index(drop=True)\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _remove_shape\n",
    "list_sh_remove = [\"Lcentered-DR-DR\", \"V-DD-DD\"]\n",
    "DFDISTS_THIS, DFDISTS_THIS_AGG = _remove_shape(DFDISTS, list_sh_remove, var_same_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter, stats\n",
    "var_datapt = None\n",
    "savedir = \"/tmp/STATS\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "# (3) Specific subspace projections.\n",
    "list_subspace = [\"task_shape\"]\n",
    "prune_version = \"sp_char_1plus\"\n",
    "\n",
    "dfdists = DFDISTS_THIS[DFDISTS_THIS[\"prune_version\"] == prune_version].reset_index(drop=True)\n",
    "assert len(dfdists)>0\n",
    "\n",
    "# Postprocess\n",
    "dfdists[\"labels_pair_unique\"] = [tuple(sorted((row[\"labels_1\"], row[\"labels_2\"]))) for _, row in dfdists.iterrows()]\n",
    "dfdists[\"shapes_pair_unique\"] = [tuple(sorted((row[\"labels_1\"][0], row[\"labels_2\"][0]))) for _, row in dfdists.iterrows()]\n",
    "dfdists = _euclidianshuff_stats_linear_load_mult_dates_postprocess(dfdists)\n",
    "\n",
    "### Plot\n",
    "DFSTATS_2BR = _euclidianshuff_stats_linear_2br_scatter_wrapper(dfdists, var_same_same, var_datapt, savedir, \n",
    "                                                               plot_heatmap_counts=False, plot_catplots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DFDISTS = DFDISTS[[x[0]!=\"line-UL-UL\" for x in DFDISTS[\"labels_1\"]]].reset_index(drop=True)\n",
    "# dfthis_dat = DFDISTS[[\"ARC\" in x[0] for x in DFDISTS[\"labels_1\"]]].reset_index(drop=True)\n",
    "for shape in [\"line\", \"ARC\", \"ZZ\", \"circle\", \"Lcentered\"]:\n",
    "    print(shape)\n",
    "    dfthis_dat = DFDISTS[[shape in x[0] for x in DFDISTS[\"labels_1\"]]].reset_index(drop=True)\n",
    "\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(dfthis_dat, var_same_same, \"1|0\", \"0|1\", \"metaparams\", \n",
    "                                        var_value, \"bregion\", True, shareaxes=True, SIZE=3.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82033c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Debug, prune to remove cases with too low n datapts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmin = 5\n",
    "\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFDISTS, [\"metaparams\", \"animal\", \"date\"])\n",
    "list_dfdist =[]\n",
    "for grp, inds in grpdict.items():\n",
    "    dfdists = DFDISTS.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "    a = dfdists.loc[:, [\"shape_semantic_grp_1\", \"n1\", \"task_kind_1\"]].values.tolist()\n",
    "    b = dfdists.loc[:, [\"shape_semantic_grp_2\", \"n2\", \"task_kind_2\"]].values.tolist()\n",
    "    dftmp = pd.DataFrame(a+b, columns=[var_effect, \"n\", \"task_kind\"])\n",
    "\n",
    "    if False:\n",
    "        from pythonlib.tools.pandastools import grouping_print_n_samples\n",
    "        grouping_print_n_samples(dftmp, [\"shape_semantic_grp\", \"n\", \"task_kind\"])\n",
    "\n",
    "    # The pool of shapes to ignore\n",
    "    shapes_ignore = dftmp[dftmp[\"n\"] < nmin][\"shape_semantic_grp\"].unique().tolist()\n",
    "\n",
    "    # Keep just shapes that are not ignored\n",
    "    a = dfdists[f\"{var_effect}_1\"].isin(shapes_ignore)\n",
    "    b = dfdists[f\"{var_effect}_2\"].isin(shapes_ignore)\n",
    "\n",
    "    dfdists_new = dfdists[~(a | b)].reset_index(drop=True)\n",
    "\n",
    "    # Store\n",
    "    list_dfdist.append(dfdists_new)\n",
    "\n",
    "    # print(grp, \"       |       \", len(dfdists), \" ---> \", len(dfdists_new), f\"[{len(dfdists_new)/len(dfdists):.2f}]\")\n",
    "\n",
    "DFDISTS = pd.concat(list_dfdist).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary plot\n",
    "dfthis_dat = DFDISTS[DFDISTS[\"metaparams\"] == \"sp_char_1plus|task_shape_si|(-0.8, 0.3)|00_stroke|(-0.5, -0.05)\"].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee855f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = dfthis_dat[\"n1\"]<nmin\n",
    "b = dfthis_dat[\"n2\"]<nmin\n",
    "print(sum(a), sum(b), len(a))\n",
    "\n",
    "dfthis_dat = dfthis_dat[(dfthis_dat[\"n1\"]>=nmin) & (dfthis_dat[\"n2\"]>=nmin)].reset_index(drop=True)\n",
    "\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping, grouping_append_and_return_inner_items_good\n",
    "var_same_same = \"same-shape_semantic_grp|task_kind\"\n",
    "var_value = \"dist_yue_diff\"\n",
    "_, fig = plot_45scatter_means_flexible_grouping(dfthis_dat, var_same_same, \"1|0\", \"0|1\", \"metaparams\", \n",
    "                                    var_value, \"bregion\", True, shareaxes=True, SIZE=3.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc6b03",
   "metadata": {},
   "source": [
    "##### Find outlier points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c88d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DFDISTS[\"bregion\"] == \"PMv\"\n",
    "b = DFDISTS[\"metaparams\"] == 'sp_char_1plus|task_shape_si|(-0.8, 0.3)|00_stroke|(-0.3, 0.0)'\n",
    "c = DFDISTS[\"same-shape_semantic|task_kind\"] == \"1|0\"\n",
    "dfdists = DFDISTS[a & b & c]\n",
    "\n",
    "dfdists[\"dist_yue_diff\"].hist(bins=30)\n",
    "\n",
    "dfdists[dfdists[\"dist_yue_diff\"]>0.2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05195bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution for each shapes\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "from neuralmonkey.analyses.euclidian_distance import dfdist_summary_plots_wrapper\n",
    "\n",
    "shapes = sorted(set(DFDISTS[\"shape_semantic_1\"].tolist() + DFDISTS[\"shape_semantic_2\"].tolist()))\n",
    "for sh in shapes:\n",
    "    dfdists = DFDISTS[(DFDISTS[\"shape_semantic_1\"]==sh) | (DFDISTS[\"shape_semantic_2\"]==sh)].reset_index(drop=True)\n",
    "    dfdists_agg = aggregGeneral(dfdists, [\"animal\", \"date\", \"metaparams\", \"bregion\", \"prune_version\", \n",
    "                                            \"which_level\", \"event\", \"subspace|twind\", var_same_same],\n",
    "                                [\"dist_mean\", \"dist_norm\", \"dist_yue_diff\", \"DIST_50\", \"DIST_98\"], nonnumercols=\"all\")\n",
    "        \n",
    "    savedir = f\"/tmp/TESTING/shape={sh}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    do_catplots = False\n",
    "    do_quick = True\n",
    "    dfdist_summary_plots_wrapper(dfdists, dfdists_agg, var_effect, var_other, SAVEDIR=savedir,\n",
    "                                PLOT_EACH_PAIR=False,  \n",
    "                                list_metaparams_plot_each_pair=list_metaparams_plot_each_pair,\n",
    "                                do_catplots=do_catplots, do_quick=do_quick)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547778",
   "metadata": {},
   "source": [
    "##### (2) Stats, and scatter plots. Good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _euclidianshuff_stats_linear_load_mult_dates_postprocess, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidian_time_resolved_fast_shuffled_mult_reload, _euclidian_time_resolved_fast_shuffled_mult_scatter_plots_params\n",
    "from pythonlib.tools.pandastools import convert_var_to_categorical\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidianshuff_stats_linear_load, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n",
    "from pythonlib.tools.pandastools import convert_var_to_categorical\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from neuralmonkey.neuralplots.brainschematic import datamod_reorder_by_bregion\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from pythonlib.tools.statstools import signrank_wilcoxon_from_df\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidianshuff_stats_linear_load, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n",
    "import os\n",
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _euclidianshuff_stats_linear_load_mult_dates_postprocess, euclidianshuff_stats_linear_load, euclidianshuff_stats_linear_plot_wrapper, euclidian_time_resolved_fast_shuffled_mult_reload, euclidian_time_resolved_fast_shuffled_mult_scatter_plots\n",
    "\n",
    "subsample_plot_each_iter_stats = True\n",
    "\n",
    "if False:\n",
    "    # Original submission params\n",
    "    merge_pancho_ss_twinds=False\n",
    "    plot_coeff = False\n",
    "    # analysis_kind = \"char_sp\"\n",
    "    # analysis_kind = \"char_sp_05_first_raise\"\n",
    "    # analysis_kind = \"char_sp_04_go_cue\"\n",
    "    analysis_kind = \"char_sp_00_stroke\"\n",
    "    do_each_date = False\n",
    "    var_effect = \"shape_semantic_grp\"\n",
    "    # for pruning.\n",
    "    list_prune_version = [\"sp_char_0\", \"pig_char_0\"]\n",
    "    list_subspace = [\"task_shape\", \"task_shape_si\"]\n",
    "    # list_subspace = [\"task_shape_si\", \"shape_all\"]\n",
    "    # list_prune_version = [\"sp_char_0\"]\n",
    "    # list_subspace = [\"task_shape_si\"]\n",
    "else:\n",
    "    # Revision params\n",
    "    # analysis_kind = \"char_sp_00_stroke_revision_regr_seman\"\n",
    "    do_each_date = False\n",
    "    var_effect = \"shape_semantic\"\n",
    "    # list_prune_version = [\"sp_char_0\", \"sp_char_1plus\", \"sp_pig_1plus\", \"pig0_char_1plus\"]\n",
    "    list_prune_version = [\"sp_char_0\", \"sp_char_1plus\", \"sp_pig_1plus\"]\n",
    "    list_subspace = [\"task_shape\", \"task_shape_si\"]\n",
    "    # list_subspace = [\"task_shape_si\"]\n",
    "    is_revision=True\n",
    "    consolidate_diego_shapes = True\n",
    "    \n",
    "# PRUNE_SHAPE = False\n",
    "old_run_number = None\n",
    "for PRUNE_SHAPE in [False]:\n",
    "    ## Revision -- loop \n",
    "    for analysis_kind in [\"char_sp_00_stroke_revision_noregr_seman\", \"char_sp_00_stroke_revision_regr_seman\"]:\n",
    "        do_each_date = False\n",
    "        var_effect = \"shape_semantic\"\n",
    "        # list_prune_version = [\"sp_char_0\", \"sp_char_1plus\", \"sp_pig_1plus\", \"pig0_char_1plus\"]\n",
    "        list_prune_version = [\"sp_char_0\", \"sp_char_1plus\", \"sp_pig_1plus\"]\n",
    "        # list_subspace = [\"task_shape_si\", \"task_shape\"]\n",
    "        list_subspace = [\"task_shape\"]\n",
    "        is_revision=True\n",
    "        consolidate_diego_shapes = True\n",
    "\n",
    "        for animal in [\"Diego\", \"Pancho\"]:\n",
    "\n",
    "            # if PRUNE_SHAPE==False and analysis_kind==\"char_sp_00_stroke_revision_regr_seman\" and animal==\"Diego\":\n",
    "            #     continue\n",
    "\n",
    "            for var_other in [\"task_kind\"]:\n",
    "\n",
    "                list_date = _euclidian_time_resolved_fast_shuffled_mult_scatter_plots_params(analysis_kind, animal, var_other)\n",
    "                var_same_same = f\"same-{var_effect}|{var_other}\"\n",
    "\n",
    "                if do_each_date:\n",
    "                    for date in list_date:\n",
    "                        dfdists, SAVEDIR_PLOTS = euclidianshuff_stats_linear_load(animal, date, var_other)\n",
    "\n",
    "                        ### ALL PLOTS\n",
    "                        euclidianshuff_stats_linear_plot_wrapper(dfdists, SAVEDIR_PLOTS, var_other)\n",
    "\n",
    "                from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import euclidianshuff_stats_linear_load_mult_dates\n",
    "                DFDISTS_ALL = euclidianshuff_stats_linear_load_mult_dates(animal, list_date, var_other, analysis_kind, \n",
    "                                                                        old_run_number=old_run_number)\n",
    "\n",
    "                for prune_version in list_prune_version:\n",
    "                    if old_run_number is None:\n",
    "                        if is_revision:\n",
    "                            SAVEDIR_PLOTS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/EUCL_QUICK_SHUFFLE_revision-VAR_SHAPE={var_effect}/MULT/stats_linear_model/{analysis_kind}/{animal}-{'|'.join([str(d) for d in list_date])}-var_other={var_other}-prune={prune_version}-ss={list_subspace}-prunesh={PRUNE_SHAPE}\"\n",
    "                            os.makedirs(SAVEDIR_PLOTS, exist_ok=True)\n",
    "                        else:\n",
    "                            SAVEDIR_PLOTS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/EUCL_QUICK_SHUFFLE/MULT/stats_linear_model/{analysis_kind}/{animal}-{'|'.join([str(d) for d in list_date])}-var_other={var_other}-prune={prune_version}-ss={list_subspace}-prunesh={PRUNE_SHAPE}\"\n",
    "                            os.makedirs(SAVEDIR_PLOTS, exist_ok=True)\n",
    "                    else:\n",
    "                        assert is_revision\n",
    "                        SAVEDIR_PLOTS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/EUCL_QUICK_SHUFFLE_revision_runs/run{old_run_number}/MULT/stats_linear_model/{analysis_kind}/{animal}-{'|'.join([str(d) for d in list_date])}-var_other={var_other}-prune={prune_version}-ss={list_subspace}-prunesh={PRUNE_SHAPE}\"\n",
    "                        os.makedirs(SAVEDIR_PLOTS, exist_ok=True)\n",
    "\n",
    "                    # Condition\n",
    "                    # (1) For now, just plot the ones that you care about (or else takes a long time)\n",
    "                    DFDISTS = DFDISTS_ALL[DFDISTS_ALL[\"prune_version\"] == prune_version].reset_index(drop=True)\n",
    "                    if len(DFDISTS)>0:\n",
    "                        # (2) FIgure if add one day that only had PIG to sp_char_0 analyses\n",
    "                        # Combine multiple prune versions.\n",
    "                        # In some cases the day doenst have single prims -- in which case should use PIG (pig_char_0).\n",
    "                        if False:\n",
    "                            if animal == \"Pancho\" and prune_version == \"sp_char_0\":    \n",
    "                                prune_extract_append = \"pig_char_0\"\n",
    "                                dates_extract_append = [220614]\n",
    "                            elif animal == \"Diego\" and prune_version == \"sp_char_0\":    \n",
    "                                prune_extract_append = \"pig_char_0\"\n",
    "                                dates_extract_append = [231120]\n",
    "                            else:\n",
    "                                prune_extract_append = None\n",
    "                                dates_extract_append = None\n",
    "                        else:\n",
    "                            # Revision, no need to add these\n",
    "                            prune_extract_append = None\n",
    "                            dates_extract_append = None\n",
    "\n",
    "                        if prune_extract_append is not None:\n",
    "                            assert not prune_extract_append == prune_version\n",
    "                            _df = DFDISTS_ALL[(DFDISTS_ALL[\"prune_version\"] == prune_extract_append) & (DFDISTS_ALL[\"date\"].isin(dates_extract_append))].reset_index(drop=True)\n",
    "                            # cleanup so that fakes that this is prims_single, or else downstream code fails.\n",
    "                            _df[\"prune_version\"] = prune_version\n",
    "                            def _replace_pig_with_sp(x):\n",
    "                                \"\"\"\n",
    "                                eg.\n",
    "                                x = (ARC-RR, prims_on_grid)\t\n",
    "                                returns:\n",
    "                                (ARC-RR, prims_single)\t\n",
    "                                \"\"\"\n",
    "                                \n",
    "                                if x[1]==\"prims_on_grid\":\n",
    "                                    return (x[0], \"prims_single\")\n",
    "                                else:\n",
    "                                    return (x[0], x[1])\n",
    "                            _df[\"labels_1\"] = [_replace_pig_with_sp(x) for x in _df[\"labels_1\"]]\n",
    "                            _df[\"labels_2\"] = [_replace_pig_with_sp(x) for x in _df[\"labels_2\"]]\n",
    "\n",
    "                            DFDISTS = pd.concat([DFDISTS, _df]).reset_index(drop=True)\n",
    "                        else:\n",
    "                            pass\n",
    "                        \n",
    "                        # Remove shapes with bad beh\n",
    "                        if PRUNE_SHAPE:\n",
    "                            from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _remove_shape\n",
    "                            if animal == \"Diego\":\n",
    "                                # list_sh_remove = [\"Lcentered-DR-DR\", \"V-DD-DD\"]\n",
    "                                # list_sh_remove = [\"arcdeep-RR-RR\", \"Lcentered-DL-DL\", \"Lcentered-DR-DR\", \"squiggle3-UU-0.0\", \"V-DD-DD\"]\n",
    "                                list_sh_remove = [\"Lcentered-DR-DR\"]\n",
    "                            elif animal == \"Pancho\":\n",
    "                                list_sh_remove = [\"line-UR-UR\"]\n",
    "                            DFDISTS, _ = _remove_shape(DFDISTS, list_sh_remove, var_same_same)\n",
    "\n",
    "                        # (3) Specific subspace projections.\n",
    "                        DFDISTS = DFDISTS[DFDISTS[\"subspace_projection\"].isin(list_subspace)].reset_index(drop=True)\n",
    "                        assert len(DFDISTS)>0\n",
    "\n",
    "                        ### Further consolidate Diego shapes\n",
    "                        if consolidate_diego_shapes:\n",
    "                            # merge all the shapes\n",
    "                            from pythonlib.drawmodel.tokens import MAP_SHAPESEM_TO_SHAPESEMGROUP, map_shsem_to_new_shsem\n",
    "                            from neuralmonkey.analyses.euclidian_distance import dfdist_extract_label_vars_specific\n",
    "                            from pythonlib.tools.pandastools import aggregGeneral\n",
    "\n",
    "                            # (1) Rename the labels\n",
    "                            # DFDISTS[\"labels_1\"].value_counts()\n",
    "                            DFDISTS[\"labels_1\"] = [x if x[0] not in map_shsem_to_new_shsem else (map_shsem_to_new_shsem[x[0]], x[1]) for x in DFDISTS[\"labels_1\"]]\n",
    "                            DFDISTS[\"labels_2\"] = [x if x[0] not in map_shsem_to_new_shsem else (map_shsem_to_new_shsem[x[0]], x[1]) for x in DFDISTS[\"labels_2\"]]\n",
    "\n",
    "                            if True:\n",
    "                                # (2) This important, becuase for Diego, if merge shapes, someitmes labels_1 is same as labels_2. Need to agg using uinque\n",
    "                                DFDISTS[\"labels_pair_unique\"] = [tuple(sorted((row[\"labels_1\"], row[\"labels_2\"]))) for _, row in DFDISTS.iterrows()]\n",
    "                                DFDISTS = aggregGeneral(DFDISTS, [\"animal\", \"date\", \"bregion\", \"prune_version\", \"which_level\", \"event\", \"metaparams\",\n",
    "                                                        \"subspace_projection\", \"subspace_projection_fitting_twind\", \"subspace|twind\",\n",
    "                                                            \"twind_scal\", \"labels_pair_unique\"], [\"dist_mean\", \"dist_norm\", \"dist_yue_diff\", \"DIST_50\", \"DIST_98\", \"n1\", \"n2\"])\n",
    "                                DFDISTS[\"labels_1\"] = [x[0] for x in DFDISTS[\"labels_pair_unique\"]]\n",
    "                                DFDISTS[\"labels_2\"] = [x[1] for x in DFDISTS[\"labels_pair_unique\"]]\n",
    "                            else:\n",
    "                                # (2) Average the labels if they exist\n",
    "                                DFDISTS = aggregGeneral(DFDISTS, [\"animal\", \"date\", \"bregion\", \"prune_version\", \"which_level\", \"event\", \"metaparams\",\n",
    "                                                        \"subspace_projection\", \"subspace_projection_fitting_twind\", \"subspace|twind\",\n",
    "                                                            \"twind_scal\", \"labels_1\", \"labels_2\"], [\"dist_mean\", \"dist_norm\", \"dist_yue_diff\", \"DIST_50\", \"DIST_98\", \"n1\", \"n2\"])\n",
    "                            # (3) Re-extract the conjunctive labels\n",
    "                            for col in [\"shape_semantic_1\", \"shape_semantic_2\", \"shape_semantic_same\", \"task_kind_1\", \"task_kind_2\", \"task_kind_same\", \"shape_semantic_12\", \"task_kind_12\", \"same-shape_semantic|task_kind\"]:\n",
    "                                if col in DFDISTS:\n",
    "                                    del DFDISTS[col]\n",
    "                            DFDISTS = dfdist_extract_label_vars_specific(DFDISTS, [var_effect, var_other])            \n",
    "\n",
    "                        # Postprocess\n",
    "                        DFDISTS[\"labels_pair_unique\"] = [tuple(sorted((row[\"labels_1\"], row[\"labels_2\"]))) for _, row in DFDISTS.iterrows()]\n",
    "                        DFDISTS[\"shapes_pair_unique\"] = [tuple(sorted((row[\"labels_1\"][0], row[\"labels_2\"][0]))) for _, row in DFDISTS.iterrows()]\n",
    "                        DFDISTS = _euclidianshuff_stats_linear_load_mult_dates_postprocess(DFDISTS)\n",
    "                        \n",
    "                        # Savedir\n",
    "                        from pythonlib.tools.expttools import writeDictToTxt\n",
    "                        writeDictToTxt(\n",
    "                            {\"list_date\":list_date, \n",
    "                            \"prune_version\":prune_version,\n",
    "                            \"prune_version_unique_exists\":DFDISTS[\"prune_version\"].unique().tolist(),\n",
    "                            \"list_subspace\":list_subspace,\n",
    "                            }, \n",
    "                            f\"{SAVEDIR_PLOTS}/params.txt\")\n",
    "                        _, _, _, _ = euclidianshuff_stats_linear_plot_wrapper(DFDISTS, SAVEDIR_PLOTS, var_other, \n",
    "                                                                              var_effect=var_effect, \n",
    "                                                                              subsample_plot_each_iter_stats = subsample_plot_each_iter_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _ = euclidianshuff_stats_linear_plot_wrapper(DFDISTS, SAVEDIR_PLOTS, var_other, var_effect=var_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _euclidianshuff_stats_linear_2br_scatter_wrapper_SUBSAMPLE\n",
    "SAVEDIR_PLOTS = \"/tmp/ASDASD\"\n",
    "os.makedirs(SAVEDIR_PLOTS, exist_ok=True)\n",
    "var_datapt = None\n",
    "_euclidianshuff_stats_linear_2br_scatter_wrapper_SUBSAMPLE(DFDISTS, var_effect, var_same_same, var_datapt, SAVEDIR_PLOTS, plot_each_iter_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb9af1",
   "metadata": {},
   "source": [
    "##### Code here (devo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58726d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR_ANALYSIS = \"/tmp/CHAR_SP\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False, \"save split, or else file gets to mult GB -- run seprately for each bregion.\"\n",
    "var_effect=\"shape_semantic_grp\"\n",
    "var_conj = \"task_kind\"\n",
    "vars_group = [var_effect, var_conj]\n",
    "N_MIN_TRIALS_PER_SHAPE = 4\n",
    "TWIND_ANALY = (-0.4, 0.5)\n",
    "NPCS_KEEP = 6\n",
    "DO_RSA_HEATMAPS = False\n",
    "\n",
    "SUBSPACE_PROJ_FIT_TWIND = {\n",
    "    \"00_stroke\":[(-0.5, -0.05), (0.05, 0.5), (-0.5, 0.5), (-0.4, 0.3)],\n",
    "}\n",
    "\n",
    "# LIST_SUBSPACE_PROJECTION = [None, \"pca_proj\", \"task_shape_si\", \"shape_prims_single\"]\n",
    "LIST_SUBSPACE_PROJECTION = [\"shape_prims_single\"]\n",
    "LIST_PRUNE_VERSION = [\"sp_char\"] # GOOD\n",
    "\n",
    "N_SPLITS = 2\n",
    "\n",
    "twind_analy = TWIND_ANALY\n",
    "tbin_dur = 0.2\n",
    "tbin_slide = 0.02\n",
    "\n",
    "map_event_to_listtwind_scal = {\n",
    "    # \"00_stroke\":[(-0.5, -0.05), (0.05, 0.5), (-0.3, 0.1)],\n",
    "    # \"00_stroke\":[(-0.5, -0.05), (-0.3, 0.1), (-0.3, 0.2), (-0.4, 0.3), (-0.2, 0.3), (0.05, 0.5)],\n",
    "    \"00_stroke\":[(0.05, 0.5)],\n",
    "    }\n",
    "\n",
    "list_dfdist =[]\n",
    "for _, row in DFallpa.iterrows():\n",
    "    bregion = row[\"bregion\"]\n",
    "    which_level = row[\"which_level\"]\n",
    "    event = row[\"event\"]\n",
    "    PA = row[\"pa\"]\n",
    "\n",
    "    for prune_version in LIST_PRUNE_VERSION:\n",
    "\n",
    "        for subspace_projection in LIST_SUBSPACE_PROJECTION:\n",
    "            # plot only cleaned up data.\n",
    "            list_unstable_badstrokes = [(True, True, True)]\n",
    "                \n",
    "            # for remove_drift in [False]:\n",
    "            for remove_drift, remove_singleprims_unstable, remove_trials_with_bad_strokes in list_unstable_badstrokes:\n",
    "\n",
    "                ############################\n",
    "                if subspace_projection in [None, \"pca\"]:\n",
    "                    list_fit_twind = [twind_analy]\n",
    "                else:\n",
    "                    list_fit_twind = SUBSPACE_PROJ_FIT_TWIND[event]\n",
    "                \n",
    "                for subspace_projection_fitting_twind in list_fit_twind:\n",
    "                    \n",
    "                    # Final save dir\n",
    "                    SAVEDIR = f\"{SAVEDIR_ANALYSIS}/{which_level}-{bregion}-{event}--prune={prune_version}-ss={subspace_projection}-nodrift={remove_drift}-SpUnstable={remove_singleprims_unstable}-RmBadStrks={remove_trials_with_bad_strokes}-fit_twind={subspace_projection_fitting_twind}\"\n",
    "                    os.makedirs(SAVEDIR, exist_ok=True)\n",
    "                    print(\"SAVING AT ... \", SAVEDIR)\n",
    "\n",
    "                    if DO_RSA_HEATMAPS:\n",
    "                        # Plot pairwise distances (rsa heatmaps).\n",
    "                        # This is done separatee to below becuase it doesnt use the train-test splits.\n",
    "                        # It shold but I would have to code way to merge multple Cl, which is doable.\n",
    "                        from neuralmonkey.analyses.euclidian_distance import timevarying_compute_fast_to_scalar\n",
    "\n",
    "                        # PAthis = preprocess_pa(PA, animal, date, var_other, None, remove_drift, \n",
    "                        #                        subspace_projection, subspace_projection_fitting_twind, \n",
    "                        #                        twind_analy, tbin_dur, tbin_slide, raw_subtract_mean_each_timepoint=False,\n",
    "                        #                        skip_dim_reduction=False)\n",
    "                        \n",
    "                        PAthis = preprocess_pa(animal, date, PA, None, prune_version, \n",
    "                                            n_min_trials_per_shape=N_MIN_TRIALS_PER_SHAPE, shape_var=var_effect, plot_drawings=False,\n",
    "                                            remove_chans_fr_drift=remove_drift, subspace_projection=subspace_projection, \n",
    "                                                twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                                                raw_subtract_mean_each_timepoint=False, remove_singleprims_unstable=remove_singleprims_unstable,\n",
    "                                                remove_trials_with_bad_strokes=remove_trials_with_bad_strokes, \n",
    "                                                subspace_projection_fitting_twind=subspace_projection_fitting_twind)\n",
    "                        \n",
    "                        list_twind_scalar = map_event_to_listtwind_scal[event]\n",
    "                        for twind_scal in list_twind_scalar:\n",
    "                            savedir = f\"{SAVEDIR}/rsa_heatmap/twindscal={twind_scal}\"\n",
    "                            os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                            # Prune to scalar window\n",
    "                            pa = PAthis.slice_by_dim_values_wrapper(\"times\", twind_scal)\n",
    "\n",
    "                            # Make rsa heatmaps.\n",
    "                            timevarying_compute_fast_to_scalar(pa, vars_group, rsa_heatmap_savedir=savedir)\n",
    "\n",
    "                    # Preprocess\n",
    "                    savedir = f\"{SAVEDIR}/preprocess\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    skip_dim_reduction = True # will do so below... THis just do other preprocessing, and widowing\n",
    "                    PAthis = preprocess_pa(animal, date, PA, savedir, prune_version, \n",
    "                                        n_min_trials_per_shape=N_MIN_TRIALS_PER_SHAPE, shape_var=var_effect, plot_drawings=False,\n",
    "                                        remove_chans_fr_drift=remove_drift, subspace_projection=subspace_projection, \n",
    "                                            twind_analy=twind_analy, tbin_dur=tbin_dur, tbin_slide=tbin_slide, NPCS_KEEP=NPCS_KEEP,\n",
    "                                            raw_subtract_mean_each_timepoint=False, remove_singleprims_unstable=remove_singleprims_unstable,\n",
    "                                            remove_trials_with_bad_strokes=remove_trials_with_bad_strokes, \n",
    "                                            subspace_projection_fitting_twind=subspace_projection_fitting_twind,\n",
    "                                            skip_dim_reduction=skip_dim_reduction)\n",
    "\n",
    "\n",
    "                    ########### DO TRAIN-TEST SPLITS\n",
    "                    folds_dflab = PAthis.split_balanced_stratified_kfold_subsample_level_of_var(vars_group, None, None, \n",
    "                                                                                                n_splits=N_SPLITS, \n",
    "                                                                                                do_balancing_of_train_inds=False)\n",
    "\n",
    "                    for _i_dimredu, (train_inds, test_inds) in enumerate(folds_dflab):\n",
    "                        # train_inds, more inds than than test_inds\n",
    "                        train_inds = [int(i) for i in train_inds]\n",
    "                        test_inds = [int(i) for i in test_inds]\n",
    "\n",
    "                        ############# DO DIM REDUCTION\n",
    "                        from neuralmonkey.scripts.analy_shape_invariance_all_plots_SP import _preprocess_pa_dim_reduction\n",
    "                        PAthisRedu = _preprocess_pa_dim_reduction(PAthis, subspace_projection, subspace_projection_fitting_twind,\n",
    "                                twind_analy, tbin_dur, tbin_slide, savedir=None, raw_subtract_mean_each_timepoint=False,\n",
    "                                inds_pa_fit=test_inds, inds_pa_final=train_inds)\n",
    "\n",
    "                        if PAthisRedu is None:\n",
    "                            print(\"SKIPPING, since PAthisRedu is None: \", SAVEDIR)\n",
    "                            assert False\n",
    "                        else:\n",
    "                            # Take different windows (for computing scalar score)\n",
    "                            # Go thru diff averaging windows (to get scalar)\n",
    "                            list_twind_scalar = map_event_to_listtwind_scal[event]\n",
    "                            for twind_scal in list_twind_scalar:\n",
    "                                \n",
    "                                pa = PAthisRedu.slice_by_dim_values_wrapper(\"times\", twind_scal)\n",
    "\n",
    "                                # ###################################### Running euclidian\n",
    "                                # from neuralmonkey.analyses.euclidian_distance import timevarying_compute_fast_to_scalar\n",
    "                                \n",
    "                                # # (1) Data\n",
    "                                # dfdist, _ = timevarying_compute_fast_to_scalar(pa, label_vars=vars_group)\n",
    "                                \n",
    "                                # dfdist[\"bregion\"] = bregion\n",
    "                                # dfdist[\"prune_version\"] = prune_version\n",
    "                                # dfdist[\"which_level\"] = which_level\n",
    "                                # dfdist[\"event\"] = event\n",
    "                                # dfdist[\"subspace_projection\"] = subspace_projection\n",
    "                                # dfdist[\"subspace_projection_fitting_twind\"] = [subspace_projection_fitting_twind for _ in range(len(dfdist))]\n",
    "                                # dfdist[\"dim_redu_fold\"] = _i_dimredu\n",
    "                                # dfdist[\"twind_scal\"] = [twind_scal for _ in range(len(dfdist))]\n",
    "                                # list_dfdist.append(dfdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PAthis.Xlabels[\"trials\"]\n",
    "\n",
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap\n",
    "grouping_plot_n_samples_conjunction_heatmap(dflab, \"shape_semantic_grp\", \"task_kind\", [\"stroke_index\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30155222",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_projection_fitting_twindsubspace_projection_fitting_twind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAthisRedu = _preprocess_pa_dim_reduction(PAthis, subspace_projection, subspace_projection_fitting_twind,\n",
    "        twind_analy, tbin_dur, tbin_slide, savedir=None, raw_subtract_mean_each_timepoint=False,\n",
    "        inds_pa_fit=test_inds, inds_pa_final=train_inds)\n",
    "print(PAthisRedu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5474aba",
   "metadata": {},
   "source": [
    "# [OLDER] Euclidian timecourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVEDIR_ANALYSIS = f\"/tmp/EUCL_TIMECOURSE\"\n",
    "os.makedirs(SAVEDIR_ANALYSIS, exist_ok=True)\n",
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import euclidian_time_resolved_wrapper\n",
    "euclidian_time_resolved_wrapper(animal, date, DFallpa, SAVEDIR_ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efc4f7",
   "metadata": {},
   "source": [
    "# Cross-temporal scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7466e",
   "metadata": {},
   "source": [
    "Consider each timebin for char vs. each timebin for SP.\n",
    "\n",
    "For each, get scores of diff|same, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# - Reshape PA so that time is on the trial axis.\n",
    "from neuralmonkey.classes.population import concatenate_popanals\n",
    "\n",
    "list_pa = []\n",
    "list_labels = []\n",
    "for i in range(len(PAredu.Times)):\n",
    "    pathis = PAredu.slice_by_dim_indices_wrapper(\"times\", [i])\n",
    "\n",
    "    # collect infoextract_snippets_trials\n",
    "    list_pa.append(pathis)\n",
    "    list_labels.append(i)\n",
    "\n",
    "PAreduScal = concatenate_popanals(list_pa, dim=\"trials\", \n",
    "                                map_idxpa_to_value=list_labels, \n",
    "                                map_idxpa_to_value_colname=\"time_bin\",\n",
    "                                assert_otherdims_have_same_values=False,\n",
    "                                times_realign_so_first_index_is_this_time=0)\n",
    "PAreduScal.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cldist = PAreduScal.dataextract_as_distance_matrix_clusters_flex([\"task_kind\", \"shape_semantic\", \"time_bin\"], \n",
    "                                                             return_as_single_mean_over_time=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8134eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres = Cldist.rsa_distmat_score_all_pairs_of_label_groups(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e21451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres[\"task_kind_same\"] = dfres[\"task_kind_1\"] == dfres[\"task_kind_2\"]\n",
    "dfres[\"shape_semantic_same\"] = dfres[\"shape_semantic_1\"] == dfres[\"shape_semantic_2\"]\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_same\", \"task_kind_same\"], \"shape_task_same\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"task_kind_1\", \"task_kind_2\"], \"task_kind_pair\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_1\", \"shape_semantic_2\"], \"shape_semantic_pair\")\n",
    "dfres = append_col_with_grp_index(dfres, [\"time_bin_1\", \"time_bin_2\"], \"time_bin_pair\")\n",
    "\n",
    "dfres = append_col_with_grp_index(dfres, [\"shape_semantic_same\", \"task_kind_1\", \"task_kind_2\"], \"ss_task_kind_pair\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c147809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f55380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subplots_heatmap(dfres, \"time_bin_1\", \"time_bin_2\", \"dist_yue_diff\", \"ss_task_kind_pair\", share_zlim=True)\n",
    "plot_subplots_heatmap(dfres, \"time_bin_1\", \"time_bin_2\", \"dist_mean\", \"ss_task_kind_pair\", share_zlim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9641ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take difference across subplots, plotting a new subplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ca7cd",
   "metadata": {},
   "source": [
    "# Check motor similarity of strokes from SP vs. CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_euclidian_chars_sp import preprocess_pa\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n",
    "\n",
    "savedir = \"/tmp/PREPROCESS\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "PA = preprocess_pa(PA, savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79e0a8",
   "metadata": {},
   "source": [
    "# Collect and plot across days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_SAVE = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/MULT\"\n",
    "\n",
    "list_animal_date = [\n",
    "    # (\"Pancho\", 230112),\n",
    "    # (\"Pancho\", 230117),\n",
    "    # (\"Pancho\", 230118),\n",
    "    (\"Pancho\", 230119),\n",
    "    (\"Pancho\", 230120),\n",
    "    (\"Pancho\", 230122),\n",
    "    (\"Pancho\", 230125),\n",
    "    (\"Pancho\", 230126),\n",
    "    (\"Pancho\", 230127),\n",
    "    ]\n",
    "\n",
    "combine = True\n",
    "ANIMAL = \"Pancho\"\n",
    "\n",
    "LIST_NPCS_KEEP = [4,6,2]\n",
    "for twind_analy in [(0.05, 0.25), (-0.05, 0.35), (0.1, 0.2)]:\n",
    "    for subspace_projection in [\"shape_prims_single\", \"pca\"]:\n",
    "        for prune_version in [\"sp_char_0\", \"sp_char\"]:\n",
    "            for NPCS_KEEP in LIST_NPCS_KEEP:\n",
    "                for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "\n",
    "                    # subspace_projection = \"shape_prims_single\"\n",
    "                    # prune_version = \"sp_char_0\"\n",
    "                    # NPCS_KEEP = 4\n",
    "                    # raw_subtract_mean_each_timepoint = False\n",
    "                    # twind_analy = (0.05, 0.25)\n",
    "\n",
    "                    try:\n",
    "                        ### Load all data for this params configuration\n",
    "                        list_dfres = []\n",
    "                        for animal, date in list_animal_date:\n",
    "                            if animal==ANIMAL:\n",
    "                                # animal = \"Pancho\"\n",
    "                                # date = 230126\n",
    "\n",
    "                                SAVEDIR_ANALYSIS = f\"/lemur2/lucas/analyses/recordings/main/euclidian_char_sp/{animal}-{date}-combine={combine}\"\n",
    "                                path = f\"{SAVEDIR_ANALYSIS}/subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}/summary/DFRES.pkl\"\n",
    "                                dfres = pd.read_pickle(path)\n",
    "\n",
    "                                dfres[\"animal\"] = animal\n",
    "                                dfres[\"date\"] = date\n",
    "\n",
    "                                list_dfres.append(dfres)\n",
    "                    except FileNotFoundError as err:\n",
    "                        print(\"Skipping, did not find all data...\", path)\n",
    "                        continue\n",
    "\n",
    "                    ######### PLOTS\n",
    "                    savedir = f\"{SAVEDIR_SAVE}/{ANIMAL}/combine={combine}-subspc={subspace_projection}-prunedat={prune_version}-npcs={NPCS_KEEP}-subtr={raw_subtract_mean_each_timepoint}-twind={twind_analy}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    DFRES = pd.concat(list_dfres).reset_index(drop=True)\n",
    "\n",
    "                    from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "                    DFRES = append_col_with_grp_index(DFRES, [\"animal\", \"date\"], \"ani_dat\")\n",
    "\n",
    "                    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", \"bregion\", \n",
    "                                                                    \"dist_yue_diff\", \"ani_dat\", True, SIZE=3, shareaxes=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-1.pdf\")\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", None, \n",
    "                                                                    \"dist_yue_diff\", \"bregion\", True, SIZE=4, shareaxes=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-2.pdf\")\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"shape_task_same\", \"1|0\", \"0|1\", \"bregion\", \n",
    "                                                                    \"dist_yue_diff\", \"ani_dat\", False, SIZE=6, shareaxes=True,\n",
    "                                                                    color_by_var_datapt=True, alpha=0.5,\n",
    "                                                                    force_all_on_same_axis=True);\n",
    "                    savefig(fig, f\"{savedir}/scatter-3.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=DFRES, x = \"bregion\", hue=\"shape_task_same\", y=\"dist_yue_diff\", kind=\"bar\", aspect=2)\n",
    "                    savefig(fig, f\"{savedir}/catplot-1.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=DFRES, x = \"bregion\", hue=\"ani_dat\", y=\"dist_yue_diff\", kind=\"point\", aspect=2, col=\"shape_task_same\", col_wrap=2)\n",
    "                    savefig(fig, f\"{savedir}/catplot-2.pdf\")\n",
    "\n",
    "\n",
    "                    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4440d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f403184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [MULT DAYS]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drag2_matlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
