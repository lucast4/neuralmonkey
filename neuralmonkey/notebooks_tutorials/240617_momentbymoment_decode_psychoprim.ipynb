{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d5b790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"5\\nDevo code for moment by moment decoding --> i.e., for each time bin, find strength of representation of\\neach class of a variable (e.g., shape), as opposed to decoding average accuracy across trials.\\n\\nPsychoprim --> goal is to decoder the base prims, given an ambiguous middle prim\"}\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\"5\n",
    "Devo code for moment by moment decoding --> i.e., for each time bin, find strength of representation of\n",
    "each class of a variable (e.g., shape), as opposed to decoding average accuracy across trials.\n",
    "\n",
    "Psychoprim --> goal is to decoder the base prims, given an ambiguous middle prim\"}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50813ff4",
   "metadata": {},
   "source": [
    "# Load a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf10d0",
   "metadata": {},
   "source": [
    "To load and plot a dataset of neural activity across population, in a PopAnal class object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53ac978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6345314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "DFallpa = load_handsaved_wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c98009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 - load a single DFallPA\n",
    "animal = \"Diego\"\n",
    "# date = 240523\n",
    "# date = 240515\n",
    "date = 240517\n",
    "\n",
    "DFallpa = load_handsaved_wrapper(animal=animal, date=date, version=\"trial\", question=\"SP_psycho_trial\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dccf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2 - Combine two dfallpa\n",
    "# DFallpa1 = load_handsaved_wrapper(animal=\"Diego\", date=230630, version=\"trial\")\n",
    "# DFallpa2 = load_handsaved_wrapper(animal=\"Diego\", date=230630, version=\"stroke\")\n",
    "# DFallpa = pd.concat([DFallpa1, DFallpa2]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2 - Combine two dfallpa\n",
    "# animal = \"Diego\"\n",
    "# date = 231211\n",
    "# COMBINE_AREAS = True\n",
    "\n",
    "# DFallpa1 = load_handsaved_wrapper(animal=animal, date=date, version=\"trial\", combine_areas=COMBINE_AREAS, use_time=False)\n",
    "# DFallpa2 = load_handsaved_wrapper(animal=animal, date=date, version=\"stroke\", combine_areas=COMBINE_AREAS, use_time=False)\n",
    "# DFallpa = pd.concat([DFallpa1, DFallpa2]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ab1b9",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f373551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import load_handsaved_wrapper, dfpa_match_chans_across_pa_each_bregion\n",
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34978a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVEDIR = f\"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/{animal}-{date}\"\n",
    "SAVEDIR = f\"/tmp/PSYCHO_SP/{animal}-{date}\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359b451",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e793d5",
   "metadata": {},
   "source": [
    "##### Devo -- removing noisy channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daefb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfpa_concatbregion_preprocess_wrapper\n",
    "dfpa_concatbregion_preprocess_wrapper(DFallpa, animal, date, fr_mean_subtract_method=\"across_time_bins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c9cf9",
   "metadata": {},
   "source": [
    "# Extract relevant variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd76772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_pig_decode_moment_syntaxTI import dfallpa_preprocess_condition\n",
    "# shape_var_suff = \"shapesemgrp\"|\n",
    "# loc_var_suff = \"loc_on_clust\"\n",
    "shape_var_suff = \"shape\"\n",
    "loc_var_suff = \"loc\"\n",
    "dfallpa_preprocess_condition(DFallpa, shape_var_suff, loc_var_suff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08206a",
   "metadata": {},
   "source": [
    "# Extract beh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import analy_psychoprim_prepare_beh_dataset\n",
    "DSmorphsets, map_tc_to_morph_info, map_morphset_to_basemorphinfo, map_tcmorphset_to_idxmorph, map_tcmorphset_to_info, \\\n",
    "    map_morphsetidx_to_assignedbase_or_ambig, map_tc_to_morph_status = analy_psychoprim_prepare_beh_dataset(animal, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f061b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune neural data to keep only those trialcodes that are in DSmorphsets\n",
    "list_trialcode = DSmorphsets.Dat[\"trialcode\"].unique().tolist()\n",
    "DFallpa[\"pa\"] = [pa.slice_by_labels(\"trials\", \"trialcode\", list_trialcode) for pa in DFallpa[\"pa\"].values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fa5de",
   "metadata": {},
   "source": [
    "# Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078863e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import train_decoder_helper, pipeline_train_test_scalar_score, pipeline_train_test_scalar_score_with_splits, pipeline_train_test_scalar_score_mult_train_dataset, test_decoder_helper, train_decoder_helper_extract_train_dataset\n",
    "from neuralmonkey.scripts.analy_pig_decode_moment_syntaxTI import get_dataset_params\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of base prim names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37ca80",
   "metadata": {},
   "source": [
    "### [GOOD] Train on data for a specific morphset -- get average score across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.plottools import savefig\n",
    "# SAVEDIR = \"/tmp/decoder_each_morphset\"\n",
    "print(SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c3c5d",
   "metadata": {},
   "source": [
    "##### [GOOD] Loop over all morphsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import analy_psychoprim_score_postsamp\n",
    "analy_psychoprim_score_postsamp(DFallpa, DSmorphsets, \n",
    "                                    map_tcmorphset_to_idxmorph, map_morphsetidx_to_assignedbase_or_ambig,\n",
    "                                    map_tcmorphset_to_info,\n",
    "                                    SAVEDIR, animal, date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e7610",
   "metadata": {},
   "source": [
    "##### METHOD 2 -- new - doing train/test split -- ask if base1/base2 decoders are better than own decoder (for ambig tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac268c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import analy_psychoprim_score_postsamp_better, _analy_psychoprim_score_postsamp_plot_scores\n",
    "\n",
    "\n",
    "SAVEDIR_BASE = \"/tmp/PSYCHOPRIM\"\n",
    "analy_psychoprim_score_postsamp_better(DFallpa, DSmorphsets, \n",
    "                                    map_tcmorphset_to_idxmorph, map_morphsetidx_to_assignedbase_or_ambig,\n",
    "                                    map_tcmorphset_to_info,\n",
    "                                    SAVEDIR_BASE, PLOT_EACH_IDX=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75fa95",
   "metadata": {},
   "source": [
    "##### METHOD 3 - like method 2 (i..e, using train/test split) but doing one analysis for each morphset (as oposed to each morph task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_moment import _analy_psychoprim_score_postsamp_plot_scores, analy_psychoprim_dfscores_condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b12667",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_BASE = \"/tmp/PSYCHONEW\"\n",
    "import seaborn as sns\n",
    "from pythonlib.tools.pandastools import savefig\n",
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n",
    "list_bregion = [bregion]\n",
    "\n",
    "# train_on_which_prims = \"all\" # all within morpjset\n",
    "# train_on_which_prims = \"base\" # just the 2 base prims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_agg_of_user_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed19d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscores_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8ca4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = f\"{SAVEDIR}/{bregion}/morphset={morphset}/plots\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "print(\"Saving plots at... \", savedir)\n",
    "_analy_psychoprim_score_postsamp_plot_scores(dfscores, savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. restrucict to idx_within that have a given set of decoders.\n",
    "# 2. \n",
    "from pythonlib.tools.pandastools import extract_with_levels_of_conjunction_vars\n",
    "\n",
    "# Clean comparison\n",
    "levels_get = [\"base1\", \"same\", \"base2\"]\n",
    "lenient_allow_data_if_has_n_levels = 2\n",
    "# lenient_allow_data_if_has_n_levels = len(levels_get)\n",
    "\n",
    "n_min = 1\n",
    "dfscores_this, dict_dfthis = extract_with_levels_of_conjunction_vars(DFSCORES, \"decoder_class_semantic_good\", \n",
    "                                                             [\"morph_set_idx|idx_within\"], levels_get, n_min,  \n",
    "                                                             lenient_allow_data_if_has_n_levels=lenient_allow_data_if_has_n_levels,\n",
    "                                                             prune_levels_with_low_n=True, \n",
    "                                                             plot_counts_heatmap_savepath=\"/tmp/counts.png\")\n",
    "\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "dfscores_this_agg = aggregGeneral(dfscores_this, [\"morph_set_idx|idx_within\", \"pa_class\", \"decoder_class\"], [\"score\"], nonnumercols=\"all\")\n",
    "\n",
    "_analy_psychoprim_score_postsamp_plot_scores(dfscores_this_agg, savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscores_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b10715",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaeaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot \n",
    "# var_score = \"score\"\n",
    "# x_var = \"decoder_class_semantic_good\"\n",
    "# x_order = (\"base1\", \"interm1\", \"same\", \"interm2\", \"base2\")\n",
    "# col = \"trial_morph_assigned_to_which_base\"\n",
    "# fig = sns.catplot(data=dfscores_this, x=x_var, y=var_score, hue = \"morph_set_idx\", kind=\"point\", errorbar=(\"ci\", 68), \n",
    "#                 col=col, order=x_order)\n",
    "# for ax in fig.axes.flatten():\n",
    "#     ax.axhline(0, color=\"k\", alpha=0.3)\n",
    "# savefig(fig, f\"{savedir}/decoder_class-x_var={x_var}-var_score={var_score}-col={col}-4.pdf\")\n",
    "\n",
    "# fig = sns.catplot(data=dfscores_this, x=x_var, y=var_score, hue = \"morph_set_idx|idx_within\", kind=\"point\", errorbar=(\"ci\", 68), \n",
    "#                 col=col, order=x_order)\n",
    "# for ax in fig.axes.flatten():\n",
    "#     ax.axhline(0, color=\"k\", alpha=0.3)\n",
    "# savefig(fig, f\"{savedir}/decoder_class-x_var={x_var}-var_score={var_score}-col={col}-4b.pdf\")\n",
    "\n",
    "##### Q: is the intermediate lower than same and base?\n",
    "# levels_get = [\"base1\", \"interm1\", \"same\"]\n",
    "# levels_get = [\"base2\", \"interm2\", \"same\"]\n",
    "# levels_get = [\"base2\", \"interm2\", \"same\"]\n",
    "levels_get = [\"same\", \"intermthis\", \"basethis\"]\n",
    "\n",
    "# Clean comparison\n",
    "lenient_allow_data_if_has_n_levels = len(levels_get)\n",
    "\n",
    "n_min = 1\n",
    "dfscores_this, dict_dfthis = extract_with_levels_of_conjunction_vars(DFSCORES, \"recoded_decoder\", \n",
    "                                                             [\"morph_set_idx|idx_within\"], levels_get, n_min,  \n",
    "                                                             lenient_allow_data_if_has_n_levels=lenient_allow_data_if_has_n_levels,\n",
    "                                                             prune_levels_with_low_n=True, \n",
    "                                                             plot_counts_heatmap_savepath=\"/tmp/counts.png\")\n",
    "\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "# dfscores_this_agg = aggregGeneral(dfscores_this, [\"morph_set_idx|idx_within\", \"pa_class\", \"decoder_class\", \"trial_morph_assigned_to_which_base\"], [\"score\"], nonnumercols=\"all\")\n",
    "\n",
    "_analy_psychoprim_score_postsamp_plot_scores(dfscores_this_agg, savedir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot \n",
    "# 1. restrucict to idx_within that have a given set of decoders.\n",
    "# 2. \n",
    "from pythonlib.tools.pandastools import extract_with_levels_of_conjunction_vars\n",
    "\n",
    "##### Q: \n",
    "# match the morphset to ask whether ambig is diff from not-ambig\n",
    "levels_get = [\"not_ambig_base1\", \"ambig_base1\"]\n",
    "n_min = 1\n",
    "dfscores_this, dict_dfthis = extract_with_levels_of_conjunction_vars(DFSCORES, \"trial_morph_assigned_to_which_base\", \n",
    "                                                             [\"morph_set_idx\", \"decoder_class_semantic_good\"], levels_get, n_min,  \n",
    "                                                             lenient_allow_data_if_has_n_levels=len(levels_get), \n",
    "                                                             prune_levels_with_low_n=True, \n",
    "                                                             plot_counts_heatmap_savepath=\"/tmp/counts.png\")\n",
    "\n",
    "from pythonlib.tools.pandastools import aggregGeneral\n",
    "dfscores_this_agg = aggregGeneral(dfscores_this, [\"morph_set_idx|idx_within\", \"pa_class\", \"decoder_class\"], [\"score\"], nonnumercols=\"all\")\n",
    "\n",
    "_analy_psychoprim_score_postsamp_plot_scores(dfscores_this_agg, savedir)\n",
    "\n",
    "# # Plot \n",
    "# var_score = \"score\"\n",
    "# x_var = \"decoder_class_semantic_good\"\n",
    "# x_order = (\"base1\", \"interm1\", \"same\", \"interm2\", \"base2\")\n",
    "# col = \"trial_morph_assigned_to_which_base\"\n",
    "# fig = sns.catplot(data=dfscores_this, x=x_var, y=var_score, hue = \"morph_set_idx\", kind=\"point\", errorbar=(\"ci\", 68), \n",
    "#                 col=col, order=x_order)\n",
    "# for ax in fig.axes.flatten():\n",
    "#     ax.axhline(0, color=\"k\", alpha=0.3)\n",
    "# savefig(fig, f\"{savedir}/decoder_class-x_var={x_var}-var_score={var_score}-col={col}-4.pdf\")\n",
    "\n",
    "# fig = sns.catplot(data=dfscores_this, x=x_var, y=var_score, hue = \"morph_set_idx|idx_within\", kind=\"point\", errorbar=(\"ci\", 68), \n",
    "#                 col=col, order=x_order)\n",
    "# for ax in fig.axes.flatten():\n",
    "#     ax.axhline(0, color=\"k\", alpha=0.3)\n",
    "# savefig(fig, f\"{savedir}/decoder_class-x_var={x_var}-var_score={var_score}-col={col}-4b.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = \"/tmp/test\"\n",
    "os.makedirs(sdir, exist_ok=True)\n",
    "Dc.scalar_score_df_plot_summary(dfscores_testsplit, sdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "dfthis = dfscores_testsplit\n",
    "var_score = \"score\"\n",
    "task_vars = [\"morph_set_idx\", \"pa_class\"]\n",
    "\n",
    "_, fig = plot_45scatter_means_flexible_grouping(dfthis, \"decoder_class_semantic_good\", \"base1\", \"base2\",\n",
    "                                    \"trial_morph_assigned_to_which_base\", var_score, task_vars,\n",
    "                                    plot_text=True, shareaxes=True, SIZE=6, alpha=0.3);\n",
    "\n",
    "_, fig = plot_45scatter_means_flexible_grouping(dfthis, \"trial_morph_assigned_to_which_base\", \"ambig_base1\", \"ambig_base2\",\n",
    "                                    \"decoder_class\", var_score, task_vars,\n",
    "                                    plot_text=True, shareaxes=True, SIZE=6, alpha=0.3);\n",
    "# savefig(fig, f\"{savedir}/scatter-x=decoder_class_semantic_good-y=trial_morph_assigned_to_which_base-dfthis_str={dfthis_str}-var_score={var_score}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1446e30",
   "metadata": {},
   "source": [
    "# [SWITCHING MORPHSETS, but also all others] MULTIPLE AREAS - combined analysis, loading all saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0294693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data saved \n",
    "# These data were saved using \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER, _REGIONS_IN_ORDER_COMBINED\n",
    "from pythonlib.tools.pandastools import grouping_plot_n_samples_conjunction_heatmap, stringify_values, aggregGeneral\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import prune_dfscores_good_morphset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548173ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot(dfscores, huevar, xvar, xvalues, huevalues, savedir):\n",
    "    from pythonlib.tools.pandastools import plot_subplots_heatmap\n",
    "    import seaborn as sns\n",
    "    from pythonlib.tools.snstools import rotateLabel\n",
    "    from pythonlib.tools.pandastools import savefig\n",
    "\n",
    "    for var_score, diverge in [(\"score\", False), (\"score_adjusted\", True)]:\n",
    "        fig, _ = plot_subplots_heatmap(dfscores, huevar, xvar, \n",
    "                            var_score, \"bregion\", share_zlim=True, diverge=diverge,\n",
    "                            row_values=huevalues, col_values=xvalues)\n",
    "        savefig(fig, f\"{savedir}/heatmap-varscore={var_score}.pdf\")\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Plot as point plot\n",
    "    for kind in [\"point\", \"bar\"]:\n",
    "        for var_score in [\"score\", \"score_adjusted\"]:\n",
    "\n",
    "            fig = sns.catplot(data=dfscores, x=xvar, y=var_score, hue=huevar, \n",
    "                        col=\"bregion\", col_wrap=6, kind=kind, errorbar=(\"ci\", 68),\n",
    "                        order = xvalues)\n",
    "            if var_score == \"score\":\n",
    "                yzero = 0.5\n",
    "            else:\n",
    "                yzero = 0.\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(yzero)            \n",
    "            rotateLabel(fig)\n",
    "            savefig(fig, f\"{savedir}/catplot-varscore={var_score}-kind={kind}.pdf\")\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    kind = \"point\"\n",
    "    for var_score in [\"score\", \"score_adjusted\"]:\n",
    "\n",
    "        fig = sns.catplot(data=dfscores, x=xvar, y=var_score, hue=\"morph_set_idx\", \n",
    "                            row=huevar, col=\"bregion\", kind=kind, errorbar=(\"ci\", 68), order = xvalues)\n",
    "        if var_score == \"score\":\n",
    "            yzero = 0.5\n",
    "        else:\n",
    "            yzero = 0.\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(yzero)            \n",
    "        rotateLabel(fig)\n",
    "        savefig(fig, f\"{savedir}/catplot-varscore={var_score}-kind={kind}-splitby-morph_set_idx.pdf\")\n",
    "    plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_BASE = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP\"\n",
    "\n",
    "combine = True\n",
    "downsample_trials = False\n",
    "version = 1\n",
    "\n",
    "WHICH_PLOTS = [1]\n",
    "for animal, date in [(\"Diego\", 240517), (\"Diego\", 240515), (\"Pancho\", 240521), (\"Pancho\", 240524),  (\"Diego\", 240523), (\"Diego\", 240521),  ]:\n",
    "    for TWIND_TEST in [(0.05, 1.2)]:\n",
    "\n",
    "        if combine:\n",
    "            list_bregion = _REGIONS_IN_ORDER_COMBINED\n",
    "        else:\n",
    "            list_bregion = _REGIONS_IN_ORDER\n",
    "\n",
    "        list_dfscores = []\n",
    "        for bregion in list_bregion:\n",
    "            path = f\"{SAVEDIR_BASE}/{animal}-{date}-logistic-combine={combine}/downsample_trials={downsample_trials}-TWIND_TEST={TWIND_TEST}-version={version}/{bregion}/DFSCORES.pkl\"\n",
    "            print(\"Loading ... \", path)\n",
    "            dfscores = pd.read_pickle(path)\n",
    "            dfscores[\"bregion\"] = bregion\n",
    "            list_dfscores.append(dfscores)\n",
    "        DFSCORES = pd.concat(list_dfscores).reset_index(drop=True)\n",
    "\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE}/MULT_BREGION/{animal}-{date}-logistic-combine={combine}/downsample_trials={downsample_trials}-TWIND_TEST={TWIND_TEST}-version={version}\"\n",
    "        print(\"SAving at: \", SAVEDIR)\n",
    "\n",
    "        ### PReprocessing\n",
    "        assert len(DFSCORES[\"twind\"].unique())==1, \"code below assumes\"\n",
    "\n",
    "        # Get score relative to chance (50%)\n",
    "        score_chance = 0.5\n",
    "        DFSCORES[\"score_adjusted\"] = (DFSCORES[\"score\"] - score_chance)/(1. - score_chance)\n",
    "\n",
    "        ######################## PLOTS\n",
    "        if 0 in WHICH_PLOTS:\n",
    "            ### (1) Biomdal, plots, categorization.\n",
    "            plot_bimodal_even_if_not_switch(DFSCORES, SAVEDIR)\n",
    "\n",
    "        if 1 in WHICH_PLOTS:\n",
    "            ### (2) Ambig trials, interanl state? (scalar plots)\n",
    "            for do_agg_over_trials in [False, True]:\n",
    "                for morphset_get in [None, \"good_ones\"]:\n",
    "\n",
    "                    dfscores = prune_dfscores_good_morphset(DFSCORES, morphset_get, animal, date)\n",
    "\n",
    "                    if do_agg_over_trials:\n",
    "                        dfscores = aggregGeneral(dfscores, \n",
    "                                                [\"bregion\", \"morph_set_idx|idx_within\", \"idx_within|assigned\", \"pa_class\", \"decoder_class\", \"twind\"], \n",
    "                                                [\"score\", \"score_adjusted\"], \n",
    "                                                nonnumercols=[\"trial_morph_assigned_to_which_base\"])\n",
    "                    # (1) \n",
    "                    huevar = \"decoder_class_semantic_good\"\n",
    "                    xvar = \"trial_morph_assigned_to_which_base\"\n",
    "                    xvalues = [\"base1\", \"not_ambig_base1\", \"ambig_base1\", \"ambig_base2\", \"not_ambig_base2\", \"base2\"]\n",
    "                    huevalues = [\"base1\", \"base2\"]\n",
    "\n",
    "                    savedir = f\"{SAVEDIR}/do_agg_over_trials={do_agg_over_trials}-morphset_get={morphset_get}/xvar={xvar}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    _plot(dfscores, huevar, xvar, xvalues, huevalues, savedir)\n",
    "\n",
    "                    # (2)\n",
    "                    huevar = \"recoded_decoder\"\n",
    "                    xvar = \"recoded_trial_morph\"\n",
    "                    xvalues = [\"ambig\", \"not_ambig\", \"base\"]\n",
    "                    huevalues = [\"baseother\", \"basethis\"]\n",
    "\n",
    "                    savedir = f\"{SAVEDIR}/do_agg_over_trials={do_agg_over_trials}-morphset_get={morphset_get}/xvar={xvar}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    _plot(dfscores, huevar, xvar, xvalues, huevalues, savedir)\n",
    "\n",
    "        if 2 in WHICH_PLOTS:\n",
    "            ### (1) \"Logistic vs. linear\" plots, categorization.\n",
    "            plot_logistic_vs_linear_curve(DFSCORES, SAVEDIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca8dcd",
   "metadata": {},
   "source": [
    "### Categorization, for non-ambiguous images (bimodal across trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94905da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bimodal_even_if_not_switch(DFSCORES, SAVEDIR):\n",
    "    \"\"\"\n",
    "    Question, qhether there is bimodal distribtuion of scores, even for inidices that are not switchign in beh\n",
    "    \"\"\"\n",
    "    from pythonlib.tools.snstools import rotateLabel\n",
    "    from pythonlib.tools.plottools import savefig\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "\n",
    "\n",
    "    def F(trial_morph_assigned_to_which_base):\n",
    "        a = \"base1\" in trial_morph_assigned_to_which_base\n",
    "        b = \"base2\" in trial_morph_assigned_to_which_base\n",
    "        if a and not b:\n",
    "            return \"base1\"\n",
    "        elif not a and b:\n",
    "            return \"base2\"\n",
    "        else:\n",
    "            print(trial_morph_assigned_to_which_base)\n",
    "            assert False\n",
    "\n",
    "    DFSCORES[\"trial_morph_assigned_to_which_base_clean\"] = [F(x) for x in DFSCORES[\"trial_morph_assigned_to_which_base\"]]\n",
    "\n",
    "\n",
    "    DFSCORES[\"score_logodds\"] = np.log(DFSCORES[\"score\"]/(1 - DFSCORES[\"score\"]))\n",
    "    assert len(DFSCORES[\"twind\"].unique())==1\n",
    "    list_bregion = DFSCORES[\"bregion\"].unique().tolist()\n",
    "    list_morphset = DFSCORES[\"morph_set_idx\"].unique().tolist()\n",
    "    for bregion in list_bregion:\n",
    "        for morphset in list_morphset:\n",
    "\n",
    "            # savedir = f\"{SAVEDIR}/logistic_categorical_plots/bregion={bregion}-morphset={morphset}\"\n",
    "            savedir = f\"{SAVEDIR}/plot_bimodal_even_if_not_switch/bregion={bregion}-morphset={morphset}\"\n",
    "            os.makedirs(savedir, exist_ok=True)\n",
    "            print(\"... Saving figures at: \", savedir)\n",
    "\n",
    "            dfscores = DFSCORES[(DFSCORES[\"morph_set_idx\"] == morphset) & (DFSCORES[\"bregion\"] == bregion)].reset_index(drop=True)\n",
    "\n",
    "            for var_score in [\"score\", \"score_logodds\"]:\n",
    "\n",
    "                if False:\n",
    "                    plot_45scatter_means_flexible_grouping(dfscores, \"decoder_class\", 0, 99, \"idx_within|assigned\", var_score, \n",
    "                                                        \"trialcode\", False, shareaxes=True, alpha=0.2)\n",
    "                    plot_45scatter_means_flexible_grouping(dfscores, \"decoder_class\", 0, 99, \"pa_class\", var_score, \n",
    "                                                        \"trialcode\", False, shareaxes=True, alpha=0.2)\n",
    "                    fig = sns.displot(data=dfscores, x=var_score, row=\"decoder_class\", col=\"pa_class\", bins=25)\n",
    "                    for ax in fig.axes.flatten():\n",
    "                        ax.axvline(0.5, color=\"k\", alpha=0.5)\n",
    "                    fig = sns.displot(data=dfscores, x=var_score, row=\"decoder_class\", col=\"pa_class\", kind=\"kde\")\n",
    "                    for ax in fig.axes.flatten():\n",
    "                        ax.axvline(0.5, color=\"k\", alpha=0.5)\n",
    "\n",
    "                ##### Quantifying bimodality.\n",
    "                # (1) Plot all, overlaid.\n",
    "                for xvar in [\"pa_class\", \"idx_within|assigned\"]:\n",
    "                    order = sorted(dfscores[xvar].unique())\n",
    "\n",
    "                    fig = sns.catplot(data=dfscores, x=xvar, y=var_score, col=\"decoder_class\", alpha=0.5, \n",
    "                                    jitter=True, hue=\"trial_morph_assigned_to_which_base_clean\", order=order)\n",
    "                    rotateLabel(fig)\n",
    "                    savefig(fig, f\"{savedir}/catplot-x={xvar}-varscore={var_score}-1.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=dfscores, x=xvar, y=var_score, col=\"decoder_class\", kind=\"violin\", order=order)\n",
    "                    rotateLabel(fig)\n",
    "                    savefig(fig, f\"{savedir}/catplot-x={xvar}-varscore={var_score}-2.pdf\")\n",
    "\n",
    "                    fig = sns.catplot(data=dfscores, x=xvar, y=var_score, col=\"decoder_class\", kind=\"point\", order=order)\n",
    "                    rotateLabel(fig)\n",
    "                    savefig(fig, f\"{savedir}/catplot-x={xvar}-varscore={var_score}-3.pdf\")\n",
    "\n",
    "                    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pythonlib.tools.statstools import curve_fit_and_test\n",
    "\n",
    "# def sigmoid(x, L, k, x0, b):\n",
    "#     return L * (1 / (1 + np.exp(-k*(x-x0)))) + b\n",
    "\n",
    "# bounds_sigmoid=([0., 0., -6., -5.], [7., 15., 6., 5])\n",
    "\n",
    "# def linear(x, a, b):\n",
    "#     return (a*x) + b\n",
    "\n",
    "# bounds_linear = ([-2, -5], [2, 5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142329dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = dfthis[var_score].values\n",
    "# indexes = dfthis[\"pa_class\"].values\n",
    "# indexes_rank = rank_items(indexes, method=\"dense\")\n",
    "\n",
    "def compute_score_diffs(dfscores, var_score):\n",
    "    # var_score = \"score_logodds\"\n",
    "    scores_0 = dfscores[dfscores[\"pa_class\"] == 0][var_score]\n",
    "    scores_1 = dfscores[dfscores[\"pa_class\"] == 1][var_score]\n",
    "    scores_2 = dfscores[dfscores[\"pa_class\"] == sorted(dfscores[\"pa_class\"].unique())[-2]][var_score]\n",
    "    scores_3 = dfscores[dfscores[\"pa_class\"] == 99][var_score]\n",
    "\n",
    "    scorediff_start = np.mean(scores_1) - np.mean(scores_0)\n",
    "    scorediff_end = np.mean(scores_3) - np.mean(scores_2)\n",
    "    scorediff_edge = np.mean([scorediff_start, scorediff_end])\n",
    "\n",
    "    scorediff_total = np.mean(scores_3) - np.mean(scores_0)\n",
    "\n",
    "    return scorediff_total, scorediff_edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d73189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_vs_linear_curve(DFSCORES, SAVEDIR):\n",
    "    \"\"\"\n",
    "    QUestion: whether curve is better fit by logistic vs. lieanr.\n",
    "\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from pythonlib.tools.listtools import rank_items\n",
    "    from pythonlib.tools.statstools import balanced_stratified_kfold\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from pythonlib.tools.plottools import savefig\n",
    "\n",
    "    # Fit logistic function\n",
    "    DFSCORES[\"score_logodds\"] = np.log(DFSCORES[\"score\"]/(1 - DFSCORES[\"score\"]))\n",
    "\n",
    "    assert len(DFSCORES[\"twind\"].unique())==1\n",
    "    list_bregion = DFSCORES[\"bregion\"].unique().tolist()\n",
    "    list_morphset = DFSCORES[\"morph_set_idx\"].unique().tolist()\n",
    "    nfolds = 10\n",
    "    n_min = 10\n",
    "\n",
    "    # list_bregion = [\"PMv\"]\n",
    "    # list_morphset = [0]\n",
    "    var_score = \"score_logodds\"\n",
    "    var_score_diffs = \"score\"\n",
    "    RES = []\n",
    "    RES_SCOREDIFFS = []\n",
    "    for bregion in list_bregion:\n",
    "        for morphset in list_morphset:\n",
    "\n",
    "            dfscores = DFSCORES[\n",
    "                (DFSCORES[\"morph_set_idx\"] == morphset) & \n",
    "                (DFSCORES[\"bregion\"] == bregion) & \n",
    "                (DFSCORES[\"decoder_class\"] == 99)\n",
    "                ].reset_index(drop=True)\n",
    "\n",
    "            from pythonlib.tools.pandastools import extract_with_levels_of_var_good\n",
    "            dfthis, inds_keep = extract_with_levels_of_var_good(dfscores, [\"pa_class\"], n_min)\n",
    "            print(\"Sample size pruning... \", len(dfscores), \" --> \", len(dfthis))\n",
    "\n",
    "            scores = dfthis[var_score].values\n",
    "            indexes = dfthis[\"pa_class\"].values\n",
    "            indexes_rank = rank_items(indexes, method=\"dense\")\n",
    "\n",
    "            ############### SCORE DIFFS (EDGE VS. TOTAL)\n",
    "            scorediff_total, scorediff_end = compute_score_diffs(dfscores, var_score_diffs)\n",
    "\n",
    "            for sd, sdv in [\n",
    "                (scorediff_total, \"total\"),\n",
    "                (scorediff_end, \"end\")\n",
    "                ]:\n",
    "                RES_SCOREDIFFS.append({\n",
    "                    \"bregion\":bregion,\n",
    "                    \"morphset\":morphset,\n",
    "                    \"scorediff\":sd,\n",
    "                    \"scorediff_ver\":sdv,\n",
    "                    \"var_score\":var_score_diffs,\n",
    "                })\n",
    "                \n",
    "            ############### REGRESSION (LOGISTIC VS LINEAR)\n",
    "            if False:\n",
    "                smin = scores.min()\n",
    "                smax = scores.max()\n",
    "                scores_norm_01 = (scores - smin)/(smax-smin)\n",
    "\n",
    "        \n",
    "            # Split into train-test splits\n",
    "            X = indexes_rank.astype(float) - np.mean(indexes_rank.astype(float))\n",
    "            Y = scores.astype(float) - np.mean(scores.astype(float))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            assert False, \"replace this part with curve_fit_and_test_wrapper_compare_logistic_linear\"\n",
    "            folds = balanced_stratified_kfold(None, indexes_rank, nfolds)\n",
    "            plot_sanity = False\n",
    "\n",
    "            res = []\n",
    "            for inds_train, inds_test in folds:\n",
    "                x_train = X[inds_train]\n",
    "                y_train = Y[inds_train]\n",
    "                x_test = X[inds_test]\n",
    "                y_test = Y[inds_test]\n",
    "\n",
    "                assert x_test.shape == y_test.shape\n",
    "                if plot_sanity:\n",
    "                    fig, axes = plt.subplots(2,2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "                    ax = axes.flatten()[0]\n",
    "                    ax.set_title(\"all data\")\n",
    "                    ax.plot(x, y, \"ob\", alpha=0.5)\n",
    "                    # ax.plot(x, sigmoid(x,  2, 1, 0, -0.9), 'go')\n",
    "\n",
    "                    ax = axes.flatten()[1]\n",
    "                    ax.set_title(\"train data\")\n",
    "                    ax.plot(x_train, y_train, \"ob\", alpha=0.5)\n",
    "                    \n",
    "                    ax = axes.flatten()[2]\n",
    "                    ax.set_title(\"test data\")\n",
    "                    ax.plot(x_test, y_test, \"ob\", alpha=0.5)\n",
    "                    \n",
    "                    assert False\n",
    "\n",
    "\n",
    "                y_pred, R2, residuals_pred, residuals_mean = curve_fit_and_test(sigmoid, bounds_sigmoid, x_train, y_train, x_test, y_test, doplot=False)\n",
    "                res.append({\n",
    "                    \"model\":\"sigmoid\",\n",
    "                    \"y_pred\":y_pred,\n",
    "                    \"y_test\":y_test,\n",
    "                    \"R2\":R2,\n",
    "                    \"residuals_pred\":residuals_pred,\n",
    "                    \"residuals_mean\":residuals_mean,\n",
    "                })\n",
    "                \n",
    "                y_pred, R2, residuals_pred, residuals_mean = curve_fit_and_test(linear, bounds_linear, x_train, y_train, x_test, y_test, doplot=False)\n",
    "                res.append({\n",
    "                    \"model\":\"linear\",\n",
    "                    \"y_pred\":y_pred,\n",
    "                    \"y_test\":y_test,\n",
    "                    \"R2\":R2,\n",
    "                    \"residuals_pred\":residuals_pred,\n",
    "                    \"residuals_mean\":residuals_mean,\n",
    "                })\n",
    "\n",
    "            # Collect single result for each model\n",
    "            dfres = pd.DataFrame(res)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for model in [\"linear\", \"sigmoid\"]:\n",
    "                # Collect all predicted values, and compute R2\n",
    "                \n",
    "                dfresthis = dfres[dfres[\"model\"] == model]\n",
    "                \n",
    "                Y_PRED = np.concatenate(dfresthis[\"y_pred\"].tolist())\n",
    "                Y_TEST = np.concatenate(dfresthis[\"y_test\"].tolist())\n",
    "                assert Y_PRED.shape == Y_TEST.shape == Y.shape\n",
    "                from pythonlib.tools.statstools import coeff_determination_R2\n",
    "                R2, residuals_pred, residuals_mean = coeff_determination_R2(Y_TEST, Y_PRED, False)\n",
    "\n",
    "                R2_mean = np.mean(dfresthis[\"R2\"])\n",
    "\n",
    "                print(bregion, \" -- \", morphset, \" -- \", model, \" -- \", R2)\n",
    "\n",
    "                # Collect output\n",
    "                RES.append({\n",
    "                    \"model\":model,\n",
    "                    \"bregion\":bregion,\n",
    "                    \"morphset\":morphset,\n",
    "                    \"R2\":R2,\n",
    "                    \"R2_mean\":R2_mean\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "    DFRES = pd.DataFrame(RES)\n",
    "    DFRES_SCOREDIFFS = pd.DataFrame(RES_SCOREDIFFS)\n",
    "\n",
    "    ###### PLOTS\n",
    "    savedir = f\"{SAVEDIR}/logistic_vs_linear\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    print(\"... Saving figures at: \", savedir)\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "    # import seaborn as sns\n",
    "    # sns.catplot(data=DFRES, x=\"model\", y=\"R2\", col=\"bregion\", kind=\"point\")\n",
    "    # sns.catplot(data=DFRES, x=\"model\", y=\"R2_mean\", col=\"bregion\", kind=\"point\")\n",
    "    # sns.catplot(data=DFRES, x=\"model\", y=\"R2\", col=\"bregion\", kind=\"point\")\n",
    "    for var_score in [\"R2\", \"R2_mean\"]:\n",
    "        _, fig = plot_45scatter_means_flexible_grouping(DFRES, \"model\", \"linear\", \"sigmoid\", \"bregion\", var_score, \"morphset\", shareaxes=True, SIZE=3, fontsize=5);\n",
    "        savefig(fig, f\"{savedir}/logistic_vs_linear-scatter-varscore={var_score}.pdf\")\n",
    "\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(DFRES_SCOREDIFFS, \"scorediff_ver\", \"total\", \"end\", \"bregion\", \"scorediff\", \"morphset\", shareaxes=True, SIZE=3, fontsize=5);\n",
    "    savefig(fig, f\"{savedir}/scorediffs-scatter-1.pdf\")\n",
    "\n",
    "    _, fig = plot_45scatter_means_flexible_grouping(DFRES_SCOREDIFFS, \"scorediff_ver\", \"end\", \"total\", \"bregion\", \"scorediff\", \"morphset\", shareaxes=True, SIZE=3, fontsize=5);\n",
    "    savefig(fig, f\"{savedir}/scorediffs-scatter-2.pdf\")\n",
    "\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc980d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logistic_vs_linear_curve(DFSCORES, \"/tmp/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b837d7",
   "metadata": {},
   "source": [
    "# STATE SPACE VISUALIZATION PLOTS [METHOD 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634737f3",
   "metadata": {},
   "source": [
    "### [Method 1], State space plots, pairwise diffs, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e152ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA = extract_single_pa(DFallpa, \"PMv\", None, \"trial\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_or_traj = \"scal\"\n",
    "version = \"dpca\"\n",
    "\n",
    "savedir = \"/tmp\"\n",
    "twind_pca = (0.1, 0.5)\n",
    "dpca_var = \"seqc_0_shape\"\n",
    "tbin_dur = 0.2\n",
    "tbin_slide = 0.1\n",
    "PAredu = PA.dataextract_dimred_wrapper(scalar_or_traj, version, savedir, twind_pca, tbin_dur, tbin_slide,\n",
    "                                       dpca_var=dpca_var)\n",
    "print(PAredu.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_psychoprim_statespace_euclidian, analy_extract_PA_conditioned\n",
    "\n",
    "SAVEDIR = \"/tmp/MORPHS\"\n",
    "list_morphset = DSmorphsets.Dat[\"morph_set_idx\"].unique().tolist()\n",
    "analy_psychoprim_statespace_euclidian(DFallpa, SAVEDIR, map_tcmorphset_to_idxmorph, list_morphset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac87dc",
   "metadata": {},
   "source": [
    "### [MULT DATES] STATE SPACE VISUALIZATION PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a38660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.dataset.dataset_analy.psychometric_singleprims import params_good_morphsets_no_switching\n",
    "import numpy as np\n",
    "from pythonlib.tools.vectools import projection_onto_axis_subspace\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.tools.pandastools import plot_subplots_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER_COMBINED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1427175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric_mult import _plot, convert_dist_to_distdiff, compute_pa_to_df_dist_index_using_splits, _rank_idxs_append, _compute_df_using_dist_index, dfdist_to_dfproj_index, _recenter_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8966f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### [OBSOLETE!!] see # analy-decode)moiment_psychometric_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.state_space_good import euclidian_distance_compute_trajectories_single\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "\n",
    "DEBUG = False\n",
    "SAVEDIR_BASE_LOAD = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP\";\n",
    "\n",
    "if DEBUG:\n",
    "    SAVEDIR_BASE_SAVE = \"/tmp\"\n",
    "    # LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517)]\n",
    "    LIST_EXPTS = [(\"Diego\", 240515)]\n",
    "else:\n",
    "    SAVEDIR_BASE_SAVE = SAVEDIR_BASE_LOAD\n",
    "    LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517), (\"Diego\", 240521), \n",
    "                            (\"Diego\", 240523), (\"Diego\", 240731), (\"Diego\", 240801), (\"Diego\", 240802), \n",
    "                            (\"Pancho\", 240516), (\"Pancho\", 240524), (\"Pancho\", 240521), \n",
    "                            (\"Pancho\", 240801), (\"Pancho\", 240802)]\n",
    "\n",
    "combine = True\n",
    "version = \"dpca\"\n",
    "subtrmean = False\n",
    "projtwind = (0.1, 1.2)\n",
    "\n",
    "# scalar_or_traj = \"scal\"\n",
    "# npcs = 8\n",
    "\n",
    "scalar_or_traj = \"traj\"\n",
    "npcs = 6\n",
    "\n",
    "# list_bregion = _REGIONS_IN_ORDER_COMBINED\n",
    "list_bregion = [\"PMv\", \"PMd\"]\n",
    "\n",
    "DOPLOTS = False\n",
    "# exclude_flankers = True\n",
    "\n",
    "# Params - computing projection score\n",
    "for exclude_flankers in [True, False]:        \n",
    "    # for ndims_proj in [2, 3, 4]:\n",
    "    for ndims_proj in [10]:\n",
    "        # ndims_proj = 3\n",
    "\n",
    "        ### \n",
    "        list_dfproj = []\n",
    "        list_dfdiffs = []\n",
    "        list_dfproj_index = []\n",
    "        list_dfdiffsproj = []\n",
    "        list_dfdiffsindex = []\n",
    "\n",
    "        for animal, date in LIST_EXPTS:\n",
    "            list_morphset = params_good_morphsets_no_switching(animal, date)\n",
    "\n",
    "            for morphset in list_morphset:\n",
    "                # morphset = 7\n",
    "\n",
    "                for bregion in list_bregion:\n",
    "                    # bregion = \"PMd\"\n",
    "\n",
    "                    SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/statespace_euclidian/bregion={bregion}/morphset={morphset}-ver={scalar_or_traj}-{version}-subtrmean={subtrmean}-projtwind={projtwind}-npcs={npcs}\"\n",
    "                \n",
    "\n",
    "                    # Load data\n",
    "                    path = f\"{SAVEDIR}/Cldist.pkl\"\n",
    "                    print(path)\n",
    "                    with open(path, \"rb\") as f:\n",
    "                        Cldist = pickle.load(f)\n",
    "\n",
    "                    path = f\"{SAVEDIR}/PAredu.pkl\"\n",
    "                    print(path)\n",
    "                    with open(path, \"rb\") as f:\n",
    "                        PAredu = pickle.load(f)\n",
    "\n",
    "                    # Dir for plotting\n",
    "                    savedir = f\"{SAVEDIR_BASE_SAVE}/MULT_BREGION/statespace_euclidian/{animal}-{date}-logistic-combine={combine}/statespace_euclidian/bregion={bregion}/ver={scalar_or_traj}-{version}-subtrmean={subtrmean}-projtwind={projtwind}-npcs={npcs}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "                    assert False\n",
    "                    \n",
    "                    if ndims_proj is not None and (ndims_proj <= PAredu.X.shape[0]):\n",
    "                        # X = X[:, :ndims_proj] # (trials, ndims)\n",
    "                        PAredu = PAredu.slice_by_dim_indices_wrapper(\"chans\", list(range(ndims_proj)))\n",
    "\n",
    "                    if exclude_flankers:\n",
    "                        _idxs_no_flankers = [x for x in PAredu.Xlabels[\"trials\"][\"idx_morph_temp\"].unique().tolist() if x>=0 and x<=99]\n",
    "                        PAredu = PAredu.slice_by_labels_filtdict({\"idx_morph_temp\":_idxs_no_flankers})\n",
    "\n",
    "                    # Dir for plotting\n",
    "                    savedir = f\"{SAVEDIR_BASE_SAVE}/MULT_BREGION/statespace_euclidian/{animal}-{date}-logistic-combine={combine}/statespace_euclidian/bregion={bregion}/ver={scalar_or_traj}-{version}-subtrmean={subtrmean}-projtwind={projtwind}-npcs={npcs}\"\n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "                    ####################### Project trials onto the axis (base1 - base2)\n",
    "                    X = PAredu.X.squeeze(axis=2).T # (trials, ndims)\n",
    "                    dflab = PAredu.Xlabels[\"trials\"]\n",
    "\n",
    "                    # Do this in cross-validated way -- split base prims into two sets, run this 2x, then \n",
    "                    # average over base prims.\n",
    "                    from pythonlib.tools.statstools import crossval_folds_indices\n",
    "\n",
    "                    inds_pool_base1 = dflab[dflab[\"idx_morph_temp\"] == 0].index.values\n",
    "                    inds_pool_base2 = dflab[dflab[\"idx_morph_temp\"] == 99].index.values\n",
    "                    list_inds_1, list_inds_2 = crossval_folds_indices(len(inds_pool_base1), len(inds_pool_base2), 2)\n",
    "\n",
    "                    # shuffle inds\n",
    "                    np.random.shuffle(inds_pool_base1)\n",
    "                    np.random.shuffle(inds_pool_base2)\n",
    "\n",
    "                    list_inds_1 = [inds_pool_base1[_inds] for _inds in list_inds_1]\n",
    "                    list_inds_2 = [inds_pool_base2[_inds] for _inds in list_inds_2]\n",
    "\n",
    "                    list_X =[]\n",
    "                    for _i, (inds_base_1, inds_base_2) in enumerate(zip(list_inds_1, list_inds_2)):\n",
    "                        # inds_base_1 = inds_pool_base1[_inds1]\n",
    "                        # inds_base_2 = inds_pool_base2[_inds2]\n",
    "                        # print(inds_base_1)\n",
    "\n",
    "                        # - get mean activity for base1, base2\n",
    "                        xmean_base1 = np.mean(X[inds_base_1,:], axis=0) # (ndims,)\n",
    "                        xmean_base2 = np.mean(X[inds_base_2,:], axis=0) # (ndims,)\n",
    "\n",
    "                        # get mean projected data for each state\n",
    "                        # xproj, fig = projection_onto_axis_subspace(xmean_base1, xmean_base2, X, doplot=True)\n",
    "                        plot_color_labels = dflab[\"idx_morph_temp\"].values\n",
    "                        xproj, fig = projection_onto_axis_subspace(xmean_base1, xmean_base2, X, doplot=True, \n",
    "                                                                plot_color_labels=plot_color_labels)\n",
    "                        savefig(fig, f\"{savedir}/morphset={morphset}-projections_preprocess-iter={_i}.pdf\")\n",
    "                        # replace the train data with nan\n",
    "                        xproj[inds_base_1] = np.nan\n",
    "                        xproj[inds_base_2] = np.nan\n",
    "\n",
    "                        list_X.append(xproj)\n",
    "\n",
    "                    # Get average over iterations\n",
    "                    Xproj = np.nanmean(np.stack(list_X), axis=0)\n",
    "\n",
    "                    dfproj = PAredu.Xlabels[\"trials\"].copy()\n",
    "                    dfproj[\"x_proj\"] = Xproj\n",
    "\n",
    "                    ####################### Get eucl distnaces between adjacent indices\n",
    "                    # Get distances between adjacent indices\n",
    "                    if True:\n",
    "                        # Use dist_yue instead\n",
    "                        cldist, _ = euclidian_distance_compute_trajectories_single(PAredu, \"idx_morph_temp\", [\"epoch\"], \n",
    "                                                                                version_distance=\"euclidian\", return_cldist=True, get_reverse_also=False)\n",
    "                        # get distance between 0 and 99\n",
    "                        # res, DIST_NULL_50, DIST_NULL_95, DIST_NULL_98 = cldist.rsa_distmat_score_same_diff_by_context(\"idx_morph_temp\", [\"epoch\"], None, \"pts\", )\n",
    "                        # pd.DataFrame(res)\n",
    "                        dfdist = cldist.rsa_distmat_score_all_pairs_of_label_groups(get_only_one_direction=False)\n",
    "                        VAR_SCORE = \"dist_yue_diff\"\n",
    "                    else:\n",
    "                        # use euclidian dist norm -- problem --> this is noisy.\n",
    "                        assert False, \"dist index below assumes is doing euclidian, not euclidian_unbiased\"\n",
    "                        cldist, _ = euclidian_distance_compute_trajectories_single(PAredu, \"idx_morph_temp\", [\"epoch\"], \n",
    "                                                                                version_distance=\"euclidian_unbiased\", return_cldist=True, get_reverse_also=False)\n",
    "                        # get distance between 0 and 99\n",
    "                        dfdist = cldist.rsa_distmat_score_all_pairs_of_label_groups()\n",
    "                        VAR_SCORE = \"dist_mean\"\n",
    "\n",
    "                    # Collect all adjacent pairs\n",
    "                    dflab = PAredu.Xlabels[\"trials\"]\n",
    "                    list_idx_morph = sorted(dflab[(dflab[\"idx_morph_temp\"]>=0) & (dflab[\"idx_morph_temp\"]<=99)][\"idx_morph_temp\"].unique().tolist())\n",
    "                    res_diffs = []\n",
    "                    for i in range(len(list_idx_morph)-1):\n",
    "                        tmp = dfdist[(dfdist[\"idx_morph_temp_1\"] == list_idx_morph[i]) & (dfdist[\"idx_morph_temp_2\"] == list_idx_morph[i+1])]\n",
    "                        assert len(tmp)==1, f\"{i}\"\n",
    "\n",
    "                        # Get the distance\n",
    "                        d = tmp[VAR_SCORE].values[0]\n",
    "\n",
    "                        # save\n",
    "                        res_diffs.append({\n",
    "                            \"idx_along_morph\":i,\n",
    "                            \"idx1\":list_idx_morph[i],\n",
    "                            \"idx2\":list_idx_morph[i+1],\n",
    "                            \"dist\":d,\n",
    "                            \"animal\":animal,\n",
    "                            \"date\":date,\n",
    "                            \"morphset\":morphset,\n",
    "                            \"bregion\":bregion\n",
    "                        })\n",
    "                    dfdiffs = pd.DataFrame(res_diffs)            \n",
    "\n",
    "\n",
    "                    ####################### EXTRA STUFF\n",
    "                    # Also figure out whether base1 and base2 are sufficiently separated to include this data\n",
    "                    # - Collect distances between adjacent morph indices.\n",
    "                    # cldist, _ = euclidian_distance_compute_trajectories_single(PAredu, \"idx_morph_temp\", \n",
    "                    #                                                                 [\"epoch\"], return_cldist=True, get_reverse_also=False)\n",
    "                    # get distance between 0 and 99\n",
    "                    # dfdist = cldist.rsa_distmat_score_all_pairs_of_label_groups()\n",
    "                    tmp = dfdist[(dfdist[\"idx_morph_temp_1\"] == 0) & (dfdist[\"idx_morph_temp_2\"] == 99)]\n",
    "                    assert len(tmp)==1\n",
    "                    dist_between_bases = tmp[VAR_SCORE].values[0]\n",
    "                    DIST_98 = tmp[\"DIST_98\"].values[0]\n",
    "                    # dfproj[\"dist_between_bases\"] = dist_between_bases\n",
    "                    # dfproj[\"dist_norm_between_bases\"] = dist_between_bases/DIST_98\n",
    "\n",
    "                    # dfdiffs[\"dist_between_bases\"] = dist_between_bases\n",
    "                    # dfdiffs[\"dist_norm_between_bases\"] = dist_between_bases/DIST_98\n",
    "\n",
    "                    ### Base prim separation score\n",
    "                    # - ie., frac (between 0, 1), if 1 then base prim separation is greater than 100% of the other prims.\n",
    "                    _dfdist_notbase = dfdist[(~dfdist[\"idx_morph_temp_1\"].isin([0, 99])) & (~dfdist[\"idx_morph_temp_2\"].isin([0, 99]))]\n",
    "                    n = sum(_dfdist_notbase[VAR_SCORE]<dist_between_bases)\n",
    "                    ntot = len(_dfdist_notbase)\n",
    "                    base_prim_separation_score = n/ntot\n",
    "                    # dfproj[\"base_prim_separation_score\"] = base_prim_separation_score\n",
    "                    # dfdiffs[\"base_prim_separation_score\"] = base_prim_separation_score\n",
    "\n",
    "                    if DOPLOTS:\n",
    "                        fig = sns.catplot(data=dfproj, x=\"idx_morph_temp\", y=\"x_proj\", jitter=True, alpha=0.5)\n",
    "                        for ax in fig.axes.flatten():\n",
    "                            ax.axhline(0, color=\"k\", alpha=0.5)\n",
    "                            ax.axhline(1, color=\"k\", alpha=0.5)\n",
    "                        savefig(fig, f\"{savedir}/morphset={morphset}-catplot-1.pdf\")\n",
    "\n",
    "                        fig = sns.catplot(data=dfproj, x=\"idx_morph_temp\", y=\"x_proj\", kind=\"point\")\n",
    "                        for ax in fig.axes.flatten():\n",
    "                            ax.axhline(0, color=\"k\", alpha=0.5)\n",
    "                            ax.axhline(1, color=\"k\", alpha=0.5)\n",
    "                        savefig(fig, f\"{savedir}/morphset={morphset}-catplot-2.pdf\")\n",
    "\n",
    "                        fig = sns.catplot(data=dfproj, x=\"idx_morph_temp\", y=\"x_proj\", kind=\"violin\")\n",
    "                        for ax in fig.axes.flatten():\n",
    "                            ax.axhline(0, color=\"k\", alpha=0.5)\n",
    "                            ax.axhline(1, color=\"k\", alpha=0.5)\n",
    "                        savefig(fig, f\"{savedir}/morphset={morphset}-catplot-3.pdf\")\n",
    "\n",
    "                    plt.close(\"all\")\n",
    "\n",
    "                    ### COLLECT ALL\n",
    "                    # Rescale so that 0 <--> 99 maps onto 0 <--> 1 index, so can compare across expts\n",
    "                    idx_max = dfproj[dfproj[\"idx_morph_temp\"] < 99][\"idx_morph_temp\"].max()\n",
    "\n",
    "                    def f(x):\n",
    "                        if x<=idx_max:\n",
    "                            return x\n",
    "                        elif x>=99:\n",
    "                            return x - 99 + idx_max + 1\n",
    "                        else:\n",
    "                            assert False\n",
    "                    \n",
    "                    _rank_idxs_append(dfproj)\n",
    "                    dfproj[\"idx_morph_rescaled\"] = [f(x) for x in dfproj[\"idx_morph_temp\"]]\n",
    "                    from pythonlib.tools.listtools import rank_items\n",
    "                    # assert np.all(dfproj[\"idx_morph_rescaled\"].tolist() == list(rank_items(dfproj[\"idx_morph_temp\"]-1, method=\"dense\"))), \"sanity cehck.\"\n",
    "                    dfproj[\"idx_morph_rescaled\"] = dfproj[\"idx_morph_rescaled\"]/(idx_max+1) # rescale\n",
    "                    # dfproj.groupby([\"idx_morph_temp\", \"idx_morph_rescaled\"]).size()\n",
    "\n",
    "                    dfproj[\"animal\"] = animal\n",
    "                    dfproj[\"date\"] = date\n",
    "                    dfproj[\"morphset\"] = morphset\n",
    "                    dfproj[\"bregion\"] = bregion\n",
    "                    \n",
    "                    # Track how many morphs exist\n",
    "                    nmorphs = len(dfproj[(dfproj[\"idx_morph_temp\"]>=0) & (dfproj[\"idx_morph_temp\"]<=99)][\"idx_morph_temp\"].unique())\n",
    "                    dfproj[\"nmorphs\"] = nmorphs\n",
    "                    dfdiffs[\"nmorphs\"] = nmorphs\n",
    "                        \n",
    "                    ########################## GET projection distance between adjacent indices\n",
    "\n",
    "                    # Optionally remove flankers\n",
    "                    var_score = \"x_proj\"\n",
    "                    dfthis = dfproj\n",
    "                    # Get differenes between adjancent morph indices, projected onto the axis.\n",
    "                    dfdiffsproj = convert_dist_to_distdiff(dfthis, var_score)\n",
    "                    # dfdiffsproj[\"animal\"] = animal\n",
    "                    # dfdiffsproj[\"date\"] = date\n",
    "                    # dfdiffsproj[\"morphset\"] = morphset\n",
    "                    # dfdiffsproj[\"bregion\"] = bregion\n",
    "                    # dfdiffsproj[\"base_prim_separation_score\"] = base_prim_separation_score\n",
    "                    # dfdiffsproj[\"dist_between_bases\"] = dist_between_bases\n",
    "                    # dfdiffsproj[\"dist_norm_between_bases\"] = dist_between_bases/DIST_98\n",
    "                    # dfdiffsproj[\"nmorphs\"] = nmorphs\n",
    "\n",
    "                    # list_idx_morph = sorted(dfthis[\"idx_morph_temp\"].unique().tolist())\n",
    "                    # for i in range(len(list_idx_morph)-1):\n",
    "                    #     idx1 = list_idx_morph[i]\n",
    "                    #     idx2 = list_idx_morph[i+1]\n",
    "                        \n",
    "                    #     score1 = dfthis[dfthis[\"idx_morph_temp\"] == idx1][var_score]\n",
    "                    #     score2 = dfthis[dfthis[\"idx_morph_temp\"] == idx2][var_score]\n",
    "\n",
    "                    #     res_diffs_proj.append({\n",
    "                    #         # f\"{var_score}-idx2-min-idx1\":np.mean(score2) - np.mean(score1),\n",
    "                    #         \"dist\":np.mean(score2) - np.mean(score1),\n",
    "                    #         \"idx_along_morph\":i,\n",
    "                    #         \"idx1\":idx1,\n",
    "                    #         \"idx2\":idx2,\n",
    "                    #         \"animal\":animal, \n",
    "                    #         \"date\":date, \n",
    "                    #         \"morphset\":morphset, \n",
    "                    #         \"bregion\":bregion,\n",
    "                    #         \"base_prim_separation_score\":base_prim_separation_score,\n",
    "                    #         \"dist_between_bases\":dist_between_bases,\n",
    "                    #         \"dist_norm_between_bases\":,\n",
    "                    #         \"nmorphs\":nmorphs\n",
    "                    #     })\n",
    "\n",
    "                    ### [Best method?] dist index\n",
    "                    if True:\n",
    "                        # Does splits, returning 2x num rows, splitting lets you recenter indinces without overfitting.\n",
    "                        dfdiffsindex, dfproj_index = compute_pa_to_df_dist_index_using_splits(PAredu)\n",
    "                    else:\n",
    "\n",
    "                        dfproj_index = dfdist_to_dfproj_index(dfdist)\n",
    "                        # # compute an index which is how close you are to base 1 vs. to base 2.\n",
    "                        # # Ranging in (0,1), where 0 means is close to base 1 and far from base 2\n",
    "                        # list_idx_morph = sorted(dfdist[\"idx_morph_temp_1\"].unique().tolist())\n",
    "                        # res_dist_index = []\n",
    "                        # for idx in list_idx_morph:\n",
    "                            \n",
    "                        #     tmp = dfdist[(dfdist['idx_morph_temp_1']==0) & (dfdist['idx_morph_temp_2']==idx)]\n",
    "                        #     assert len(tmp)==1\n",
    "                        #     d1 = tmp[\"dist_mean\"].values[0]\n",
    "                            \n",
    "                        #     tmp = dfdist[(dfdist['idx_morph_temp_1']==idx) & (dfdist['idx_morph_temp_2']==99)]\n",
    "                        #     assert len(tmp)==1\n",
    "                        #     d2 = tmp[\"dist_mean\"].values[0]\n",
    "\n",
    "                        #     dist_index = d1/(d1+d2)\n",
    "\n",
    "                        #     # print(idx, d1, d2, \" -- \", dist_index)\n",
    "                        #     res_dist_index.append({\n",
    "                        #         \"dist_index\":dist_index,\n",
    "                        #         \"idx_morph_temp\":idx,\n",
    "                        #         # \"animal\":animal, \n",
    "                        #         # \"date\":date, \n",
    "                        #         # \"morphset\":morphset, \n",
    "                        #         # \"bregion\":bregion,\n",
    "                        #         # \"base_prim_separation_score\":base_prim_separation_score,\n",
    "                        #         # \"dist_between_bases\":dist_between_bases,\n",
    "                        #         # \"dist_norm_between_bases\":dist_between_bases/DIST_98,\n",
    "                        #         # \"nmorphs\":nmorphs\n",
    "                        #     })\n",
    "                        # dfproj_index = pd.DataFrame(res_dist_index)\n",
    "\n",
    "                        # Get diffs\n",
    "                        dfdiffsindex = convert_dist_to_distdiff(dfproj_index, \"dist_index\")\n",
    "\n",
    "                    # Append useful stuff\n",
    "                    for _df in [dfproj, dfproj_index, dfdiffs, dfdiffsproj, dfdiffsindex]:\n",
    "                        _df[\"animal\"] = animal\n",
    "                        _df[\"date\"] = date\n",
    "                        _df[\"morphset\"] = morphset\n",
    "                        _df[\"bregion\"] = bregion\n",
    "                        _df[\"base_prim_separation_score\"] = base_prim_separation_score\n",
    "                        _df[\"dist_between_bases\"] = dist_between_bases\n",
    "                        _df[\"dist_norm_between_bases\"] = dist_between_bases/DIST_98\n",
    "                        _df[\"nmorphs\"] = nmorphs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    ############ COLLECT ALL\n",
    "                    list_dfproj.append(dfproj)\n",
    "                    list_dfproj_index.append(dfproj_index)\n",
    "\n",
    "                    list_dfdiffs.append(dfdiffs)\n",
    "                    list_dfdiffsproj.append(dfdiffsproj)\n",
    "                    list_dfdiffsindex.append(dfdiffsindex)\n",
    "\n",
    "\n",
    "        ############ PLOTS\n",
    "        savedir = f\"{SAVEDIR_BASE_SAVE}/MULT_BREGION/statespace_euclidian/COMBINED/USING_DIFFS_ADJACENT_INDS-ndims_proj={ndims_proj}-excludeflank={exclude_flankers}\"\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "        from pythonlib.tools.snstools import rotateLabel\n",
    "\n",
    "        DFPROJ = pd.concat(list_dfproj).reset_index(drop=True)\n",
    "        DFPROJ_INDEX = pd.concat(list_dfproj_index).reset_index(drop=True)\n",
    "\n",
    "        DFDIFFS = pd.concat(list_dfdiffs).reset_index(drop=True)\n",
    "        DFDIFFSPROJ = pd.concat(list_dfdiffsproj).reset_index(drop=True)\n",
    "        DFDIFFSINDEX = pd.concat(list_dfdiffsindex).reset_index(drop=True)\n",
    "        \n",
    "        DFDIFFSPROJ = append_col_with_grp_index(DFDIFFSPROJ, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "        DFDIFFS = append_col_with_grp_index(DFDIFFS, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "        DFPROJ = append_col_with_grp_index(DFPROJ, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "        DFPROJ_INDEX = append_col_with_grp_index(DFPROJ_INDEX, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "        DFDIFFSINDEX = append_col_with_grp_index(DFDIFFSINDEX, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "        \n",
    "        DFDIFFSPROJ[\"dist_norm\"] = DFDIFFSPROJ[\"dist\"] * DFDIFFSPROJ[\"dist_between_bases\"]\n",
    "\n",
    "\n",
    "        ### Label if morphset has intermediate shapes (e..g, line1-line2-line3)\n",
    "        from pythonlib.dataset.dataset_analy.psychometric_singleprims import params_has_intermediate_shape\n",
    "        cachedict = {}\n",
    "        for dfthis in [DFDIFFSPROJ, DFDIFFS, DFPROJ, DFPROJ_INDEX, DFDIFFSINDEX]:\n",
    "            list_has_intermediate = []\n",
    "            for i, row in dfthis.iterrows():\n",
    "                animal = row[\"animal\"]\n",
    "                date = row[\"date\"]\n",
    "\n",
    "                if (animal, date) not in cachedict:\n",
    "                    cachedict[(animal, date)] = params_has_intermediate_shape(animal, date)\n",
    "                \n",
    "                # Check if this has intermediate\n",
    "                list_has_intermediate.append(row[\"morphset\"] in cachedict[(animal, date)])\n",
    "\n",
    "            dfthis[\"morphset_has_intermediate\"] = list_has_intermediate\n",
    "\n",
    "        if DEBUG:\n",
    "            assert False\n",
    "\n",
    "        # all projection distance, rescale by the distance between base1 and base2, so that the aggregated data is not super noisy.\n",
    "        # - i.e., \n",
    "        # DFPROJ[\"x_proj_norm\"] = [row[\"x_proj\"] * row[\"dist_norm_between_bases\"] for i, row in DFPROJ.iterrows()]\n",
    "        # DFPROJ[\"x_proj_norm\"] = [row[\"x_proj\"] * row[\"dist_between_bases\"] for i, row in DFPROJ.iterrows()]\n",
    "        # Plot results, using differences between adjacent indices\n",
    "\n",
    "        # SCore for how well base prims are separated\n",
    "        fig = sns.catplot(data=DFPROJ, x=\"ani_date_mrp\", y=\"base_prim_separation_score\", hue=\"bregion\", kind=\"point\", aspect=2)\n",
    "        rotateLabel(fig)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "        savefig(fig, f\"{savedir}/base_prim_separation_score-1.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=DFPROJ, x=\"ani_date_mrp\", y=\"base_prim_separation_score\", hue=\"nmorphs\", \n",
    "                        col=\"bregion\", col_wrap=6, kind=\"point\", aspect=1.5)\n",
    "        rotateLabel(fig)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "        savefig(fig, f\"{savedir}/base_prim_separation_score-2.pdf\")\n",
    "        \n",
    "        ### SAVE DATAFRAMES\n",
    "        for _df, _name in [\n",
    "            (DFPROJ, \"DFPROJ\"), \n",
    "            (DFPROJ_INDEX, \"DFPROJ_INDEX\"), \n",
    "            (DFDIFFS, \"DFDIFFS\"), \n",
    "            (DFDIFFSPROJ, \"DFDIFFSPROJ\"), \n",
    "            (DFDIFFSINDEX, \"DFDIFFSINDEX\"),\n",
    "            ]:\n",
    "            path = f\"{savedir}/{_name}.pkl\"\n",
    "            pd.to_pickle(_df, path)\n",
    "\n",
    "        print(\"Saving at ...: \", savedir)\n",
    "        for thresh_separation in [0.2, 0.4, 0.6, 0.8]:\n",
    "            sdir = f\"{savedir}/thresh-base_prim_separation_score={thresh_separation}\"\n",
    "            os.makedirs(sdir, exist_ok=True)\n",
    "            _plot(DFPROJ, DFPROJ_INDEX, DFDIFFS, DFDIFFSPROJ, DFDIFFSINDEX, thresh_separation, sdir)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b764ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cldist, _ = euclidian_distance_compute_trajectories_single(PAredu, \"idx_morph_temp\", [\"epoch\"], \n",
    "                                                        version_distance=\"euclidian\", return_cldist=True, get_reverse_also=False)\n",
    "# get distance between 0 and 99\n",
    "# res, DIST_NULL_50, DIST_NULL_95, DIST_NULL_98 = cldist.rsa_distmat_score_same_diff_by_context(\"idx_morph_temp\", [\"epoch\"], None, \"pts\", )\n",
    "# pd.DataFrame(res)\n",
    "dfdist, cldist_agg = cldist.rsa_distmat_score_all_pairs_of_label_groups(get_only_one_direction=False, return_as_clustclass=True)\n",
    "VAR_SCORE = \"dist_yue_diff\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28292a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open\n",
    "/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/Diego-240515-logistic-combine=True/statespace_euclidian/bregion=PMv/morphset=0-ver=traj-dpca-subtrmean=False-projtwind=(0.1, 1.2)-npcs=6/PAredu.pkl\n",
    "# /lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/Diego-240515-logistic-combine=True/statespace_euclidian/bregion=PMv/morphset=0-ver=traj-dpca-subtrmean=False-projtwind=(0.1, 1.2)-npcs=6/Cldist.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cldist_agg.rsa_plot_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu = PAredu.agg_by_time_windows_binned(0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAredu = PAredu.agg_wrapper(\"times\")\n",
    "# PAredu.Times = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d38a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b17416",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiffsindex, dfproj_index = compute_pa_to_df_dist_index_using_splits(PAredu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ab7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiffsindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a112808",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfproj_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=dfproj_index, x=\"idx_morph_temp_rank\", y=\"dist_index\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84423061",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=dfproj_index, x=\"idx_morph_temp_rank\", y=\"dist_index\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d299dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=dfproj_index, x=\"idx_morph_temp_rank\", y=\"dist_index\", kind=\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5533c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Entire pipeline and plots, starting at P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved agg plots\n",
    "# 1. align to transition\n",
    "# 2. plot deviation from linear. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4499555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### find the max index\n",
    "# NOT GOOD -- this overfits to the algined index. Instead, use above, where it usees train-test splits to align.\n",
    "map_anidatemrp_to_peak_idxalongmorph = {}\n",
    "map_anidatemrp_to_peak_idx1 = {}\n",
    "map_anidatemrp_to_peak_idx2 = {}\n",
    "\n",
    "for i, row in DFDIFFSINDEX.groupby([\"ani_date_mrp\", \"bregion\"])[\"dist\"].idxmax().reset_index().iterrows():\n",
    "    map_anidatemrp_to_peak_idxalongmorph[(row[\"ani_date_mrp\"], row[\"bregion\"])] = DFDIFFSINDEX.iloc[row[\"dist\"]][\"idx_along_morph\"]\n",
    "    map_anidatemrp_to_peak_idx1[(row[\"ani_date_mrp\"], row[\"bregion\"])] = DFDIFFSINDEX.iloc[row[\"dist\"]][\"idx1\"]\n",
    "    map_anidatemrp_to_peak_idx2[(row[\"ani_date_mrp\"], row[\"bregion\"])] = DFDIFFSINDEX.iloc[row[\"dist\"]][\"idx2\"]\n",
    "# new index is flanking 0.5, so that 0, 1 are two new indices that flank the peak.\n",
    "list_idx_morph_temp_aligned = []\n",
    "for i, row in DFDIFFSINDEX.iterrows():\n",
    "    idx_along_morph = map_anidatemrp_to_peak_idxalongmorph[(row[\"ani_date_mrp\"], row[\"bregion\"])]\n",
    "    list_idx_morph_temp_aligned.append(row[\"idx_along_morph\"] - idx_along_morph)\n",
    "DFDIFFSINDEX[\"idx_along_morph_centered\"] = list_idx_morph_temp_aligned\n",
    "\n",
    "# Heatmaps\n",
    "dfthis = DFDIFFSINDEX\n",
    "ydist = \"dist\"\n",
    "xvar = \"idx_along_morph_centered\"\n",
    "for morphset_has_intermediate in [False, True]:\n",
    "    dfthisthis = dfthis[dfthis[\"morphset_has_intermediate\"] == morphset_has_intermediate].reset_index(drop=True)\n",
    "    fig, _ = plot_subplots_heatmap(dfthisthis, \"ani_date_mrp\", xvar, ydist, \"bregion\", share_zlim=True, ZLIMS=None)\n",
    "    # savefig(fig, f\"{savedir}/DIST_FROM_BASE_PRIMS-heatmaps_all-{savesuff}-mshasinterm={morphset_has_intermediate}.pdf\")\n",
    "    # plt.close(\"all\")\n",
    "\n",
    "    fig = sns.catplot(data=dfthisthis, x=xvar, y=ydist, hue=\"ani_date_mrp\", kind=\"point\", col=\"bregion\", row=\"animal\")\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0)\n",
    "    fig = sns.catplot(data=dfthisthis, x=xvar, y=ydist, kind=\"point\", col=\"bregion\", row=\"animal\")\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0)\n",
    "\n",
    "# new index is flanking 0.5, so that 0, 1 are two new indices that flank the peak.\n",
    "list_idx_morph_temp_aligned = []\n",
    "for i, row in DFPROJ_INDEX.iterrows():\n",
    "    idx1 = map_anidatemrp_to_peak_idx1[(row[\"ani_date_mrp\"], row[\"bregion\"])]\n",
    "    list_idx_morph_temp_aligned.append(row[\"idx_morph_temp\"] - idx1)\n",
    "\n",
    "DFPROJ_INDEX[\"idx_morph_temp_centered\"] = list_idx_morph_temp_aligned\n",
    "\n",
    "\n",
    "# Heatmaps\n",
    "dfthis = DFPROJ_INDEX\n",
    "ydist = \"dist_index\"\n",
    "xvar = \"idx_morph_temp_centered\"\n",
    "for morphset_has_intermediate in [False, True]:\n",
    "    dfthisthis = dfthis[dfthis[\"morphset_has_intermediate\"] == morphset_has_intermediate].reset_index(drop=True)\n",
    "    fig, _ = plot_subplots_heatmap(dfthisthis, \"ani_date_mrp\", xvar, ydist, \"bregion\", share_zlim=True, ZLIMS=None)\n",
    "    # savefig(fig, f\"{savedir}/DIST_FROM_BASE_PRIMS-heatmaps_all-{savesuff}-mshasinterm={morphset_has_intermediate}.pdf\")\n",
    "    # plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab7089",
   "metadata": {},
   "source": [
    "### COMBINED PLOTS - using logistic score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally remove flankers\n",
    "for exclude_flankers in [False, True]:\n",
    "\n",
    "    if exclude_flankers:\n",
    "        DFPROJ_THIS = DFPROJ[(DFPROJ[\"idx_morph_temp\"]>=0) & (DFPROJ[\"idx_morph_temp\"]<=99)].reset_index(drop=True)\n",
    "    else:\n",
    "        DFPROJ_THIS = DFPROJ\n",
    "\n",
    "    fig, axes = plot_subplots_heatmap(DFPROJ_THIS, \"ani_date_mrp\", \"idx_morph_temp\", \"x_proj\", \"bregion\", share_zlim=True, ZLIMS=[-0.2, 1.2])\n",
    "    savefig(fig, f\"{SAVEDIR}/heatmaps_all-idx_morph_temp-exclude_flankers={exclude_flankers}.pdf\")\n",
    "\n",
    "    fig, axes = plot_subplots_heatmap(DFPROJ_THIS, \"ani_date_mrp\", \"idx_morph_rescaled\", \"x_proj\", \"bregion\", share_zlim=True, ZLIMS=[-0.2, 1.2])\n",
    "    savefig(fig, f\"{SAVEDIR}/heatmaps_all-idx_morph_rescaled-exclude_flankers={exclude_flankers}.pdf\")\n",
    "\n",
    "    if False:\n",
    "        # fig = sns.relplot(data=DFPROJ, x=\"idx_morph_rescaled\", y=\"x_proj\", hue=\"ani_date_mrp\", col=\"bregion\")\n",
    "        fig = sns.relplot(data=DFPROJ_THIS, x=\"idx_morph_rescaled\", y=\"x_proj\", hue=\"ani_date_mrp\", col=\"bregion\", kind=\"line\")\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "            ax.axhline(1)\n",
    "            # ax.set_ylim(0, 1)\n",
    "        savefig(fig, f\"{SAVEDIR}/heatmaps_all-idx_morph_rescaled-exclude_flankers={exclude_flankers}.pdf\")\n",
    "\n",
    "    if False:\n",
    "        fig = sns.relplot(data=DFPROJ_THIS, x=\"idx_morph_rescaled\", y=\"x_proj\", col=\"bregion\", kind=\"line\")\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "            ax.axhline(1)\n",
    "    \n",
    "plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4e07d",
   "metadata": {},
   "source": [
    "##### Compoare logistic vs. linear fit [good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ac0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Devo]\n",
    "\n",
    "if True:\n",
    "    dfthis = dfproj[(dfproj[\"idx_morph_temp\"]>=0) & (dfproj[\"idx_morph_temp\"]<=99)].reset_index(drop=True)\n",
    "else:\n",
    "    dfthis = dfproj\n",
    "# Compare fit using logistic vs. linear curve\n",
    "\n",
    "var_score = \"x_proj\"\n",
    "var_x = \"idx_morph_rescaled\"\n",
    "Y = dfthis[var_score].values\n",
    "X = dfthis[var_x].values\n",
    "X_cat = dfthis[\"idx_morph_temp\"].tolist()\n",
    "nfolds = 2\n",
    "doplot = True\n",
    "\n",
    "dfres_summary, dfres = curve_fit_and_test_wrapper_compare_logistic_linear(X, Y, X_cat, nfolds, doplot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GOOD] pipeline\n",
    "from pythonlib.tools.statstools import curve_fit_and_test\n",
    "from pythonlib.tools.statstools import balanced_stratified_kfold, curve_fit_and_test_wrapper_compare_logistic_linear\n",
    "\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "grpdict = grouping_append_and_return_inner_items_good(DFPROJ, [\"animal\", \"date\", \"morphset\", \"bregion\"])\n",
    "\n",
    "# list_bregion = [\"PMv\", \"PMd\", \"M1\"]\n",
    "# list_bregion = [\"PMv\", \"PMd\", \"SMA\", \"preSMA\", \"M1\", \"FP\"]\n",
    "list_bregion = _REGIONS_IN_ORDER_COMBINED\n",
    "\n",
    "for exclude_flankers in [False, True]:\n",
    "\n",
    "    savedir = f\"{SAVEDIR}/logistic_vs_linear=exclude_flankers={exclude_flankers}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    print(\"... Saving figures at: \", savedir)\n",
    "\n",
    "    RES = []\n",
    "    for (animal, date, morphset, bregion), inds in grpdict.items():\n",
    "        \n",
    "        if bregion not in list_bregion:\n",
    "            continue\n",
    "\n",
    "        print((animal, date, morphset, bregion))\n",
    "        dfproj = DFPROJ.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "        ##### Compare fit using logistic vs. linear curve\n",
    "        # Optionally remove flankers\n",
    "        if exclude_flankers:\n",
    "            dfthis = dfproj[(dfproj[\"idx_morph_temp\"]>=0) & (dfproj[\"idx_morph_temp\"]<=99)].reset_index(drop=True)\n",
    "        else:\n",
    "            dfthis = dfproj\n",
    "\n",
    "        # Compare fit using logistic vs. linear curve\n",
    "        var_score = \"x_proj\"\n",
    "        var_x = \"idx_morph_rescaled\"\n",
    "        Y = dfthis[var_score].values\n",
    "        X = dfthis[var_x].values\n",
    "        X_cat = dfthis[\"idx_morph_temp\"].tolist()\n",
    "        nfolds = 8\n",
    "        doplot = False\n",
    "\n",
    "        dfres_summary, dfres = curve_fit_and_test_wrapper_compare_logistic_linear(X, Y, X_cat, nfolds, doplot)\n",
    "\n",
    "        if dfres_summary is None:\n",
    "            print(\"Failed to converge for: \", bregion)\n",
    "            continue\n",
    "\n",
    "        dfres_summary[\"animal\"] = animal\n",
    "        dfres_summary[\"date\"] = date\n",
    "        dfres_summary[\"morphset\"] = morphset\n",
    "        dfres_summary[\"bregion\"] = bregion\n",
    "\n",
    "        RES.append(dfres_summary)\n",
    "\n",
    "    DFRES = pd.concat(RES).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "    DFRES = append_col_with_grp_index(DFRES, [\"date\", \"morphset\"], \"date_morphset\")\n",
    "    DFRES = append_col_with_grp_index(DFRES, [\"animal\", \"date\", \"morphset\"], \"animal_date_morphset\")\n",
    "\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "    # import seaborn as sns\n",
    "    # sns.catplot(data=DFRES, x=\"model\", y=\"R2\", col=\"bregion\", kind=\"point\")\n",
    "    # sns.catplot(data=DFRES, x=\"model\", y=\"R2_mean\", col=\"bregion\", kind=\"point\")\n",
    "    # sns.catplot(data=DFRES, x=\"model\", y=\"R2\", col=\"bregion\", kind=\"point\")\n",
    "    for animal in DFRES[\"animal\"].unique():\n",
    "        dfres = DFRES[DFRES[\"animal\"] == animal].reset_index(drop=True)\n",
    "        for var_score in [\"R2\"]:\n",
    "            for plot_text in [False, True]:\n",
    "                _, fig = plot_45scatter_means_flexible_grouping(dfres, \"model\", \"linear\", \"sigmoid\", \"bregion\", \n",
    "                                                                var_score, \"date_morphset\", shareaxes=True, SIZE=3, fontsize=5,\n",
    "                                                                plot_text=plot_text)\n",
    "                savefig(fig, f\"{savedir}/scatter-animal={animal}-varscore={var_score}-text={plot_text}-1.pdf\")\n",
    "\n",
    "                _, fig = plot_45scatter_means_flexible_grouping(dfres, \"model\", \"linear\", \"sigmoid\", \"animal_date_morphset\", \n",
    "                                                                var_score, \"bregion\", shareaxes=True, SIZE=3, fontsize=5,\n",
    "                                                                plot_text=plot_text)\n",
    "                savefig(fig, f\"{savedir}/scatter-animal={animal}-varscore={var_score}-text={plot_text}-2.pdf\")\n",
    "\n",
    "                _, fig = plot_45scatter_means_flexible_grouping(dfres, \"model\", \"linear\", \"sigmoid\", \"animal\", \n",
    "                                                                            var_score, \"bregion\", shareaxes=True, SIZE=3, fontsize=5,\n",
    "                                                                            plot_text=plot_text)\n",
    "                savefig(fig, f\"{savedir}/scatter-animal={animal}-varscore={var_score}-text={plot_text}-3.pdf\")\n",
    "\n",
    "                plt.close(\"all\")\n",
    "\n",
    "    dftmp, fig = plot_45scatter_means_flexible_grouping(DFRES, \"model\", \"linear\", \"sigmoid\", \"animal\", \n",
    "                                                                var_score, \"bregion\", shareaxes=True, SIZE=3, fontsize=5,\n",
    "                                                                plot_text=plot_text)\n",
    "    savefig(fig, f\"{savedir}/scatter-by_animal.pdf\")\n",
    "    \n",
    "    from pythonlib.tools.pandastools import summarize_featurediff\n",
    "    dfsummary, dfsummaryflat, COLNAMES_NOABS, COLNAMES_ABS, COLNAMES_DIFF = summarize_featurediff(DFRES, \n",
    "                                                                                        \"model\", \n",
    "                                                                                        [\"linear\", \"sigmoid\"], [\"R2\"], \n",
    "                                                                                        [\"animal\", \"date\", \"morphset\", \"bregion\", \"date_morphset\", \"animal_date_morphset\"])\n",
    "\n",
    "    # Directly compare two areas\n",
    "    var_score = \"R2\"\n",
    "    dftmp, fig = plot_45scatter_means_flexible_grouping(dfsummaryflat, \"bregion\", \"PMv\", \"PMd\", \"animal\", \n",
    "                                                                \"value\", \"date_morphset\", shareaxes=True, SIZE=3, fontsize=5,\n",
    "                                                                plot_text=plot_text)\n",
    "    savefig(fig, f\"{savedir}/scatter-compare-PMd-vs-PMv.pdf\")\n",
    "\n",
    "\n",
    "    fig = sns.catplot(data=dfsummaryflat, x=\"bregion\", y=\"value\", hue=\"date_morphset\", col=\"animal\", legend=False)\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0, color=\"k\", alpha=0.5)\n",
    "    savefig(fig, f\"{savedir}/catplot-1.pdf\")\n",
    "\n",
    "    fig = sns.catplot(data=dfsummaryflat, x=\"bregion\", y=\"value\", col=\"animal\", legend=False, kind=\"bar\")\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0, color=\"k\", alpha=0.5)\n",
    "    savefig(fig, f\"{savedir}/catplot-2.pdf\")\n",
    "\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698f47a",
   "metadata": {},
   "source": [
    "# STATE SPACE VISUALIZATION PLOTS [METHOD 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb5554",
   "metadata": {},
   "source": [
    "### [Method 2] MORE STUFF -- good metrics for categorical response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d9b23",
   "metadata": {},
   "source": [
    "##### [GOOD] Wrapper for all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b193365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_psychoprim_statespace_euclidian_score_and_plot\n",
    "from pythonlib.dataset.dataset_analy.psychometric_singleprims import params_good_morphsets_no_switching\n",
    "\n",
    "list_morphset = sorted(DSmorphsets.Dat[\"morph_set_idx\"].unique())\n",
    "SAVEDIR_BASE = \"/tmp/TESTGOOD\"\n",
    "list_morphset = params_good_morphsets_no_switching(animal, date)\n",
    "analy_psychoprim_statespace_euclidian_score_and_plot(DFallpa, SAVEDIR_BASE, map_tcmorphset_to_idxmorph, animal, date, list_morphset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc8287",
   "metadata": {},
   "source": [
    "##### Devo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be260d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_extract_PA_conditioned\n",
    "scalar_or_traj = \"traj\"\n",
    "tbin_dur = 0.1\n",
    "pca_tbin_slice = 0.05\n",
    "\n",
    "# scalar_or_traj = \"scal\"\n",
    "# tbin_dur = 0.15\n",
    "# pca_tbin_slice = 0.15\n",
    "\n",
    "event = \"03_samp\"\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "\n",
    "dim_red_method = \"dpca\"\n",
    "proj_twind = (0.1, 1.2)\n",
    "\n",
    "NPCS_KEEP = 4\n",
    "bregion = \"PMv\"\n",
    "morphset = 0\n",
    "\n",
    "savedir = f\"/tmp/TESTTEST2/morphset={morphset}\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "Xredu, PAredu= analy_extract_PA_conditioned(DFallpa, bregion, morphset, map_tcmorphset_to_idxmorph, scalar_or_traj, \n",
    "                             event, raw_subtract_mean_each_timepoint, \n",
    "                             dim_red_method, proj_twind, tbin_dur, pca_tbin_slice, NPCS_KEEP, \n",
    "                             savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_morphsmooth_euclidian_score\n",
    "dfproj, dfproj_index, dfdiffs, dfdiffsproj, dfdiffsindex, dfdiffs_categ = analy_morphsmooth_euclidian_score(PAredu, \n",
    "                                                                                                  savedir, \n",
    "                                                                                                  False,\n",
    "                                                                                                  morphset, \n",
    "                                                                                                  DOPLOTS=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d19b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9/0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiffsproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d32367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiffsproj.iloc[2:-2][\"dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _dftmp in [dfproj, dfproj_index, dfdiffs, dfdiffsproj, dfdiffsindex, dfdiffs_categ]:\n",
    "    _dftmp[\"animal\"] = animal\n",
    "    _dftmp[\"date\"] = date\n",
    "    _dftmp[\"morphset\"] = morphset\n",
    "    _dftmp[\"bregion\"] = bregion\n",
    "\n",
    "\n",
    "# Plot finals\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric_mult import plot_with_preprocess\n",
    "plot_with_preprocess(dfproj, dfproj_index, dfdiffs, dfdiffsproj, dfdiffsindex, dfdiffs_categ, savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import _compute_df_using_category_dist_index\n",
    "DFDISTS = _compute_df_using_category_dist_index(PAredu, n_flank_boundary=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8826e93",
   "metadata": {},
   "source": [
    "### Load mult (animal, date) and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPROJ_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data across all (animal, date), combine, and plot.\n",
    "\n",
    "from neuralmonkey.analyses.state_space_good import euclidian_distance_compute_trajectories_single\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "\n",
    "DEBUG = False\n",
    "SAVEDIR_BASE_LOAD = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP\";\n",
    "\n",
    "if DEBUG:\n",
    "    SAVEDIR_BASE_SAVE = \"/tmp\"\n",
    "    # LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517)]\n",
    "    LIST_EXPTS = [(\"Diego\", 240515)]\n",
    "else:\n",
    "    SAVEDIR_BASE_SAVE = SAVEDIR_BASE_LOAD\n",
    "    LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517), (\"Diego\", 240523), (\"Diego\", 240731), (\"Diego\", 240801), (\"Diego\", 240802), \n",
    "                            (\"Pancho\", 240516), (\"Pancho\", 240524), (\"Pancho\", 240801), (\"Pancho\", 240802)]\n",
    "    \n",
    "    if False:\n",
    "        # This is a bad day -- spacing between indices is not uniform\n",
    "        LIST_EXPTS.append((\"Pancho\", 240521))\n",
    "\n",
    "# Params - computing projection score\n",
    "EVENT = \"03_samp\"\n",
    "\n",
    "# scalar_or_traj = \"traj\"\n",
    "# NPCS_KEEP = 8\n",
    "exclude_flank = True\n",
    "nflank = 2\n",
    "dim_red_method = \"dpca\"\n",
    "proj_twind = (0.1, 1.0)\n",
    "combine = True\n",
    "\n",
    "for raw_subtract_mean_each_timepoint in [False, True]:\n",
    "    for scalar_or_traj in [\"traj\", \"scal\"]:\n",
    "        for NPCS_KEEP in [7, 4, 10]:\n",
    "\n",
    "            ### \n",
    "            list_dfproj = []\n",
    "            list_dfdiffs = []\n",
    "            list_dfproj_index = []\n",
    "            list_dfdiffsproj = []\n",
    "            list_dfdiffsindex = []\n",
    "            list_dfdiffscateg = []\n",
    "\n",
    "            for animal, date in LIST_EXPTS:\n",
    "\n",
    "                # LOAD\n",
    "                if nflank is not None:\n",
    "                    SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/euclidian_score_and_plot/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-flank={exclude_flank}-cat_n_flank={nflank}\"\n",
    "                else:\n",
    "                    SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/euclidian_score_and_plot/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-flank={exclude_flank}\"\n",
    "\n",
    "                print(\"Loading from: \", SAVEDIR)\n",
    "\n",
    "                # Load data\n",
    "                DFDIFFS = pd.read_pickle(f\"{SAVEDIR}/DFDIFFS.pkl\")\n",
    "                DFDIFFS_CATEG = pd.read_pickle(f\"{SAVEDIR}/DFDIFFSCATEG.pkl\")\n",
    "                DFDIFFS_INDEX = pd.read_pickle(f\"{SAVEDIR}/DFDIFFSINDEX.pkl\")\n",
    "                DFDIFFS_PROJ = pd.read_pickle(f\"{SAVEDIR}/DFDIFFSPROJ.pkl\")\n",
    "                DFPROJ = pd.read_pickle(f\"{SAVEDIR}/DFPROJ.pkl\")\n",
    "                DFPROJ_INDEX = pd.read_pickle(f\"{SAVEDIR}/DFPROJ_INDEX.pkl\")\n",
    "\n",
    "                ############ COLLECT ALL\n",
    "                list_dfproj.append(DFPROJ)\n",
    "                list_dfproj_index.append(DFPROJ_INDEX)\n",
    "\n",
    "                list_dfdiffs.append(DFDIFFS)\n",
    "                list_dfdiffsproj.append(DFDIFFS_PROJ)\n",
    "                list_dfdiffsindex.append(DFDIFFS_INDEX)\n",
    "                list_dfdiffscateg.append(DFDIFFS_CATEG)\n",
    "\n",
    "            DFPROJ = pd.concat(list_dfproj).reset_index(drop=True)\n",
    "            DFPROJ_INDEX = pd.concat(list_dfproj_index).reset_index(drop=True)\n",
    "            DFDIFFS = pd.concat(list_dfdiffs).reset_index(drop=True)\n",
    "            DFDIFFSPROJ = pd.concat(list_dfdiffsproj).reset_index(drop=True)\n",
    "            DFDIFFSINDEX = pd.concat(list_dfdiffsindex).reset_index(drop=True)\n",
    "            DFDIFFSCATEG = pd.concat(list_dfdiffscateg).reset_index(drop=True)\n",
    "\n",
    "            ############ PLOTS\n",
    "            # # Dir for plotting\n",
    "            if nflank is not None:\n",
    "                savedir = f\"{SAVEDIR_BASE_SAVE}/MULT_BREGION/euclidian_score_and_plot/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-flank={exclude_flank}-cat_n_flank={nflank}\"\n",
    "            else:\n",
    "                savedir = f\"{SAVEDIR_BASE_SAVE}/MULT_BREGION/euclidian_score_and_plot/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-flank={exclude_flank}\"\n",
    "            os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "            ### PLOT\n",
    "            from neuralmonkey.scripts.analy_decode_moment_psychometric_mult import plot_with_preprocess\n",
    "            list_thresh_separation = [0.] # Plot all data, since this is just a single (animal, date)\n",
    "            # list_thresh_separation = None\n",
    "            plot_with_preprocess(DFPROJ, DFPROJ_INDEX, DFDIFFS, DFDIFFSPROJ, DFDIFFSINDEX, DFDIFFSCATEG, savedir,\n",
    "                                    list_thresh_separation=list_thresh_separation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Various post-processing and cleanups\n",
    "\n",
    "# Remove flankers\n",
    "\n",
    "# \n",
    "\n",
    "# Filter to keep just expts with at least n indices relative to aligned center.\n",
    "DFDIFFSCATEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250fe47",
   "metadata": {},
   "source": [
    "# [GOOD] Smooth morph, final version of metric - each morphset get max diff minus min diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38674c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_BASE = \"/tmp/CATEGORICAL\"\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_extract_PA_conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd93748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD code here.\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_psychoprim_statespace_euclidian_smooth_good\n",
    "analy_psychoprim_statespace_euclidian_smooth_good(DFallpa, SAVEDIR, map_tcmorphset_to_idxmorph, map_tcmorphset_to_info, animal, date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ac4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n",
    "morphset = 0\n",
    "NPCS_KEEP = 10\n",
    "_, PAredu = analy_extract_PA_conditioned(DFallpa, bregion, morphset, map_tcmorphset_to_idxmorph, map_tcmorphset_to_info,\n",
    "                                            scalar_or_traj, EVENT, raw_subtract_mean_each_timepoint, \n",
    "                    dim_red_method, proj_twind, tbin_dur, pca_tbin_slice, NPCS_KEEP, \n",
    "                    savedir, restricted_twind_for_dpca=restricted_twind_for_dpca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359928fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/Diego-240802-logistic-combine=True/smooth_euclidian_dist_index_good/ev=03_samp-subtr=False-scal=traj-dimred=dpca-twind=(0.05, 0.7)-npcs=7-nsplitstot=100/bregion=PMv/morphset=5/PAredu.pkl\"\n",
    "import pickle\n",
    "with open(path, \"rb\") as f:\n",
    "    PAredu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1567b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "##### (1) Plot over time\n",
    "### Get single trial pairwise distances over time.\n",
    "var_effect = \"idx_morph_temp\"\n",
    "effect_lev_base1 = 0\n",
    "effect_lev_base2 = 99\n",
    "list_grps_get = [(0,), (99,)] # This is important, or else will fail if there are any (idx|assign) with only one datapt.\n",
    "\n",
    "# (A) plot over time, including negetive times.\n",
    "version = \"pts_time\"    \n",
    "DFPROJ_INDEX, DFDIST, DFPROJ_INDEX_AGG, DFDIST_AGG, DFPROJ_INDEX_DIFFS = PAredu.dataextract_as_distance_index_between_two_base_classes(var_effect, \n",
    "                                                                                    effect_lev_base1, effect_lev_base2, list_grps_get=list_grps_get,\n",
    "                                                                                    version = version)\n",
    "fig = sns.relplot(data=DFPROJ_INDEX, x=\"time_bin\", y=\"dist_index\", kind=\"line\", hue=\"idx_morph_temp\")\n",
    "savefig(fig, f\"{savedir}/DFPROJ_INDEX-time.pdf\")\n",
    "\n",
    "# (B) Average over time, post-samp\n",
    "pa = PAredu.slice_by_dim_values_wrapper(\"times\", twind_score)\n",
    "version = \"pts_scal\"\n",
    "DFPROJ_INDEX, DFDIST, DFPROJ_INDEX_AGG, DFDIST_AGG, DFPROJ_INDEX_DIFFS = pa.dataextract_as_distance_index_between_two_base_classes(var_effect, \n",
    "                                                                                    effect_lev_base1, effect_lev_base2, list_grps_get=list_grps_get,\n",
    "                                                                                    version = version)\n",
    "fig = sns.catplot(data=DFPROJ_INDEX, x=\"idx_morph_temp\", y=\"dist_index\", kind=\"point\")\n",
    "savefig(fig, f\"{savedir}/DFPROJ_INDEX-scal-twind={twind_score}.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed168a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPROJ_INDEX[\"idx_morph_temp_str\"] = DFPROJ_INDEX[\"idx_morph_temp\"].astype(str)\n",
    "hue_order = [str(x) for x in sorted(DFPROJ_INDEX[\"idx_morph_temp\"].unique())]\n",
    "fig = sns.relplot(data=DFPROJ_INDEX, x=\"time_bin\", y=\"dist_index\", kind=\"line\", hue=\"idx_morph_temp_str\", \n",
    "                  hue_order = hue_order, errorbar=(\"ci\", 68))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db968478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot state space trajectories.\n",
    "SAVEDIR_BASE = \"/tmp/STATESPACE\"\n",
    "##############################\n",
    "### State space plots\n",
    "savedir = f\"{SAVEDIR_BASE}/state_space\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "LIST_VAR = [\n",
    "    \"idx_morph_temp\",\n",
    "]\n",
    "LIST_VARS_OTHERS = [\n",
    "    (\"task_kind\",),\n",
    "]\n",
    "LIST_PRUNE_MIN_N_LEVS = [1 for _ in range(len(LIST_VAR))]\n",
    "nmin_trials_per_lev = 1\n",
    "list_dim_timecourse = [0,1,2,3,4,5, 6, 7]\n",
    "list_dims = [(0,1), (2,3), (4,5), (6,7)]\n",
    "PAredu.plot_state_space_good_wrapper(savedir, LIST_VAR, LIST_VARS_OTHERS, \n",
    "                                    LIST_PRUNE_MIN_N_LEVS=LIST_PRUNE_MIN_N_LEVS, nmin_trials_per_lev=nmin_trials_per_lev,\n",
    "                                    list_dim_timecourse=list_dim_timecourse, list_dims=list_dims)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c6922",
   "metadata": {},
   "source": [
    "# Load mult data across (animal, date) and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a5159",
   "metadata": {},
   "source": [
    "### (1) Set of plots #1 -- simply, specific of effect to PMv\n",
    "(Below: loads processed metrics for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main plots, to show that PMv has stronger neural jumps compared to chance and to other areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data across all (animal, date), combine, and plot.\n",
    "from neuralmonkey.analyses.state_space_good import euclidian_distance_compute_trajectories_single\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index, aggregGeneral\n",
    "from pythonlib.dataset.dataset_analy.psychometric_singleprims import params_has_intermediate_shape, params_base_prims_not_separated\n",
    "\n",
    "DEBUG = False\n",
    "SAVEDIR_BASE_LOAD = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP\";\n",
    "\n",
    "if DEBUG:\n",
    "    SAVEDIR_BASE_SAVE = \"/tmp\"\n",
    "    # LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517)]\n",
    "    LIST_EXPTS = [(\"Diego\", 240515)]\n",
    "else:\n",
    "    SAVEDIR_BASE_SAVE = SAVEDIR_BASE_LOAD\n",
    "    LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517), (\"Diego\", 240523), (\"Diego\", 240731), (\"Diego\", 240801), \n",
    "                  (\"Diego\", 240802), (\"Pancho\", 240516), (\"Pancho\", 240524), (\"Pancho\", 240801), (\"Pancho\", 240802)]\n",
    "    \n",
    "    if False:\n",
    "        # This is a bad day -- spacing between indices is not uniform\n",
    "        LIST_EXPTS.append((\"Pancho\", 240521))\n",
    "\n",
    "# Params - computing projection score\n",
    "EVENT = \"03_samp\"\n",
    "\n",
    "# scalar_or_traj = \"traj\"\n",
    "# NPCS_KEEP = 8\n",
    "\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "scalar_or_traj = \"traj\"\n",
    "NPCS_KEEP = 7\n",
    "\n",
    "dim_red_method = \"dpca\"\n",
    "\n",
    "# proj_twind = (-0.1, 1.2)\n",
    "# n_splits_tot = 200\n",
    "# exclude_flank = None\n",
    "\n",
    "# n_splits_tot = 100\n",
    "# proj_twind = (0.05, 0.7)\n",
    "# exclude_flank = True\n",
    "\n",
    "# proj_twind = (0.05, 1.1)\n",
    "# n_splits_tot = 100\n",
    "# exclude_flank = True\n",
    "# nsplits_inner = 2\n",
    "\n",
    "proj_twind = (0.05, 0.7)\n",
    "n_splits_tot = 100\n",
    "exclude_flank = True\n",
    "nsplits_inner = 2\n",
    "\n",
    "### \n",
    "\n",
    "combine = True\n",
    "\n",
    "list_df1 = []\n",
    "list_df2 = []\n",
    "list_df3 = []\n",
    "\n",
    "for animal, date in LIST_EXPTS:\n",
    "\n",
    "    if exclude_flank is None:\n",
    "        # Older, diff name\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}\"\n",
    "    elif nsplits_inner is None:\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}-exclflank={exclude_flank}\"\n",
    "    else:\n",
    "        # The most recent\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}-nsplitinn={nsplits_inner}-exclflank={exclude_flank}\"\n",
    "\n",
    "    # \n",
    "    print(\"Loading from: \", SAVEDIR)\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        # DF_DISTIDX_SCORE = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE.pkl\")\n",
    "        DF_DISTIDX_SCORE_SPLITS = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE_SPLITS.pkl\")\n",
    "        DF_DISTIDX_SCORE_SPLITS_STKBEH = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE_SPLITS_STKBEH.pkl\")\n",
    "        DF_DISTIDX_SCORE_SPLITS_STKTASK = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE_SPLITS_STKTASK.pkl\")\n",
    "\n",
    "    except FileNotFoundError as err:\n",
    "        print(\"DID NOT FIND FILE\")\n",
    "\n",
    "    # Agg\n",
    "    DF_DISTIDX_SCORE = aggregGeneral(DF_DISTIDX_SCORE_SPLITS, [\"bregion\", \"morphset\"], [\"dist_index_diff_min\", \"dist_index_diff_max\", \"dist_index_diff_mean_notmax\", \"dist_index_diff_max_single\", \"dist_index_diff_v2_max\", \"dist_index_diff_v2_minleft\", \"dist_index_diff_v2_minright\"])\n",
    "    DF_DISTIDX_SCORE_STKBEH = aggregGeneral(DF_DISTIDX_SCORE_SPLITS_STKBEH, [\"bregion\", \"morphset\"], [\"dist_index_diff_min\", \"dist_index_diff_max\", \"dist_index_diff_mean_notmax\", \"dist_index_diff_max_single\", \"dist_index_diff_v2_max\", \"dist_index_diff_v2_minleft\", \"dist_index_diff_v2_minright\"])\n",
    "    DF_DISTIDX_SCORE_STKTASK = aggregGeneral(DF_DISTIDX_SCORE_SPLITS_STKTASK, [\"bregion\", \"morphset\"], [\"dist_index_diff_min\", \"dist_index_diff_max\", \"dist_index_diff_mean_notmax\", \"dist_index_diff_max_single\", \"dist_index_diff_v2_max\", \"dist_index_diff_v2_minleft\", \"dist_index_diff_v2_minright\"])\n",
    "\n",
    "    for df in [DF_DISTIDX_SCORE_SPLITS, DF_DISTIDX_SCORE_SPLITS_STKBEH, DF_DISTIDX_SCORE_SPLITS_STKTASK, DF_DISTIDX_SCORE, DF_DISTIDX_SCORE_STKBEH, DF_DISTIDX_SCORE_STKTASK]:\n",
    "        df[\"maxsingle_minus_min\"] = df[\"dist_index_diff_max_single\"] - df[\"dist_index_diff_min\"]\n",
    "        df[\"maxsingle_minus_mean\"] = df[\"dist_index_diff_max_single\"] - df[\"dist_index_diff_mean_notmax\"]\n",
    "        df[\"max_minus_min\"] = df[\"dist_index_diff_max\"] - df[\"dist_index_diff_min\"]\n",
    "        df[\"max_minus_mean\"] = df[\"dist_index_diff_max\"] - df[\"dist_index_diff_mean_notmax\"]\n",
    "        df[\"dist_index_diff_v2_minboth\"] = 0.5*(df[\"dist_index_diff_v2_minleft\"] + df[\"dist_index_diff_v2_minright\"])\n",
    "        df[\"max_minus_min_v2\"] = df[\"dist_index_diff_v2_max\"] - df[\"dist_index_diff_v2_minboth\"]\n",
    "\n",
    "        if \"dist_index_diff_mean\" in df.columns:\n",
    "            df[\"dist_index_diff_mean_all\"] = df[\"dist_index_diff_mean\"]\n",
    "        else:\n",
    "            # Hacky, take mean of these            \n",
    "            df[\"dist_index_diff_mean_all\"] = 0.5*(df[\"dist_index_diff_max\"] +  df[\"dist_index_diff_mean_notmax\"])\n",
    "        # df[\"dist_index_diff_mean_all\"] = df[\"dist_index_diff_max\"] +  df[\"dist_index_diff_mean_notmax\"]\n",
    "\n",
    "        df[\"animal\"] = animal\n",
    "        df[\"date\"] = date\n",
    "\n",
    "        df[\"morphset_has_intermediate\"] = [row[\"morphset\"] in params_has_intermediate_shape(row[\"animal\"], row[\"date\"]) for _, row in df.iterrows()]\n",
    "        df[\"keep_because_base_prims_separated\"] = [row[\"morphset\"] not in params_base_prims_not_separated(row[\"animal\"], row[\"date\"]) for _, row in df.iterrows()]\n",
    "\n",
    "        # Hacky, should really get mean over all, but here get average of max and min. close enougn\n",
    "        # df[\"MAX_INDEX\"] = (df[\"dist_index_diff_max\"]/(df[\"dist_index_diff_max\"] + df[\"dist_index_diff_mean_notmax\"])) - 0.5\n",
    "        df[\"MAX_INDEX\"] = np.log2((df[\"dist_index_diff_max\"]/(df[\"dist_index_diff_mean_all\"])))\n",
    "\n",
    "    DF_DISTIDX_SCORE = append_col_with_grp_index(DF_DISTIDX_SCORE, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "    DF_DISTIDX_SCORE_STKBEH = append_col_with_grp_index(DF_DISTIDX_SCORE_STKBEH, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "    DF_DISTIDX_SCORE_STKTASK = append_col_with_grp_index(DF_DISTIDX_SCORE_STKTASK, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")\n",
    "\n",
    "    DF_DISTIDX_SCORE = append_col_with_grp_index(DF_DISTIDX_SCORE, [\"morphset_has_intermediate\", \"keep_because_base_prims_separated\"], \"hasinterm|separated\")\n",
    "    \n",
    "    ############ COLLECT ALL\n",
    "    list_df1.append(DF_DISTIDX_SCORE)\n",
    "    # list_df2.append(DF_DISTIDX_SCORE_SPLITS)\n",
    "    list_df2.append(DF_DISTIDX_SCORE_STKBEH)\n",
    "    list_df3.append(DF_DISTIDX_SCORE_STKTASK)\n",
    "\n",
    "DF_DISTIDX_SCORE = pd.concat(list_df1).reset_index(drop=True)\n",
    "DF_DISTIDX_SCORE_STKBEH = pd.concat(list_df2).reset_index(drop=True)\n",
    "DF_DISTIDX_SCORE_STKTASK = pd.concat(list_df3).reset_index(drop=True)\n",
    "# DF_DISTIDX_SCORE_SPLITS = pd.concat(list_df2).reset_index(drop=True)\n",
    "\n",
    "#######################\n",
    "# Append beh and task scores onto neural dataframe\n",
    "from pythonlib.tools.pandastools import slice_by_row_label\n",
    "\n",
    "row_labels = DF_DISTIDX_SCORE[\"ani_date_mrp\"].tolist()\n",
    "\n",
    "dfappend = slice_by_row_label(DF_DISTIDX_SCORE_STKBEH, \"ani_date_mrp\", row_labels, prune_to_values_that_exist_in_df=False)\n",
    "DF_DISTIDX_SCORE[\"MAX_INDEX_BEH\"] = dfappend[\"MAX_INDEX\"]\n",
    "\n",
    "dfappend = slice_by_row_label(DF_DISTIDX_SCORE_STKTASK, \"ani_date_mrp\", row_labels, prune_to_values_that_exist_in_df=False)\n",
    "DF_DISTIDX_SCORE[\"MAX_INDEX_TASK\"] = dfappend[\"MAX_INDEX\"]\n",
    "\n",
    "from pythonlib.tools.pandastools import stringify_values\n",
    "DF_DISTIDX_SCORE = stringify_values(DF_DISTIDX_SCORE)\n",
    "# DF_DISTIDX_SCORE_SPLITS = stringify_values(DF_DISTIDX_SCORE_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores\n",
    "SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/MULT_BREGION/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}-nsplitinn={nsplits_inner}-exclflank={exclude_flank}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfthis = DF_DISTIDX_SCORE[DF_DISTIDX_SCORE[\"hasinterm|separated\"] == \"0|1\"].reset_index(drop=True)\n",
    "LIST_THRESH = np.percentile(dfthis[\"dist_index_diff_mean_all\"], [0, 20, 50, 70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plots\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping, plot_45scatter_means_flexible_grouping_from_wideform\n",
    "import seaborn as sns\n",
    "from pythonlib.tools.plottools import savefig\n",
    "\n",
    "import seaborn as sns\n",
    "for thresh in LIST_THRESH:\n",
    "\n",
    "    savedir = f\"{SAVEDIR}/plots_summarize_diff_index-thresh={thresh}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    for y in [\"max_minus_min\", \"max_minus_mean\", \"max_minus_min_v2\", \"dist_index_diff_max\", \"dist_index_diff_mean_notmax\"]:\n",
    "\n",
    "        fig = sns.catplot(data=DF_DISTIDX_SCORE, x=\"bregion\", y=y, row=\"animal\", col=\"hasinterm|separated\")\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/catplot-{y}-1.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=DF_DISTIDX_SCORE, x=\"bregion\", y=y, row=\"animal\", col=\"hasinterm|separated\", kind=\"point\", hue=\"ani_date_mrp\", legend=False)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/catplot-{y}-2.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=DF_DISTIDX_SCORE, x=\"bregion\", y=y, row=\"animal\", col=\"hasinterm|separated\", kind=\"bar\")\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/catplot-{y}-3.pdf\")\n",
    "\n",
    "        for animal in DF_DISTIDX_SCORE[\"animal\"].unique():\n",
    "            \n",
    "            dfthis = DF_DISTIDX_SCORE[DF_DISTIDX_SCORE[\"animal\"] == animal].reset_index(drop=True)\n",
    "\n",
    "            fig = sns.catplot(data=dfthis, x=\"bregion\", y=y, col=\"date\", hue=\"morphset\", kind=\"point\", row=\"hasinterm|separated\")\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "            savefig(fig, f\"{savedir}/catplot-{y}-ani={animal}-1.pdf\")\n",
    "\n",
    "            fig = sns.catplot(data=dfthis, x=\"bregion\", y=y, col=\"date\", kind=\"bar\", row=\"hasinterm|separated\")\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "            savefig(fig, f\"{savedir}/catplot-{y}-ani={animal}-2.pdf\")\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    # Good plots of dist_ratio (index), which requires filtering data to just those that are actually well-separated (thresh).\n",
    "    # NOTE: this doesnt look that good, in terms of using the slope of dist_index_diff_max vs. dist_index_diff_mean_notmax\n",
    "    # Instead, use MAX_INDEX\n",
    "    dfthis = DF_DISTIDX_SCORE[DF_DISTIDX_SCORE[\"hasinterm|separated\"] == \"0|1\"].reset_index(drop=True)\n",
    "    dfthis = dfthis[dfthis[\"dist_index_diff_mean_all\"] > thresh].reset_index(drop=True)\n",
    "\n",
    "    var_datapt = \"ani_date_mrp\"\n",
    "    var_subplot = \"bregion\"\n",
    "\n",
    "    for x_lev_manip, y_lev_manip in [\n",
    "        (\"dist_index_diff_min\", \"dist_index_diff_max\"),\n",
    "        (\"dist_index_diff_mean_notmax\", \"dist_index_diff_max\"),\n",
    "        (\"dist_index_diff_mean_all\", \"dist_index_diff_max\"),\n",
    "        (\"dist_index_diff_mean_all\", \"dist_index_diff_max\"),\n",
    "        (\"dist_index_diff_v2_minboth\", \"dist_index_diff_v2_max\"),\n",
    "        # (\"dist_index_diff_mean_all\", \"MAX_INDEX\"), # These are done below.\n",
    "        # (\"MAX_INDEX_BEH\", \"MAX_INDEX\"),\n",
    "        # (\"MAX_INDEX_TASK\", \"MAX_INDEX\"),\n",
    "        ]:\n",
    "        for animal in [\"Diego\", \"Pancho\"]:\n",
    "            for plot_text in [False, True]:\n",
    "                dfthisthis = dfthis[dfthis[\"animal\"] == animal].reset_index(drop=True)\n",
    "                _, fig = plot_45scatter_means_flexible_grouping_from_wideform(dfthisthis, x_lev_manip, y_lev_manip,\n",
    "                                                        var_subplot, var_datapt, plot_text=plot_text, SIZE=4, shareaxes=True)\n",
    "                savefig(fig, f\"{savedir}/scatter-ani={animal}-x={x_lev_manip}-y={y_lev_manip}-text={plot_text}.pdf\")\n",
    "\n",
    "        # fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", col=\"animal\", jitter=True, alpha=0.8)\n",
    "        # for ax in fig.axes.flatten():\n",
    "        #     ax.axhline(0)\n",
    "        #     fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", col=\"animal\", kind=\"bar\", errorbar=(\"ci\", 68))\n",
    "        # for ax in fig.axes.flatten():\n",
    "        #     ax.axhline(0)\n",
    "    plt.close(\"all\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90f9b1",
   "metadata": {},
   "source": [
    "### (2) Set of plots #2 -- comparison of neural to behavioral jumps\n",
    "Load PAredu (e.g., to recompute things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask series of questions related to whether any neural jumps are related to behavioral jumps.\n",
    "# See gslides for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data across all (animal, date), combine, and plot.\n",
    "from neuralmonkey.analyses.state_space_good import euclidian_distance_compute_trajectories_single\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index, aggregGeneral\n",
    "from pythonlib.dataset.dataset_analy.psychometric_singleprims import params_has_intermediate_shape, params_base_prims_not_separated\n",
    "import pickle\n",
    "\n",
    "### \n",
    "\n",
    "combine = True\n",
    "\n",
    "list_df1 = []\n",
    "list_df2 = []\n",
    "list_df3 = []\n",
    "\n",
    "LIST_PAREDU = []\n",
    "\n",
    "for animal, date in LIST_EXPTS:\n",
    "\n",
    "    if exclude_flank is None:\n",
    "        # Older, diff name\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}\"\n",
    "    elif nsplits_inner is None:\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}-exclflank={exclude_flank}\"\n",
    "    else:\n",
    "        # The most recent\n",
    "        SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}-nsplitinn={nsplits_inner}-exclflank={exclude_flank}\"\n",
    "    \n",
    "    print(\"Loading from: \", SAVEDIR)\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        # DF_DISTIDX_SCORE = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE.pkl\")\n",
    "        DF_DISTIDX_SCORE_SPLITS = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE_SPLITS.pkl\")\n",
    "        DF_DISTIDX_SCORE_SPLITS_STKBEH = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE_SPLITS_STKBEH.pkl\")\n",
    "        DF_DISTIDX_SCORE_SPLITS_STKTASK = pd.read_pickle(f\"{SAVEDIR}/DF_DISTIDX_SCORE_SPLITS_STKTASK.pkl\")\n",
    "\n",
    "    except FileNotFoundError as err:\n",
    "        print(\"DID NOT FIND FILE\")\n",
    "        assert False\n",
    "\n",
    "    list_bregion = DF_DISTIDX_SCORE_SPLITS[\"bregion\"].unique().tolist()\n",
    "    list_morphset = DF_DISTIDX_SCORE_SPLITS[\"morphset\"].unique().tolist()\n",
    "    for bregion in list_bregion:\n",
    "        for morphset in list_morphset:\n",
    "            path = f\"{SAVEDIR}/bregion={bregion}/morphset={morphset}/PAredu.pkl\"\n",
    "            with open(path, \"rb\") as f:\n",
    "                paredu = pickle.load(f)\n",
    "\n",
    "            LIST_PAREDU.append({\n",
    "                \"PAredu\":paredu,\n",
    "                \"animal\":animal,\n",
    "                \"date\":date,\n",
    "                \"bregion\":bregion,\n",
    "                \"morphset\":morphset\n",
    "            })\n",
    "\n",
    "DF_PAREDU = pd.DataFrame(LIST_PAREDU)\n",
    "DF_PAREDU = append_col_with_grp_index(DF_PAREDU, [\"animal\", \"date\", \"morphset\"], \"ani_date_mrp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d42ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "SAVEDIR_BASE_LOAD = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP\";\n",
    "SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/MULT_BREGION/smooth_euclidian_dist_index_good/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-nsplitstot={n_splits_tot}-nsplitinn={nsplits_inner}-exclflank={exclude_flank}\"\n",
    "\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf597ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PAREDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfddd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect correlation between (beh and neural) for all PA\n",
    "from scipy import stats\n",
    "\n",
    "# twind_score_this = (0.05, 1.1) \n",
    "twind_score_this = proj_twind \n",
    "plot_example = False\n",
    "\n",
    "list_corr = []\n",
    "list_p = []\n",
    "list_dfindex_neural = []\n",
    "list_dfindexdiffs_neural = []\n",
    "\n",
    "list_dfindex_beh = []\n",
    "list_dfindexdiffs_beh = []\n",
    "\n",
    "map_anidamo_to_dfindexdiffs_beh = {}\n",
    "map_anidamo_to_dfindex_beh = {}\n",
    "\n",
    "for i, row in DF_PAREDU.iterrows():\n",
    "    PAredu = row[\"PAredu\"]\n",
    "    \n",
    "    # Process PAredu\n",
    "    PAneural = PAredu.slice_by_dim_values_wrapper(\"times\", twind_score_this)\n",
    "\n",
    "    # Get their dist index as function of index morph.\n",
    "    var_effect = \"idx_morph_temp\"\n",
    "    effect_lev_base1 = 0\n",
    "    effect_lev_base2 = 99\n",
    "    list_grps_get = [(0,), (99,)] # This is important, or else will fail if there are any (idx|assign) with only one datapt.\n",
    "    plot_train_test_scores = True\n",
    "    version = \"grps_scal\"\n",
    "    dfindex_neural , _, _, _, dfindexdiffs_neural = PAneural.dataextract_as_distance_index_between_two_base_classes(var_effect, \n",
    "                                                                    effect_lev_base1, effect_lev_base2, list_grps_get=list_grps_get,\n",
    "                                                                    version = version, PLOT=plot_example)\n",
    "    list_dfindex_neural.append(dfindex_neural)\n",
    "    list_dfindexdiffs_neural.append(dfindexdiffs_neural)\n",
    "\n",
    "    key = (row[\"animal\"], row[\"date\"], row[\"morphset\"], row[\"bregion\"])\n",
    "    if key in map_anidamo_to_dfindexdiffs_beh:\n",
    "        dfindex_beh = map_anidamo_to_dfindex_beh[key]\n",
    "        dfindexdiffs_beh = map_anidamo_to_dfindexdiffs_beh[key]\n",
    "    else:    \n",
    "        PAstroke = PAredu.behavior_replace_neural_with_strokes(\"beh\")\n",
    "        dfindex_beh , _, _, _, dfindexdiffs_beh = PAstroke.dataextract_as_distance_index_between_two_base_classes(var_effect, \n",
    "                                                                        effect_lev_base1, effect_lev_base2, list_grps_get=list_grps_get,\n",
    "                                                                        version = version, PLOT=plot_example)\n",
    "        map_anidamo_to_dfindex_beh[key] = dfindex_beh\n",
    "        map_anidamo_to_dfindexdiffs_beh[key] = dfindexdiffs_beh\n",
    "    list_dfindex_beh.append(dfindex_beh)\n",
    "    list_dfindexdiffs_beh.append(dfindexdiffs_beh)\n",
    "\n",
    "    ################################\n",
    "    # (1) Get correlation\n",
    "    res = stats.spearmanr(dfindexdiffs_neural[\"dist\"], dfindexdiffs_beh[\"dist\"])\n",
    "    res = stats.pearsonr(dfindexdiffs_neural[\"dist\"], dfindexdiffs_beh[\"dist\"])\n",
    "\n",
    "    if plot_example:\n",
    "        # (2) Plot them, overlaying\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        # sns.lineplot(dfindexdiffs_beh, x=\"idx_along_morph\", y=\"dist\", ax=ax)\n",
    "        # sns.lineplot(dfindexdiffs_neural, x=\"idx_along_morph\", y=\"dist\", ax=ax)\n",
    "\n",
    "        # sns.lineplot(dfindex_neural, x=\"idx_morph_temp_rank\", y=\"dist_index\", ax=ax)\n",
    "        # sns.lineplot(dfindex_beh, x=\"idx_morph_temp_rank\", y=\"dist_index\", ax=ax)\n",
    "\n",
    "        # sns.lineplot(dfindex_neural, x=\"idx_morph_temp_rank\", y=\"dist_index_norm\", ax=ax)\n",
    "        # sns.lineplot(dfindex_beh, x=\"idx_morph_temp_rank\", y=\"dist_index_norm\", ax=ax)\n",
    "\n",
    "        ax.plot(dfindex_neural[\"idx_morph_temp_rank\"], dfindex_neural[\"dist_index_norm\"], \"-o\", label=\"dfindex_neural\")\n",
    "        ax.plot(dfindex_beh[\"idx_morph_temp_rank\"], dfindex_beh[\"dist_index_norm\"], \"-o\", label=\"dfindex_beh\")\n",
    "        # sns.lineplot(dfindex_beh, x=\"idx_morph_temp_rank\", y=\"dist_index_norm\", ax=ax)\n",
    "\n",
    "        ax.legend()\n",
    "\n",
    "    if plot_example:\n",
    "        assert False, \"so not too many plots\"\n",
    "\n",
    "    list_corr.append(res.statistic)\n",
    "    list_p.append(res.pvalue)\n",
    "\n",
    "DF_PAREDU[\"corr\"] = list_corr\n",
    "DF_PAREDU[\"pval\"] = list_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ad836",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PAREDU[\"vals_neural\"] = [x.loc[:, [\"idx_morph_temp_rank\", \"dist_index\"]].values for x in list_dfindex_neural] # (ninds, 2)\n",
    "DF_PAREDU[\"valdiffs_neural\"] = [x.loc[:, [\"idx_along_morph\", \"dist\"]].values for x in list_dfindexdiffs_neural]\n",
    "\n",
    "DF_PAREDU[\"vals_beh\"] = [x.loc[:, [\"idx_morph_temp_rank\", \"dist_index\"]].values for x in list_dfindex_beh] # (ninds, 2)\n",
    "DF_PAREDU[\"valdiffs_beh\"] = [x.loc[:, [\"idx_along_morph\", \"dist\"]].values for x in list_dfindexdiffs_beh]\n",
    "\n",
    "from pythonlib.tools.pandastools import slice_by_row_label\n",
    "\n",
    "# Append corr results to main dataframe\n",
    "row_labels = DF_DISTIDX_SCORE[\"ani_date_mrp\"].tolist()\n",
    "\n",
    "dfappend = slice_by_row_label(DF_PAREDU, \"ani_date_mrp\", row_labels, prune_to_values_that_exist_in_df=False)\n",
    "DF_DISTIDX_SCORE[\"corr\"] = dfappend[\"corr\"]\n",
    "DF_DISTIDX_SCORE[\"pval\"] = dfappend[\"pval\"]\n",
    "\n",
    "alpha = 0.05\n",
    "DF_DISTIDX_SCORE[\"pval_significant\"] = DF_DISTIDX_SCORE[\"pval\"]<alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9af221",
   "metadata": {},
   "source": [
    "##### Plot summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b72975",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfthis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be44dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63cb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All plots of corr (between nerual and beh).\n",
    "for thresh, thresh_prctile in [(thresh, thresh_prctile) for thresh in LIST_THRESH for thresh_prctile in [20, 40]]:\n",
    "    savedir = f\"{SAVEDIR}/plots_corr_of_neural_beh_function-thresh={thresh}-threshprctl={thresh_prctile}\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping, plot_45scatter_means_flexible_grouping_from_wideform\n",
    "    import seaborn as sns\n",
    "    from pythonlib.tools.plottools import savefig\n",
    "\n",
    "    ### Filter dataframes\n",
    "    # (1) Simple, hand coded is good for PMv\n",
    "    DFTHIS = DF_DISTIDX_SCORE[DF_DISTIDX_SCORE[\"hasinterm|separated\"] == \"0|1\"].reset_index(drop=True)\n",
    "\n",
    "    # (2) For all areas, there is separation \n",
    "    # Determine threshold using this:\n",
    "    DFTHIS[\"dist_index_diff_mean_all\"].hist(bins=50)\n",
    "    # thresh = 0.005 \n",
    "    DFTHIS_SIGNI = DFTHIS[DFTHIS[\"dist_index_diff_mean_all\"] > thresh].reset_index(drop=True)\n",
    "\n",
    "    # (3) Just those with clear jump, for both neural and beh\n",
    "    # thresh_prctile = 25 # Determine this empriclaly, not sure what makes sense.\n",
    "    thresh_neural = np.percentile(DFTHIS_SIGNI[\"MAX_INDEX\"], [thresh_prctile])[0]\n",
    "    thresh_beh = np.percentile(DFTHIS_SIGNI[\"MAX_INDEX_BEH\"], [thresh_prctile])[0]\n",
    "    DFTHIS_SIGNI[\"max_index_high_beh_neural\"] = (DFTHIS_SIGNI[\"MAX_INDEX\"]>thresh_neural) & (DFTHIS_SIGNI[\"MAX_INDEX_BEH\"]>thresh_beh) \n",
    "    DFTHIS_SIGNI_HIGHMAXINDEX = DFTHIS_SIGNI[(DFTHIS_SIGNI[\"MAX_INDEX\"]>thresh_neural) & (DFTHIS_SIGNI[\"MAX_INDEX_BEH\"]>thresh_beh) ]\n",
    "\n",
    "    # (4) Just those without high beh separation\n",
    "    DFTHIS_SIGNI_LOWJUMP = DFTHIS_SIGNI[(DFTHIS_SIGNI[\"MAX_INDEX_BEH\"]<thresh_beh)]\n",
    "\n",
    "\n",
    "    # Are correlations positive?\n",
    "    for dfthis, savesuff in [\n",
    "        (DFTHIS, \"1_all_neur_separated\"),\n",
    "        (DFTHIS_SIGNI, \"2_all_neural_separated_strict\"), \n",
    "        (DFTHIS_SIGNI_HIGHMAXINDEX, \"3_both_behneural_jump\"),\n",
    "        (DFTHIS_SIGNI_LOWJUMP, \"4_no_beh_jump\")\n",
    "        ]:\n",
    "\n",
    "        if len(dfthis)==0:\n",
    "            continue\n",
    "        \n",
    "        ############# CORRELATIONS\n",
    "        # Simply plot all values of corr\n",
    "        y_lev_manip = \"corr\"\n",
    "        fig = sns.catplot(dfthis, x=\"ani_date_mrp\", y=y_lev_manip, hue=\"animal\", kind=\"bar\", aspect=3, col = \"bregion\", col_wrap=4)\n",
    "        from pythonlib.tools.snstools import rotateLabel\n",
    "        rotateLabel(fig)\n",
    "        savefig(fig, f\"{savedir}/df={savesuff}-catplot-allcorr-0.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", hue=\"pval_significant\", \n",
    "                        col=\"animal\", jitter=True, alpha=0.8)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "        savefig(fig, f\"{savedir}/df={savesuff}-catplot-allcorr-1.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", col=\"animal\", kind=\"bar\", errorbar=(\"ci\", 68))\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "        savefig(fig, f\"{savedir}/df={savesuff}-catplot-allcorr-2.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", hue=\"animal\", \n",
    "                        col=\"animal\", jitter=True, alpha=0.8)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "        savefig(fig, f\"{savedir}/df={savesuff}-catplot-allcorr-3.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", kind=\"bar\", errorbar=(\"ci\", 68))\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0)\n",
    "        savefig(fig, f\"{savedir}/df={savesuff}-catplot-allcorr-4.pdf\")\n",
    "\n",
    "        ################ ALL QUESTIONS\n",
    "        # Good plots of dist_ratio (index), which requires filtering data to just those that are actually well-separated (thresh).\n",
    "        var_datapt = \"ani_date_mrp\"\n",
    "        var_subplot = \"bregion\"\n",
    "        for x_lev_manip, y_lev_manip in [\n",
    "            (\"MAX_INDEX_BEH\", \"MAX_INDEX\"), # Q: neural strong discretization compared to beh (even when beh jump is 0)\n",
    "            (\"MAX_INDEX_BEH\", \"corr\"), # Q: Do neural and beh indices line up?\n",
    "            (\"MAX_INDEX\", \"corr\"),\n",
    "            (\"MAX_INDEX_TASK\", \"MAX_INDEX_BEH\"), # Q: behavior also shows jump?\n",
    "            ]:\n",
    "\n",
    "            fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", col=\"animal\", jitter=True, alpha=0.8)\n",
    "            savefig(fig, f\"{savedir}/df={savesuff}-catplot-{y_lev_manip}-1.pdf\")\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(0)\n",
    "            \n",
    "            fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, row=\"hasinterm|separated\", col=\"animal\", kind=\"bar\", errorbar=(\"ci\", 68))\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(0)\n",
    "            savefig(fig, f\"{savedir}/df={savesuff}-catplot-{y_lev_manip}-2.pdf\")\n",
    "\n",
    "            fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, col=\"hasinterm|separated\", jitter=True, alpha=0.8)\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(0)\n",
    "            savefig(fig, f\"{savedir}/df={savesuff}-catplot-{y_lev_manip}-3.pdf\")\n",
    "\n",
    "            fig = sns.catplot(data=dfthis, x=\"bregion\", y=y_lev_manip, col=\"hasinterm|separated\", kind=\"bar\", errorbar=(\"ci\", 68))\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(0)\n",
    "            savefig(fig, f\"{savedir}/df={savesuff}-catplot-{y_lev_manip}-4.pdf\")\n",
    "\n",
    "            for animal in [\"Diego\", \"Pancho\"]:\n",
    "                dfthisthis = dfthis[dfthis[\"animal\"] == animal].reset_index(drop=True)\n",
    "                for shareaxes in [False, True]:\n",
    "                    for plot_text in [False, True]:\n",
    "                        _, fig = plot_45scatter_means_flexible_grouping_from_wideform(dfthisthis, x_lev_manip, y_lev_manip,\n",
    "                                                                var_subplot, var_datapt, plot_text=plot_text, SIZE=4, shareaxes=shareaxes)\n",
    "                        \n",
    "                        if fig is not None:\n",
    "                            savefig(fig, f\"{savedir}/df={savesuff}-ani={animal}-scatter-x={x_lev_manip}-y={y_lev_manip}-shareax={shareaxes}-text={plot_text}.pdf\")\n",
    "                \n",
    "        plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f65bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: corr, with shuffle (to more strongly show that beh and neural are not aligned)\n",
    "\n",
    "# TODO: overlay plots of all neural and beh\n",
    "# use this:\n",
    "# DF_PAREDU[\"valdiffs_neural\"]\n",
    "# DF_PAREDU[\"valdiffs_beh\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48c534",
   "metadata": {},
   "source": [
    "### Also score \"difference index\" for behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ebe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this has all been moved into analy_psychoprim_statespace_euclidian_smooth_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96553107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/Diego-240515-logistic-combine=True/smooth_euclidian_dist_index_good/ev=03_samp-subtr=False-scal=traj-dimred=dpca-twind=(-0.1, 1.2)-npcs=7-nsplitstot=200/bregion=PMv/morphset=0/PAredu.pkl\"\n",
    "import pickle\n",
    "with open(path, \"rb\") as f:\n",
    "    PAredu = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"beh\"\n",
    "PAstroke = PAredu.behavior_replace_neural_with_strokes(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fdf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFDIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bc7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1684b6",
   "metadata": {},
   "source": [
    "# [State space and euclidian] Switching morphs -- Traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4803e20",
   "metadata": {},
   "source": [
    "##### [Good] Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR_BASE = \"/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_switching_statespace_euclidian_score_and_plot\n",
    "analy_switching_statespace_euclidian_score_and_plot(DFallpa, SAVEDIR_BASE, map_tcmorphset_to_idxmorph, list_morphset=[0,1],\n",
    "                                                    map_tcmorphset_to_info=map_tcmorphset_to_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c51f7",
   "metadata": {},
   "source": [
    "##### [Devo] Code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f765a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_extract_PA_conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04af293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune neural data to keep only those trialcodes that are in DSmorphsets\n",
    "list_trialcode = sorted(set([x[0] for x in map_tcmorphset_to_idxmorph]))\n",
    "DFallpa[\"pa\"] = [pa.slice_by_labels(\"trials\", \"trialcode\", list_trialcode) for pa in DFallpa[\"pa\"].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff216fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bregion = \"PMv\"\n",
    "morphset = 5\n",
    "\n",
    "# scalar_or_traj = \"scal\"\n",
    "# tbin_dur = 0.15\n",
    "# pca_tbin_slice = 0.15\n",
    "# proj_twind = (0.01, 1.2)\n",
    "\n",
    "scalar_or_traj = \"traj\"\n",
    "tbin_dur = 0.1\n",
    "pca_tbin_slice = 0.1\n",
    "proj_twind = (-0.1, 1.2)\n",
    "restricted_twind_for_dpca = (0.1, 1.2)\n",
    "\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "EVENT = \"03_samp\"\n",
    "dim_red_method = \"dpca\"\n",
    "NPCS_KEEP = 5\n",
    "savedir = \"/tmp\"\n",
    "\n",
    "\n",
    "_, PAredu = analy_extract_PA_conditioned(DFallpa, bregion, morphset, map_tcmorphset_to_idxmorph, map_tcmorphset_to_info,\n",
    "                                            scalar_or_traj, EVENT, raw_subtract_mean_each_timepoint, \n",
    "                    dim_red_method, proj_twind, tbin_dur, pca_tbin_slice, NPCS_KEEP, \n",
    "                    savedir, restricted_twind_for_dpca=restricted_twind_for_dpca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b259376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_switching_statespace_euclidian_traj\n",
    "SAVEDIR = \"/tmp/TRAJ\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "DFPROJ_INDEX, DFDIST, DFPROJ_INDEX_AGG, DFDIST_AGG, dist_index_diff_base, dist_index_diff_notambig, dist_index_diff_ambig = analy_switching_statespace_euclidian_traj(PAredu, SAVEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d981587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import _compute_df_using_dist_index_traj\n",
    "var_effect = \"idxmorph_assigned\"\n",
    "effect_lev_base1 = \"0|base1\"\n",
    "effect_lev_base2 = \"99|base2\"\n",
    "\n",
    "DFPROJ_INDEX, DFDIST, DFPROJ_INDEX_AGG, DFDIST_AGG = _compute_df_using_dist_index_traj(PAredu, var_effect, effect_lev_base1, effect_lev_base2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OLD METHOD, obsolete, replaced by better ocde above]\n",
    "# labs2 = [x for x in dfthis.index.tolist() if map_idxassign_to_assignedbase[x] == \"not_ambig_base2\"]\n",
    "\n",
    "# dfthis.loc[labs2, :].mean(axis)\n",
    "\n",
    "# labs1 = [x for x in dfthis.index.tolist() if map_idxassign_to_assignedbase[x] == \"not_ambig_base1\"]\n",
    "\n",
    "# [x for x in dfthis.index.tolist() if map_idxassign_to_assignedbase[x] == \"ambig_base2\"]\n",
    "# [x for x in dfthis.index.tolist() if map_idxassign_to_assignedbase[x] == \"ambig_base1\"]\n",
    "\n",
    "# map_idxassign_to_label = {}\n",
    "# for idx in list_idx:\n",
    "#     if idx == 0:\n",
    "#         map_idxassign_to_label[f\"{idx}|base1\"] = \"base1\"\n",
    "#     elif idx == 99:\n",
    "#         map_idxassign_to_label[f\"{idx}|base2\"] = \"base2\"\n",
    "#     elif map_morphsetidx_to_assignedbase_or_ambig[(morphset, idx)] == \"not_ambig_base1\":\n",
    "#         map_idxassign_to_label[f\"{idx}|base1\"] = \"base1\"\n",
    "#     elif map_morphsetidx_to_assignedbase_or_ambig[(morphset, idx)] == \"not_ambig_base2\":\n",
    "#         map_idxassign_to_label[f\"{idx}|base2\"] = \"base2\"\n",
    "#     elif map_morphsetidx_to_assignedbase_or_ambig[(morphset, idx)] == \"is_ambig\":\n",
    "#         map_idxassign_to_label[f\"{idx}|base1\"] = \"base1\"\n",
    "#         map_idxassign_to_label[f\"{idx}|base2\"] = \"base2\"\n",
    "#     else:\n",
    "#         assert False\n",
    "    \n",
    "# map_idxassign_to_label = {}\n",
    "# for idx in list_idx:\n",
    "#     if idx == 0:\n",
    "#         map_idxassign_to_label[f\"{idx}|base1\"] = \"base\"\n",
    "#     elif idx == 99:\n",
    "#         map_idxassign_to_label[f\"{idx}|base2\"] = \"base\"\n",
    "#     elif map_morphsetidx_to_assignedbase_or_ambig[(morphset, idx)] == \"not_ambig_base1\":\n",
    "#         map_idxassign_to_label[f\"{idx}|base1\"] = \"not_ambig\"\n",
    "#     elif map_morphsetidx_to_assignedbase_or_ambig[(morphset, idx)] == \"not_ambig_base2\":\n",
    "#         map_idxassign_to_label[f\"{idx}|base2\"] = \"not_ambig\"\n",
    "#     elif map_morphsetidx_to_assignedbase_or_ambig[(morphset, idx)] == \"is_ambig\":\n",
    "#         map_idxassign_to_label[f\"{idx}|base1\"] = \"ambig\"\n",
    "#         map_idxassign_to_label[f\"{idx}|base2\"] = \"ambig\"\n",
    "#     else:\n",
    "#         assert False\n",
    "    \n",
    "# for k, v in map_idxassign_to_label.items():\n",
    "#     print(k, \" -- \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cf41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code:\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_switching_statespace_euclidian_good\n",
    "analy_switching_statespace_euclidian_good(PAredu, SAVEDIR, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ace72",
   "metadata": {},
   "source": [
    "### [GOOD] Load results across (animal, dates) [switching]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.dataset.dataset_analy.psychometric_singleprims import params_good_morphsets_switching\n",
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import _analy_switching_statespace_euclidian_traj_plots, _analy_switching_statespace_euclidian_traj_computediff\n",
    "from pythonlib.tools.pandastools import grouping_append_and_return_inner_items_good\n",
    "from neuralmonkey.classes.session import _REGIONS_IN_ORDER_COMBINED\n",
    "from pythonlib.tools.plottools import savefig\n",
    "from pythonlib.tools.pandastools import aggregGeneral, extract_with_levels_of_conjunction_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = \"03_samp\"\n",
    "\n",
    "LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517), (\"Diego\", 240521), (\"Diego\", 240523), (\"Diego\", 240730), \n",
    "              (\"Pancho\", 240516), (\"Pancho\", 240521), (\"Pancho\", 240524)]\n",
    "# LIST_EXPTS = [(\"Diego\", 240515), (\"Diego\", 240517), (\"Diego\", 240730), \n",
    "#               (\"Pancho\", 240516), (\"Pancho\", 240521), (\"Pancho\", 240524)]\n",
    "\n",
    "LIST_REGIONS = _REGIONS_IN_ORDER_COMBINED\n",
    "\n",
    "\n",
    "exclude_flank = True\n",
    "dim_red_method = \"dpca\"\n",
    "proj_twind = (0.1, 1.0)\n",
    "combine = True\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "scalar_or_traj = \"traj\"\n",
    "NPCS_KEEP = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_clean_labels = False\n",
    "\n",
    "list_dfdist = []\n",
    "list_dfdist_agg = []\n",
    "list_dfproj_index = []\n",
    "list_dfproj_index_agg = []\n",
    "for animal, date in LIST_EXPTS:\n",
    "    map_morphsetgood_to_indices = params_good_morphsets_switching(animal, date)\n",
    "    for bregion in LIST_REGIONS:\n",
    "        for morphset, inds_keep in map_morphsetgood_to_indices.items():     \n",
    "\n",
    "            if len(inds_keep)==0:\n",
    "                # hand label not inlcudes this morphset...\n",
    "                continue\n",
    "            \n",
    "            # LOAD\n",
    "            SAVEDIR = f\"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/{animal}-{date}-logistic-combine=True/switching_euclidian_score_and_plot_traj/ev={event}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred=dpca-twind=(-0.1, 1.2)-npcs={NPCS_KEEP}/bregion={bregion}/morphset={morphset}\"\n",
    "            # SAVEDIR = f\"{SAVEDIR_BASE_LOAD}/{animal}-{date}-logistic-combine={combine}/euclidian_score_and_plot/ev={EVENT}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}-flank={exclude_flank}\"\n",
    "            print(\"Loading from: \", SAVEDIR)\n",
    "\n",
    "            # Load data\n",
    "            DFDIST = pd.read_pickle(f\"{SAVEDIR}/DFDIST.pkl\")\n",
    "            DFDIST_AGG = pd.read_pickle(f\"{SAVEDIR}/DFDIST_AGG.pkl\")\n",
    "            DFPROJ_INDEX = pd.read_pickle(f\"{SAVEDIR}/DFPROJ_INDEX.pkl\")\n",
    "            DFPROJ_INDEX_AGG = pd.read_pickle(f\"{SAVEDIR}/DFPROJ_INDEX_AGG.pkl\")\n",
    "\n",
    "            try:\n",
    "                # all inds that should be ambig --> make sure they are called that.\n",
    "                # prolbem is that some cases I did not call ambig if there were only a few cases of base1/base2.  But Ishould not \n",
    "                # do this.\n",
    "                # if False: # skip this check, since sometimes auto detection doesnt call it ambig (too few trials, or is too noisy)\n",
    "                assert all(DFDIST[DFDIST[\"idx_morph_temp\"].isin(inds_keep)][\"assigned_label\"] == \"ambig\"), \"why this was not called ambig? Prob it only had a couple trials... Fix the original code that called ambig vs. not-ambig.\"\n",
    "\n",
    "                # all other inds --> make sure not called ambig.\n",
    "                assert not any(DFDIST[~DFDIST[\"idx_morph_temp\"].isin(inds_keep)][\"assigned_label\"]==\"ambig\"), \"in this case, definitely go with my hand label\"\n",
    "            except AssertionError as err:\n",
    "                from pythonlib.tools.pandastools import grouping_print_n_samples\n",
    "                print(\"--------------\")\n",
    "                print(animal, date, morphset)\n",
    "                print(\"Indices I manually said to keep: \", inds_keep)\n",
    "                print(\"Indices automatilcaly labeled as ambig: \", grouping_print_n_samples(DFDIST, [\"idx_morph_temp\", \"assigned_label\"]))\n",
    "                # print(err)\n",
    "                raise err\n",
    "            \n",
    "            if not all([x in DFPROJ_INDEX_AGG[\"assigned_base\"].unique().tolist() for x in ['base1', 'ambig_base1', 'ambig_base2', 'base2']]):\n",
    "            # if not sorted(DFPROJ_INDEX_AGG[\"assigned_label\"].unique()) == ['ambig', 'base', 'not_ambig']:\n",
    "                print(\"Skipping\", animal, date, morphset, \" since doesnt have all 3 trial labels. Just has: \", sorted(DFPROJ_INDEX_AGG[\"assigned_label\"].unique()))     \n",
    "                continue\n",
    "\n",
    "            for df in [DFDIST, DFDIST_AGG, DFPROJ_INDEX, DFPROJ_INDEX_AGG]:\n",
    "                df[\"animal\"] = animal\n",
    "                df[\"date\"] = date\n",
    "                df[\"bregion\"] = bregion\n",
    "                df[\"morphset\"] = morphset\n",
    "\n",
    "                # Important, numerical precision...\n",
    "                df[\"time_bin\"] = df[\"time_bin\"].apply(lambda x:np.round(x, 3))\n",
    "                # df.sort_values(\"time_bin\").reset_index(drop=True)\n",
    "\n",
    "            ############ COLLECT ALL\n",
    "            list_dfdist.append(DFDIST)\n",
    "            list_dfdist_agg.append(DFDIST_AGG)\n",
    "            list_dfproj_index.append(DFPROJ_INDEX)\n",
    "            list_dfproj_index_agg.append(DFPROJ_INDEX_AGG)\n",
    "\n",
    "\n",
    "for do_clean_labels in [False, True]:\n",
    "\n",
    "    SAVEDIR = f\"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/MULT_BREGION/switching_euclidian_score_and_plot_traj/ev={event}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred=dpca-twind=(-0.1, 1.2)-npcs={NPCS_KEEP}-cleanlabs={do_clean_labels}\"\n",
    "    savedir = SAVEDIR\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    if False: # not being used\n",
    "        DFDIST = pd.concat(list_dfdist).reset_index(drop=True)\n",
    "    DFDIST_AGG = pd.concat(list_dfdist_agg).reset_index(drop=True)\n",
    "    DFPROJ_INDEX = pd.concat(list_dfproj_index).reset_index(drop=True)\n",
    "    DFPROJ_INDEX_AGG = pd.concat(list_dfproj_index_agg).reset_index(drop=True)\n",
    "\n",
    "    # del list_dfdist, list_dfdist_agg, list_dfproj_index, list_dfproj_index_agg\n",
    "\n",
    "    from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "    DFPROJ_INDEX = append_col_with_grp_index(DFPROJ_INDEX, [\"animal\", \"date\", \"morphset\"], \"an_da_ms\")\n",
    "    DFPROJ_INDEX_AGG = append_col_with_grp_index(DFPROJ_INDEX_AGG, [\"animal\", \"date\", \"morphset\"], \"an_da_ms\")\n",
    "    DFDIST_AGG = append_col_with_grp_index(DFDIST_AGG, [\"animal\", \"date\", \"morphset\"], \"an_da_ms\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### OPTIONALLY, clean to just morphsets with both bases for all 3 labels\n",
    "    if do_clean_labels:\n",
    "        ### First, each of (ambig, base, notambig) should have both bas1 and badses2\n",
    "        vars_others = [\"an_da_ms\", \"assigned_label\"]\n",
    "        levels_var = [\"base1\", \"base2\"]\n",
    "        DFPROJ_INDEX, _= extract_with_levels_of_conjunction_vars(DFPROJ_INDEX, \"assigned_base_simple\", vars_others, levels_var, \n",
    "                                                1, False, 2, False, plot_counts_heatmap_savepath=None)\n",
    "\n",
    "        DFPROJ_INDEX_AGG, _= extract_with_levels_of_conjunction_vars(DFPROJ_INDEX_AGG, \"assigned_base_simple\", vars_others, levels_var, \n",
    "                                                1, False, 2, False, plot_counts_heatmap_savepath=None)\n",
    "\n",
    "        DFDIST_AGG, _= extract_with_levels_of_conjunction_vars(DFDIST_AGG, \"assigned_base_simple\", vars_others, levels_var, \n",
    "                                                1, False, 2, False, plot_counts_heatmap_savepath=None)\n",
    "        \n",
    "        # Second, only morphsets with all (3 labels x 2 base1/base2)\n",
    "        vars_others = [\"an_da_ms\"]\n",
    "        levels_var = [\"not_ambig\", \"base\", \"ambig\"]\n",
    "        DFPROJ_INDEX, _= extract_with_levels_of_conjunction_vars(DFPROJ_INDEX, \"assigned_label\", vars_others, levels_var, \n",
    "                                                1, False, len(levels_var), False, plot_counts_heatmap_savepath=None)\n",
    "\n",
    "        DFPROJ_INDEX_AGG, _= extract_with_levels_of_conjunction_vars(DFPROJ_INDEX_AGG, \"assigned_label\", vars_others, levels_var, \n",
    "                                                1, False, len(levels_var), False, plot_counts_heatmap_savepath=None)\n",
    "\n",
    "        DFDIST_AGG, _= extract_with_levels_of_conjunction_vars(DFDIST_AGG, \"assigned_label\", vars_others, levels_var, \n",
    "                                                1, False, len(levels_var), False, plot_counts_heatmap_savepath=None)\n",
    "\n",
    "    # (2) Get different of dist (base2-base1)\n",
    "    # A single score (difference of dist index)\n",
    "    grpdict = grouping_append_and_return_inner_items_good(DFPROJ_INDEX_AGG, [\"animal\", \"date\", \"morphset\", \"bregion\"])\n",
    "\n",
    "    # - get for each (animal, date, morphset)\n",
    "    res = []\n",
    "    res2 = []\n",
    "    for (animal, date, morphset, bregion), inds in grpdict.items():\n",
    "        df = DFPROJ_INDEX_AGG.iloc[inds].reset_index(drop=True)\n",
    "        times, dist_index_diff_base, dist_index_diff_notambig, dist_index_diff_ambig = _analy_switching_statespace_euclidian_traj_computediff(df)\n",
    "\n",
    "        label = \"base\"\n",
    "        for t, d in zip(times, dist_index_diff_base):\n",
    "            res2.append({\n",
    "                \"animal\":animal,\n",
    "                \"date\":date, \n",
    "                \"morphset\":morphset,\n",
    "                \"bregion\":bregion,\n",
    "                \"time_bin\":t, \n",
    "                \"dist_index_diff\":d,\n",
    "                \"label\":label\n",
    "            })\n",
    "\n",
    "        if dist_index_diff_notambig is not None:\n",
    "            label = \"not_ambig\"\n",
    "            for t, d in zip(times, dist_index_diff_notambig):\n",
    "                res2.append({\n",
    "                    \"animal\":animal,\n",
    "                    \"date\":date, \n",
    "                    \"morphset\":morphset,\n",
    "                    \"bregion\":bregion,\n",
    "                    \"time_bin\":t, \n",
    "                    \"dist_index_diff\":d,\n",
    "                    \"label\":label\n",
    "                })\n",
    "\n",
    "        if dist_index_diff_ambig is not None:\n",
    "            label = \"ambig\"\n",
    "            for t, d in zip(times, dist_index_diff_ambig):\n",
    "                res2.append({\n",
    "                    \"animal\":animal,\n",
    "                    \"date\":date, \n",
    "                    \"morphset\":morphset,\n",
    "                    \"bregion\":bregion,\n",
    "                    \"time_bin\":t, \n",
    "                    \"dist_index_diff\":d,\n",
    "                    \"label\":label\n",
    "                })\n",
    "\n",
    "    DFPROJ_INDEX_AGG_DIFF = pd.DataFrame(res2)\n",
    "    DFPROJ_INDEX_AGG_DIFF = append_col_with_grp_index(DFPROJ_INDEX_AGG_DIFF, [\"animal\", \"date\", \"morphset\"], \"an_da_ms\")\n",
    "\n",
    "    ############### PLOTS\n",
    "    # (1) Heatmaps\n",
    "\n",
    "    # (1) Catplots\n",
    "    import seaborn as sns\n",
    "    hue_order = [\"base1\", \"not_ambig_base1\", \"ambig_base1\", \"ambig_base2\", \"not_ambig_base2\", \"base2\"]\n",
    "    fig = sns.relplot(data=DFPROJ_INDEX_AGG, x=\"time_bin\", y=\"dist_index\", hue=\"assigned_base\", kind=\"line\", errorbar=(\"ci\", 68),\n",
    "                    col = \"bregion\", row=\"animal\", hue_order=hue_order)\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "    savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG-dist_index-assigned_base.pdf\")\n",
    "\n",
    "    hue_order = [\"base1\", \"not_ambig_base1\", \"ambig_base1\", \"ambig_base2\", \"not_ambig_base2\", \"base2\"]\n",
    "    fig = sns.relplot(data=DFPROJ_INDEX_AGG, x=\"time_bin\", y=\"dist_index\", hue=\"assigned_base\", kind=\"line\", errorbar=(\"ci\", 68),\n",
    "                    col = \"an_da_ms\", col_wrap = 6, hue_order=hue_order)\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "    savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG-dist_index-assigned_base-2.pdf\")\n",
    "\n",
    "    for animal in DFPROJ_INDEX_AGG[\"animal\"].unique().tolist():\n",
    "        dfthis = DFPROJ_INDEX_AGG[DFPROJ_INDEX_AGG[\"animal\"]==animal].reset_index(drop=True)\n",
    "\n",
    "        hue_order = sorted(dfthis[\"assigned_base_simple\"].unique())\n",
    "        fig = sns.relplot(data=dfthis, x=\"time_bin\", y=\"dist_index\", hue=\"assigned_base_simple\", \n",
    "                        row=\"assigned_label\", col=\"bregion\", kind=\"line\", errorbar=(\"ci\", 68), hue_order=hue_order)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "            ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG-dist_index-assigned_base_simple-{animal}.pdf\")\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    hue_order = [\"base\", \"not_ambig\", \"ambig\"]\n",
    "    fig = sns.relplot(data=DFPROJ_INDEX_AGG_DIFF, x=\"time_bin\", y=\"dist_index_diff\", hue=\"label\", \n",
    "                    kind=\"line\", errorbar=(\"ci\", 68), col = \"bregion\", row=\"animal\", hue_order=hue_order,\n",
    "                    col_order=LIST_REGIONS)\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "        ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "    savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_DIFF-dist_index-label.pdf\")\n",
    "\n",
    "\n",
    "    for animal in DFPROJ_INDEX_AGG_DIFF[\"animal\"].unique().tolist():\n",
    "        dfthis = DFPROJ_INDEX_AGG_DIFF[DFPROJ_INDEX_AGG_DIFF[\"animal\"]==animal].reset_index(drop=True)\n",
    "\n",
    "        hue_order = [\"base\", \"not_ambig\", \"ambig\"]\n",
    "        fig = sns.relplot(data=dfthis, x=\"time_bin\", y=\"dist_index_diff\", hue=\"an_da_ms\", \n",
    "                        kind=\"line\", errorbar=(\"ci\", 68), col = \"bregion\", row=\"label\", col_order=LIST_REGIONS)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "            ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_DIFF-dist_index-label-{animal}-1.pdf\")\n",
    "        \n",
    "    for animal in DFPROJ_INDEX_AGG_DIFF[\"animal\"].unique().tolist():\n",
    "        dfthis = DFPROJ_INDEX_AGG_DIFF[DFPROJ_INDEX_AGG_DIFF[\"animal\"]==animal].reset_index(drop=True)\n",
    "\n",
    "        hue_order = [\"base\", \"not_ambig\", \"ambig\"]\n",
    "        fig = sns.relplot(data=dfthis, x=\"time_bin\", y=\"dist_index_diff\", hue=\"label\", \n",
    "                        kind=\"line\", errorbar=(\"ci\", 68), col = \"bregion\", row=\"an_da_ms\", col_order=LIST_REGIONS)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "            ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_DIFF-dist_index-label-{animal}-2.pdf\")\n",
    "\n",
    "    col_order = [\"base\", \"not_ambig\", \"ambig\"]\n",
    "    fig = sns.relplot(data=DFPROJ_INDEX_AGG_DIFF, x=\"time_bin\", y=\"dist_index_diff\", hue=\"bregion\", \n",
    "                    kind=\"line\", errorbar=(\"ci\", 68), col = \"label\", row=\"animal\", hue_order=LIST_REGIONS,\n",
    "                    col_order=col_order)\n",
    "    for ax in fig.axes.flatten():\n",
    "        ax.axhline(0, color=\"k\", alpha=0.25)\n",
    "        ax.axvline(0, color=\"k\", alpha=0.25)\n",
    "    savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_DIFF-dist_index-bregion.pdf\")\n",
    "\n",
    "    plt.close(\"all\")\n",
    "        \n",
    "    # Convert to scalar, for single trial plot\n",
    "\n",
    "    for twind_scal, suff in [\n",
    "        [(0.05, 1.1), \"all\"],\n",
    "        [(0.65, 1.1), \"late\"],\n",
    "        [(0.05,  0.65), \"early\"],\n",
    "        ]:\n",
    "\n",
    "        # GEt time window, and scalar\n",
    "        _DFPROJ_INDEX = DFPROJ_INDEX[(DFPROJ_INDEX[\"time_bin\"]>twind_scal[0]) & (DFPROJ_INDEX[\"time_bin\"]<twind_scal[1])].reset_index(drop=True)\n",
    "        DFPROJ_INDEX_SCAL = aggregGeneral(_DFPROJ_INDEX, [\"idx_row_datapt\", \"animal\", \"date\", \"morphset\", \"bregion\"], [\"dist_index\"], nonnumercols=\"all\")\n",
    "\n",
    "        _DFPROJ_INDEX_AGG = DFPROJ_INDEX_AGG[(DFPROJ_INDEX_AGG[\"time_bin\"]>twind_scal[0]) & (DFPROJ_INDEX_AGG[\"time_bin\"]<twind_scal[1])].reset_index(drop=True)\n",
    "        DFPROJ_INDEX_AGG_SCAL = aggregGeneral(_DFPROJ_INDEX_AGG, [\"idxmorph_assigned\", \"animal\", \"date\", \"morphset\", \"bregion\"], [\"dist_index\"], nonnumercols=\"all\")\n",
    "\n",
    "        dfthis = DFPROJ_INDEX_AGG_DIFF[(DFPROJ_INDEX_AGG_DIFF[\"time_bin\"]>twind_scal[0]) & (DFPROJ_INDEX_AGG_DIFF[\"time_bin\"]<twind_scal[1])].reset_index(drop=True)\n",
    "        DFPROJ_INDEX_AGG_DIFF_SCAL = aggregGeneral(dfthis, [\"animal\", \"date\", \"morphset\", \"bregion\", \"label\", \"an_da_ms\"], [\"dist_index_diff\"], nonnumercols=\"all\")\n",
    "\n",
    "        fig = sns.catplot(data=DFPROJ_INDEX_SCAL, x=\"idx_morph_temp\", y=\"dist_index\", hue=\"assigned_base_simple\", col=\"bregion\", row=\"an_da_ms\", \n",
    "                    jitter=True, alpha=0.75, sharex=False)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_SCAL-all-1-twind={suff}.pdf\")\n",
    "\n",
    "        fig = sns.catplot(data=DFPROJ_INDEX_SCAL, x=\"idx_morph_temp\", y=\"dist_index\", hue=\"assigned_base_simple\", col=\"bregion\", row=\"an_da_ms\", \n",
    "                    kind=\"point\", errorbar=(\"ci\", 68), sharex=False)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_SCAL-all-2-twind={suff}.pdf\")\n",
    "        \n",
    "        # Summary \n",
    "        order = [\"ambig\", \"not_ambig\", \"base\"]\n",
    "        fig = sns.catplot(data=DFPROJ_INDEX_AGG_SCAL, x=\"assigned_label\", y=\"dist_index\", hue=\"assigned_base_simple\", \n",
    "                    col=\"bregion\", row=\"animal\", kind=\"point\", errorbar=(\"ci\", 68), sharex=False, order=order)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_SCAL-assigned_label-twind={suff}.pdf\")\n",
    "\n",
    "        # # Summary (pts)\n",
    "        # order = [\"ambig\", \"not_ambig\", \"base\"]\n",
    "        # fig = sns.catplot(data=DFPROJ_INDEX_SCAL, x=\"assigned_label\", y=\"dist_index\", hue=\"assigned_base_simple\", \n",
    "        #             col=\"bregion\", row=\"animal\", alpha=0.2, jitter=True, order=order)\n",
    "        # for ax in fig.axes.flatten():\n",
    "        #     ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        # savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_SCAL-assigned_base.pdf\")\n",
    "\n",
    "        # Summary \n",
    "        order = [\"base1\", \"not_ambig_base1\", \"ambig_base1\", \"ambig_base2\", \"not_ambig_base2\", \"base2\"]\n",
    "        fig = sns.catplot(data=DFPROJ_INDEX_AGG_SCAL, x=\"assigned_base\", y=\"dist_index\", hue=\"assigned_base_simple\", \n",
    "                    col=\"bregion\", row=\"animal\", kind=\"point\", errorbar=(\"ci\", 68), sharex=False, order=order)\n",
    "        for ax in fig.axes.flatten():\n",
    "            ax.axhline(0.5, color=\"k\", alpha=0.25)\n",
    "        savefig(fig, f\"{savedir}/DFPROJ_INDEX_AGG_SCAL-assigned_base-twind={suff}.pdf\")\n",
    "\n",
    "        plt.close(\"all\")\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ### Bimodality of trials, comparing base1 vs. base2, for ambiguous (and other labels)\n",
    "        # --> Mainly plots to visualzie distributes, and normaliozation to center all morphsets so they line up. \n",
    "        # (i.e., For each morphset, subtract the mean between the means of base1/base2 -- i.e, to center it for easy plotting)\n",
    "        from pythonlib.tools.pandastools import extract_with_levels_of_conjunction_vars\n",
    "        from pythonlib.tools.snstools import rotateLabel\n",
    "\n",
    "        # First, for each label (i.e, out of 3 kinds), get its mean, once for each (animal, date, morphset, bregion).\n",
    "        # Do this in multiple \"meaning\" steps, so that it is not influenced by n trials.\n",
    "        dftmp = aggregGeneral(DFPROJ_INDEX_SCAL, [\"an_da_ms\", \"bregion\", \"assigned_base_simple\", \"assigned_label\", \"idx_morph_temp\"], [\"dist_index\"], nonnumercols=\"all\")\n",
    "        dftmp = aggregGeneral(dftmp, [\"an_da_ms\", \"bregion\", \"assigned_base_simple\", \"assigned_label\"], [\"dist_index\"], nonnumercols=\"all\")\n",
    "        dftmp = aggregGeneral(dftmp, [\"an_da_ms\", \"bregion\", \"assigned_label\"], [\"dist_index\"], nonnumercols=\"all\")\n",
    "        dftmp = dftmp.rename(columns={\"dist_index\": f'dist_index_mean'})\n",
    "\n",
    "        # Second, append this to the original dataframe\n",
    "        # 1. Clean up dataset, so all labels have base1/base2\n",
    "        vars_others = [\"an_da_ms\", \"bregion\", \"assigned_label\"]\n",
    "        levels_var = [\"base1\", \"base2\"]\n",
    "        DFPROJ_INDEX_SCAL_CLEAN, _= extract_with_levels_of_conjunction_vars(DFPROJ_INDEX_SCAL, \"assigned_base_simple\", vars_others, levels_var, \n",
    "                                                1, False, 2, False, plot_counts_heatmap_savepath=None)\n",
    "        print(len(DFPROJ_INDEX_SCAL), len(DFPROJ_INDEX_SCAL_CLEAN))\n",
    "\n",
    "        # 2. Merge mean bvalues to this trial-dataset\n",
    "        DFPROJ_INDEX_SCAL_CLEAN = pd.merge(DFPROJ_INDEX_SCAL_CLEAN, dftmp, on=[\"an_da_ms\", \"bregion\", \"assigned_label\"], how='left')\n",
    "        DFPROJ_INDEX_SCAL_CLEAN[\"dist_index_minus_mean\"] = DFPROJ_INDEX_SCAL_CLEAN[\"dist_index\"] - DFPROJ_INDEX_SCAL_CLEAN[\"dist_index_mean\"]\n",
    "\n",
    "        DFPROJ_INDEX_SCAL_CLEAN[\"animal\"] = DFPROJ_INDEX_SCAL_CLEAN[\"animal_x\"]\n",
    "\n",
    "        ##### plots\n",
    "        for yval, yzero in [\n",
    "            (\"dist_index_minus_mean\", 0),\n",
    "            (\"dist_index\", 0.5)]:\n",
    "            fig = sns.catplot(data=DFPROJ_INDEX_SCAL_CLEAN, x=\"an_da_ms\", y=yval, hue=\"assigned_base_simple\", \n",
    "                            row=\"assigned_label\", col=\"bregion\", alpha=0.25, jitter=True, aspect=1.5)\n",
    "            rotateLabel(fig, 90)\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(yzero, color=\"k\", alpha=0.2)\n",
    "            savefig(fig, f\"{savedir}/bimodal-y={yval}-1-{suff}.pdf\")\n",
    "\n",
    "            fig = sns.catplot(data=DFPROJ_INDEX_SCAL_CLEAN, x=\"an_da_ms\", y=yval, hue=\"assigned_base_simple\", \n",
    "                            row=\"assigned_label\", col=\"bregion\", kind=\"point\", errorbar=(\"ci\", 68), aspect=1.5)\n",
    "            rotateLabel(fig, 90)\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(yzero, color=\"k\", alpha=0.2)\n",
    "            savefig(fig, f\"{savedir}/bimodal-y={yval}-2-{suff}.pdf\")\n",
    "\n",
    "            fig = sns.catplot(data=DFPROJ_INDEX_SCAL_CLEAN, x=\"an_da_ms\", y=yval, hue=\"assigned_base_simple\", \n",
    "                            row=\"assigned_label\", col=\"bregion\", kind=\"violin\", bw_adjust=.5, cut=0, split=True, aspect=2)\n",
    "            rotateLabel(fig, 90)\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(yzero, color=\"k\", alpha=0.2)\n",
    "            savefig(fig, f\"{savedir}/bimodal-y={yval}-3-{suff}.pdf\")\n",
    "\n",
    "            ## Combine across morphsets\n",
    "            fig = sns.catplot(data=DFPROJ_INDEX_SCAL_CLEAN, x=\"bregion\", y=yval, hue=\"assigned_base_simple\", \n",
    "                        row=\"assigned_label\", col=\"animal\", kind=\"violin\", bw_adjust=.5, cut=0, split=True, aspect=2)\n",
    "            rotateLabel(fig, 90)\n",
    "            for ax in fig.axes.flatten():\n",
    "                ax.axhline(yzero, color=\"k\", alpha=0.2)\n",
    "            savefig(fig, f\"{savedir}/bimodal_combined-y={yval}-3-{suff}.pdf\")\n",
    "\n",
    "            plt.close(\"all\")\n",
    "\n",
    "        if False:\n",
    "            # Trying somthing -- scoring bimodality -- problem is that this doesnt take into account the\n",
    "            # label (e..,g, base1, base2) so is underpowered. Many cases by eye is clearly separated, but here doesnt detect it.\n",
    "            list_adm = sorted(df[\"an_da_ms\"].unique().tolist())\n",
    "            list_bregion = df[\"bregion\"].unique().tolist()\n",
    "            list_al = sorted(df[\"assigned_label\"].unique().tolist())\n",
    "\n",
    "            for adm in list_adm:\n",
    "                for bregion in list_bregion:\n",
    "                    for assigned_label in list_al:\n",
    "                        a = df[\"an_da_ms\"] == adm   \n",
    "                        b = df[\"bregion\"] == bregion   \n",
    "                        c = df[\"assigned_label\"] == assigned_label\n",
    "                        dfthis = df[a & b & c]\n",
    "                        if len(dfthis)>0:\n",
    "                            x = dfthis[\"dist_index\"]\n",
    "                            dip, pval = diptest.diptest(x)\n",
    "                            if pval<0.05:\n",
    "                                suff = \"*\"\n",
    "                            else:\n",
    "                                suff = \"\"\n",
    "                            print(suff, adm, bregion, assigned_label, \" --- \", dip, pval)\n",
    "\n",
    "\n",
    "\n",
    "        ### Scatterplot of ambig vs. (not_ambig)\n",
    "        from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "\n",
    "        for var1, var2 in [\n",
    "            (\"base\", \"ambig\"),\n",
    "            (\"not_ambig\", \"ambig\"),\n",
    "            (\"base\", \"not_ambig\")]:\n",
    "            for plot_text in [False, True]: \n",
    "\n",
    "                # Each animal.\n",
    "                list_animal= [\"Diego\", \"Pancho\"]\n",
    "                for animal in list_animal:\n",
    "                    \n",
    "                    dfthis = DFPROJ_INDEX_AGG_DIFF_SCAL[DFPROJ_INDEX_AGG_DIFF_SCAL[\"animal\"] == animal].reset_index(drop=True)\n",
    "\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(dfthis, \"label\", var1, var2, \"bregion\", \"dist_index_diff\", \"an_da_ms\", \n",
    "                                                                    plot_text=plot_text, shareaxes=True, SIZE=4);\n",
    "                    savefig(fig, f\"{savedir}/scatter-{var1}-{var2}-twind={suff}-text={plot_text}-an={animal}.pdf\")\n",
    "                plt.close(\"all\")\n",
    "\n",
    "                # Combined across animsl.\n",
    "                _, fig = plot_45scatter_means_flexible_grouping(DFPROJ_INDEX_AGG_DIFF_SCAL, \"label\", var1, var2, \"bregion\", \"dist_index_diff\", \"an_da_ms\", \n",
    "                                                                plot_text=plot_text, shareaxes=True, SIZE=4);\n",
    "                savefig(fig, f\"{savedir}/scatter-{var1}-{var2}-twind={suff}-text={plot_text}-combined.pdf\")\n",
    "                plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d79cd",
   "metadata": {},
   "source": [
    "### [Switching] quick plots of motor distance from base prims -- load PAredu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "proj_twind = (0.1, 1.2)\n",
    "for animal, date in LIST_EXPTS:\n",
    "    map_morphsetgood_to_indices = params_good_morphsets_switching(animal, date)\n",
    "    for bregion in LIST_REGIONS:\n",
    "        for morphset, inds_keep in map_morphsetgood_to_indices.items():     \n",
    "\n",
    "            if len(inds_keep)==0:\n",
    "                # hand label not inlcudes this morphset...\n",
    "                continue\n",
    "            \n",
    "            SAVEDIR = f\"/lemur2/lucas/analyses/recordings/main/decode_moment/PSYCHO_SP/{animal}-{date}-logistic-combine={combine}/switching_euclidian_score_and_plot/ev={event}-subtr={raw_subtract_mean_each_timepoint}-scal={scalar_or_traj}-dimred={dim_red_method}-twind={proj_twind}-npcs={NPCS_KEEP}/bregion={bregion}/morphset={morphset}\"\n",
    "            print(\"Loading from: \", SAVEDIR)\n",
    "\n",
    "            # Load data\n",
    "            with open(f\"{SAVEDIR}/PAredu.pkl\", \"rb\") as f:\n",
    "                PAredu = pickle.load(f)\n",
    "            sadsad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a89741",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"beh\"\n",
    "PAstroke = PAredu.behavior_replace_neural_with_strokes(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_effect = \"idxmorph_assigned\"\n",
    "effect_lev_base1 = \"0|base1\"\n",
    "effect_lev_base2 = \"99|base2\"\n",
    "list_grps_get = [\n",
    "    (\"0|base1\",),  \n",
    "    (\"99|base2\",)\n",
    "    ] # This is important, or else will fail if there are any (idx|assign) with only one datapt.\n",
    "plot_train_test_scores = True\n",
    "version = \"pts_time\"\n",
    "\n",
    "# twind_score_this = \n",
    "\n",
    "########################################\n",
    "##### (1) Plot over time\n",
    "### Get single trial pairwise distances over time.\n",
    "\n",
    "# (A) plot over time, including negetive times.\n",
    "version = \"pts_time\"    \n",
    "DFPROJ_INDEX, DFDIST, DFPROJ_INDEX_AGG, DFDIST_AGG, DFPROJ_INDEX_DIFFS = PAstroke.dataextract_as_distance_index_between_two_base_classes(var_effect, \n",
    "                                                                                    effect_lev_base1, effect_lev_base2, list_grps_get=list_grps_get,\n",
    "                                                                                    version = version)\n",
    "hue_order = [str(x) for x in sorted(DFPROJ_INDEX[var_effect].unique())]\n",
    "fig = sns.relplot(data=DFPROJ_INDEX, x=\"time_bin\", y=\"dist_index\", kind=\"line\", hue=var_effect, hue_order = hue_order, errorbar=(\"ci\", 68))\n",
    "# savefig(fig, f\"{savedir}/DFPROJ_INDEX-time.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd69c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflab = PAredu.Xlabels[\"trials\"]\n",
    "map_idxmorph_assigned_to_parts = {}\n",
    "for i, row in dflab.iterrows():\n",
    "    key = f\"{row['idx_morph_temp']}|{row['assigned_base_simple']}\"\n",
    "    val = (row['idx_morph_temp'], row['assigned_base_simple'])\n",
    "    if key in map_idxmorph_assigned_to_parts:\n",
    "        assert val == map_idxmorph_assigned_to_parts[key]\n",
    "    else:\n",
    "        map_idxmorph_assigned_to_parts[key] = val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)\n",
    "print(map_idxmorph_assigned_to_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1000b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPROJ_INDEX[\"idx_morph_temp\"] = [map_idxmorph_assigned_to_parts[x][0] for x in DFPROJ_INDEX[\"idxmorph_assigned\"]]\n",
    "DFPROJ_INDEX[\"assigned_base_simple\"] = [map_idxmorph_assigned_to_parts[x][1] for x in DFPROJ_INDEX[\"idxmorph_assigned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFPROJ_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae11f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAredu.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmonkey.scripts.analy_decode_moment_psychometric import analy_extract_PA_conditioned\n",
    "# scalar_or_traj = \"traj\"\n",
    "tbin_dur = 0.1\n",
    "pca_tbin_slice = 0.1\n",
    "proj_twind = (-0.1, 1.2)\n",
    "# restricted_twind_for_dpca = (0.1, 1.2)\n",
    "restricted_twind_for_dpca = None\n",
    "\n",
    "\n",
    "raw_subtract_mean_each_timepoint = False\n",
    "EVENT = \"03_samp\"\n",
    "dim_red_method = \"dpca\"\n",
    "NPCS_KEEP = 7\n",
    "    \n",
    "savedirpca = None\n",
    "\n",
    "superv_dpca_var = \"idx_morph_temp\"\n",
    "superv_dpca_vars_group = None\n",
    "\n",
    "scalar_or_traj = \"traj\"\n",
    "Xredu, _ = PAredu.dataextract_dimred_wrapper(scalar_or_traj, dim_red_method, savedirpca, \n",
    "        restricted_twind_for_dpca, tbin_dur, pca_tbin_slice, NPCS_KEEP = NPCS_KEEP,\n",
    "        dpca_var = superv_dpca_var, dpca_vars_group = superv_dpca_vars_group, dpca_proj_twind = proj_twind, \n",
    "        raw_subtract_mean_each_timepoint=raw_subtract_mean_each_timepoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonlib.tools.snstools import rotateLabel\n",
    "\n",
    "# (B) Average over time, post-samp\n",
    "order = sorted(DFPROJ_INDEX[var_effect].unique())\n",
    "fig = sns.catplot(data=DFPROJ_INDEX, x=var_effect, y=\"dist_index\", kind=\"point\", errorbar=(\"ci\", 68),\n",
    "                  order=order)\n",
    "rotateLabel(fig)\n",
    "\n",
    "order = sorted(DFPROJ_INDEX[\"idx_morph_temp\"].unique())\n",
    "fig = sns.catplot(data=DFPROJ_INDEX, x=\"idx_morph_temp\", y=\"dist_index_norm\", hue=\"assigned_base_simple\", \n",
    "                  kind=\"point\", errorbar=(\"ci\", 68),\n",
    "                  order=order)\n",
    "rotateLabel(fig)\n",
    "\n",
    "\n",
    "order = sorted(DFPROJ_INDEX[\"idx_morph_temp\"].unique())\n",
    "fig = sns.catplot(data=DFPROJ_INDEX, x=\"idx_morph_temp\", y=\"dist_index_norm\", hue=\"assigned_base_simple\", \n",
    "                  jitter=True, alpha=0.1,order=order)\n",
    "rotateLabel(fig)\n",
    "\n",
    "# savefig(fig, f\"{savedir}/DFPROJ_INDEX-scal-twind={twind_score_this}.pdf\")\n",
    "\n",
    "# fig = sns.catplot(data=DFPROJ_INDEX_DIFFS, x=var_effect, y=\"dist\", kind=\"point\", errorbar=(\"ci\", 68))\n",
    "# # savefig(fig, f\"{savedir}/DFPROJ_INDEX_DIFFS-scal-twind={twind_score_this}.pdf\")\n",
    "\n",
    "# plt.close(\"all\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359d72f",
   "metadata": {},
   "source": [
    "# ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2d02b",
   "metadata": {},
   "source": [
    "# OLDER STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7825482",
   "metadata": {},
   "source": [
    "### Plot single trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING - Plot an example training trial\n",
    "\n",
    "indtrial = 13\n",
    "dflab = PAtrain.Xlabels['trials']\n",
    "print(f\"For this trial {indtrial}, {Dc.VarDecode} = {dflab.iloc[indtrial][Dc.VarDecode]}\")\n",
    "Dc.plot_single_trial(indtrial, PA=PAtrain);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, extract PA\n",
    "\n",
    "idx_in_morphset = 4\n",
    "trialcodes = find_morphset_morphidx(DSmorphsets, morphset, idx_in_morphset)\n",
    "\n",
    "\n",
    "[(i, \" --- \", map_tcmorphset_to_info[(tc, morphset)]) for i, tc in enumerate(trialcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "tc = trialcodes[13]\n",
    "\n",
    "dflab = PAtest.Xlabels[\"trials\"]\n",
    "\n",
    "indtrial = dflab[dflab[\"trialcode\"] == tc].index.tolist()[0]\n",
    "\n",
    "print(\"Trialcode: \", tc, \", idx=\", indtrial)\n",
    "print(f\"For this trial {indtrial}, {Dc.VarDecode} = {dflab.iloc[indtrial][Dc.VarDecode]}\")\n",
    "\n",
    "# What was actually drawn on this trial?\n",
    "print(\"What was actually drawn on this trial?\", map_tcmorphset_to_info[(tc, morphset)])\n",
    "\n",
    "tbin_dur=0.15\n",
    "tbin_slide=0.01\n",
    "if False:\n",
    "    Dc.plot_single_trial(indtrial, PA=PAtest, tbin_dur=tbin_dur, tbin_slide=tbin_slide);\n",
    "else:\n",
    "    # Hack, testing out if including null helps\n",
    "    Dc.plot_single_trial(indtrial, PA=PAtest, tbin_dur=tbin_dur, tbin_slide=tbin_slide);\n",
    "    DcNoNull.plot_single_trial(indtrial, PA=PAtest, tbin_dur=tbin_dur, tbin_slide=tbin_slide);\n",
    "    DcNull.plot_single_trial(indtrial, PA=PAtest, tbin_dur=tbin_dur, tbin_slide=tbin_slide);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e760d4",
   "metadata": {},
   "source": [
    "# Timecourse plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2d215",
   "metadata": {},
   "source": [
    "### Useing single case for the one trinaed on [B1 M B2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee94edf",
   "metadata": {},
   "source": [
    "##### A single decode train-test split (i.e., is not good, not testing on held out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b56aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscores_testsplit, dfscores_usertest, decoders, trainsets, PAtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eaf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_trainsplit = 0\n",
    "Dc = decoders[idx_trainsplit]\n",
    "PAtrain_orig = trainsets[idx_trainsplit][\"PAtrain_orig\"]\n",
    "test_index_PAtrain_orig = trainsets[idx_trainsplit][\"test_index_PAtrain_orig\"]\n",
    "\n",
    "# Slice out the test dataset\n",
    "if False:\n",
    "    patest = PAtrain_orig.slice_by_dim_indices_wrapper(\"trials\", test_index_PAtrain_orig, reset_trial_indices=True)\n",
    "else:\n",
    "    patest = PAtrain_orig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd68e6",
   "metadata": {},
   "source": [
    "##### [Good] loop thru each train-test split, and collect data from test trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1589140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the same decoder idx, overlay means from diff trials\n",
    "\n",
    "assert False, \"in progress -- need to color each line differntly..\"\n",
    "\n",
    "decoder_idx = 0\n",
    "list_inds = [[0,1,2], [3,4,5]]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for indstrials in list_inds:\n",
    "\n",
    "    probs_mat_all_this = probs_mat_all[indstrials, :, decoder_idx][:, :, None]\n",
    "    Dc._timeseries_plot_by_shape_drawn_order(probs_mat_all_this, times, [\"test\"], MAP_INDEX_TO_COL, ax)\n",
    "    # ax.set_title(f\"{idx_within}, {assigned}\")\n",
    "    # ax.set_title(f\"{idx_within}, {_assigned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfres = pd.DataFrame(res)\n",
    "dfres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: is (base1 decode during ambig_base2) higher than (base1 decode during base2)\n",
    "\n",
    "decoder_class = 99\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _extract_probs_mat(dfres, labels, idx_within, _assigned, decoder_class):\n",
    "    tmp = dfres[(dfres[\"idx_within\"] == idx_within) & (dfres[\"_assigned\"] == _assigned)]\n",
    "    assert len(tmp)==1\n",
    "    probs_mat_all_this = tmp[\"probs_mat_all_this\"].values[0] # (trials, times, labels)\n",
    "    idx_decode = labels.index(decoder_class)\n",
    "    probs_mat = probs_mat_all_this[:, :, idx_decode]\n",
    "    return probs_mat\n",
    "\n",
    "idx_within = 3\n",
    "_assigned = \"ambig_base1\"\n",
    "probs_mat_1 = _extract_probs_mat(dfres, labels, idx_within, _assigned, decoder_class)\n",
    "\n",
    "idx_within = 0\n",
    "_assigned = \"base1\"\n",
    "probs_mat_2 = _extract_probs_mat(dfres, labels, idx_within, _assigned, decoder_class)\n",
    "\n",
    "probs_vec_1 = np.mean(probs_mat_1, axis=0)\n",
    "probs_vec_2 = np.mean(probs_mat_2, axis=0)\n",
    "\n",
    "# take diff\n",
    "probs_vec_diff = probs_vec_1 - probs_vec_2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(times, probs_vec_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract baseline trials (when the)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f85574",
   "metadata": {},
   "source": [
    "### METHOD -- trial by trial scatter plots [including KDE plots...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    indtrials = list(range(20))\n",
    "elif False:\n",
    "    indtrials = PAtest.Xlabels[\"trials\"][PAtest.Xlabels[\"trials\"][Dc.VarDecode]==\"0|99\"].index\n",
    "else:\n",
    "    # First, extract PA\n",
    "    morphset = 3\n",
    "    idx_in_morphset = 2\n",
    "    trialcodes = find_morphset_morphidx(DSmorphsets, morphset, idx_in_morphset)\n",
    "    display([(i, \" --- \", map_tcmorphset_to_info[(tc, morphset)]) for i, tc in enumerate(trialcodes)])\n",
    "\n",
    "    indtrials = PAtest.Xlabels[\"trials\"][PAtest.Xlabels[\"trials\"][\"trialcode\"].isin(trialcodes)].index.tolist()\n",
    "    indtrials\n",
    "\n",
    "print(len(indtrials))\n",
    "twind = (0.1, 1.8)\n",
    "PAprobs, probs_mat_all, times, labels = Dc.timeseries_score_wrapper(PAtest, twind, indtrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x1 = probs_mat_all[0, :, 0]\n",
    "x2 = probs_mat_all[0, :, 1]\n",
    "\n",
    "def _plot_pairwise(xvals, yvals, ax):\n",
    "\n",
    "    # ax.plot(x1, x2, \"-x\", alpha=0.5)\n",
    "    # sns.scatterplot(x=xvals, y=yvals, ax=ax, color=\"k\", alpha=0.02)\n",
    "    \n",
    "    sns.kdeplot(x=xvals, y=yvals, ax=ax, fill=True, thresh=0, levels=50, cmap=\"mako\")\n",
    "    \n",
    "    # This is the worst\n",
    "    # sns.histplot(x=xvals, y=yvals, ax=ax, bins=20, stat=\"probability\", cbar=True, alpha=0.5)\n",
    "    # sns.rugplot(x=xvals, y=yvals, ax=ax, color=\"k\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x1 = probs_mat_all[0, :, 0]\n",
    "x2 = probs_mat_all[0, :, 1]\n",
    "_plot_pairwise(x1, x2, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot, comparing each pair of labels\n",
    "\n",
    "ncols = len(labels)\n",
    "nrows = len(labels)\n",
    "SIZE = 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(nrows*SIZE, ncols*SIZE))\n",
    "\n",
    "for i, lab1 in enumerate(labels): # rows\n",
    "    for j, lab2 in enumerate(labels): # cols\n",
    "        if j!=i:\n",
    "            ax = axes[i][j]\n",
    "\n",
    "            xvals = probs_mat[:, j]\n",
    "            yvals = probs_mat[:, i]\n",
    "            # ax.set_title((i, j))\n",
    "            # ax.set_title((i, j))\n",
    "            _plot_pairwise(xvals, yvals, ax)\n",
    "            ax.set_xlabel(lab2)\n",
    "            ax.set_ylabel(lab1)\n",
    "\n",
    "            # assert False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drag2_matlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
