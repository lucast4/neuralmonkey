{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\"\n",
    "Devo code for all kinds of decoding analyses, inclduing:\n",
    "[NOTE: all are time-resolved]\n",
    "- default\n",
    "- cross-generalization (at each time point)\n",
    "- generalization over time\n",
    "--- variation: single-trial covert.\n",
    "- shared subspace (single decoder over time)\n",
    "\n",
    "Inspired to do this mainly re: quyestion of evidence for single trial shape decoding during palnning, espeically for chars.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e3fd17c4f13ba43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting a DFallpa [for Lucas. Xuan can ignore]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b19797efdaef85a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "animal = \"Diego\"\n",
    "date = 230615\n",
    "question = \"SP_shape_loc\"\n",
    "list_time_windows = [(-0.4, 0.6)]\n",
    "events_keep = [\"03_samp\", \"06_on_strokeidx_0\"]\n",
    "combine_into_larger_areas = False\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                events_keep = events_keep,\n",
    "                                HACK_RENAME_SHAPES=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "703578e54f80fbe2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load a question\n",
    "from neuralmonkey.analyses.rsa import rsagood_questions_dict\n",
    "DictParamsEachQuestion = rsagood_questions_dict(animal, date)\n",
    "q_params = DictParamsEachQuestion[question]\n",
    "print(q_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f16c766a83258bf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize, etc\n",
    "# Clean up DFallpa\n",
    "from neuralmonkey.analyses.rsa import preprocess_rsa_prepare_popanal_wrapper, popanal_preprocess_scalar_normalization\n",
    "\n",
    "subtract_mean_each_level_of_var = None\n",
    "plot_example_chan = None\n",
    "\n",
    "list_pa =[]\n",
    "list_panorm = []\n",
    "for pa in DFallpa[\"pa\"].tolist():\n",
    "    print(pa.X.shape)\n",
    "    pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "    print(pa.X.shape)\n",
    "\n",
    "    panorm, _, _, _, _, _ = popanal_preprocess_scalar_normalization(pa, q_params[\"effect_vars\"],\n",
    "                                                                                  subtract_mean_each_level_of_var)\n",
    "    \n",
    "    list_pa.append(pa)\n",
    "    list_panorm.append(panorm)\n",
    "    \n",
    "DFallpa[\"pa\"] = list_pa\n",
    "DFallpa[\"pa_norm\"] = list_panorm\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b237f105e939e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save it\n",
    "import pickle\n",
    "path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa.pkl\"\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(DFallpa, f)\n",
    "print(\"Saved to:\", path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f94451b0831398"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a pre-computed Snippets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e64d1e318d5757d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "animal = \"Diego\"\n",
    "date = 230628\n",
    "# question = \"SP_shape_loc\"\n",
    "question = \"PIG_planning_shape_loc\"\n",
    "list_time_windows = [(-0.6, 0.6)]\n",
    "events_keep = [\"03_samp\", \"04_go_cue\", \"06_on_strokeidx_0\"]\n",
    "combine_into_larger_areas = True\n",
    "HACK_RENAME_SHAPES = False\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                events_keep = events_keep,\n",
    "                                HACK_RENAME_SHAPES=HACK_RENAME_SHAPES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b6e8425ba4d829d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### To concatenate trial and stroke datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8005e8005ba26c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper_combine_trial_strokes\n",
    "animal = \"Pancho\"\n",
    "date = 230623\n",
    "# animal = \"Diego\"\n",
    "# date = 230630\n",
    "# question = \"SP_shape_loc\"\n",
    "# question = \"PIG_planning_shape_loc\"\n",
    "list_time_windows = [(-0.6, 0.6)]\n",
    "combine_into_larger_areas = True\n",
    "HACK_RENAME_SHAPES = False\n",
    "exclude_bad_areas = True\n",
    "\n",
    "question_trial = \"PIG_BASE_trial\"\n",
    "question_stroke = \"PIG_BASE_stroke\"\n",
    "DFallpa = dfallpa_extraction_load_wrapper_combine_trial_strokes(animal, date, question_trial,\n",
    "                                                                   question_stroke,\n",
    "                                            list_time_windows,\n",
    "                                           combine_into_larger_areas = combine_into_larger_areas,\n",
    "                                           exclude_bad_areas=exclude_bad_areas,\n",
    "                                            SPIKES_VERSION=\"tdt\",\n",
    "                                            HACK_RENAME_SHAPES = HACK_RENAME_SHAPES,\n",
    "                                           do_fr_normalization=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "658f5e69291dca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TO save "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7611aedc7d232185"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save it\n",
    "import pickle\n",
    "path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_char_concat_trial_and_stroke_Pancho_230126.pkl\"\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(DFallpa, f)\n",
    "print(\"Saved to:\", path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e57fd663336e808"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a dataset (saved for Xuan)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f471c575413735b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load and plot a dataset of neural activity across population, in a PopAnal class object.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401a9851a443292c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d72addf5c1494c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# this is the path to the dataset\n",
    "# path = '/gorilla1/analyses/recordings/main/RSA/Diego-230615/agg_True-subtr_None-dist_euclidian_unbiased/SP_shape_loc/DFallpa.pkl'\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_samp_and_stroke.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_pig_planning.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_pig_concat_trial_and_stroke_which_levels.pkl\"\n",
    "path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_char_concat_trial_and_stroke_Pancho_230126.pkl\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "320be7d5ecbddf5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa = pd.read_pickle(path)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "335e4da034da5b2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a DFallpa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122e1e23ec6b587"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Char, \n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "# do_combine = True\n",
    "\n",
    "# Single prim, novels\n",
    "animal = \"Pancho\"\n",
    "date = 221220\n",
    "do_combine = False\n",
    "\n",
    "if do_combine:\n",
    "    # COMBINE trial and stroke\n",
    "    dir_suffix = \"test\"\n",
    "    question = None\n",
    "    # q_params = None\n",
    "    which_level = None\n",
    "    q_params = {\n",
    "        \"effect_vars\": [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "    }\n",
    "    \n",
    "    combine_trial_and_stroke = True\n",
    "    \n",
    "    # PIG\n",
    "    # question_trial = \"PIG_BASE_trial\"\n",
    "    # question_stroke = \"PIG_BASE_stroke\"\n",
    "    # check_that_locs_match = True\n",
    "    \n",
    "    # CHAR\n",
    "    question_trial = \"CHAR_BASE_trial\"\n",
    "    question_stroke = \"CHAR_BASE_stroke\"\n",
    "    check_that_locs_match = True\n",
    "    check_that_shapes_match = True\n",
    "else:\n",
    "    # DONT COMBINE, use questions.\n",
    "    # question = \"CHAR_BASE_trial\"\n",
    "    question = \"SP_shape_loc\"\n",
    "    combine_trial_and_stroke = False\n",
    "    which_level = \"trial\" # Doesnt matter\n",
    "    dir_suffix = question\n",
    "\n",
    "    # Load q_params\n",
    "    from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "    q_params = rsagood_questions_dict(animal, date, question)[question]\n",
    "\n",
    "############### PARAMS\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "combine_into_larger_areas = False\n",
    "HACK_RENAME_SHAPES = False\n",
    "list_time_windows = [(-0.6, 0.6)]\n",
    "\n",
    "########################################## RUN\n",
    "\n",
    "if combine_trial_and_stroke:\n",
    "    from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper_combine_trial_strokes\n",
    "    DFallpa = dfallpa_extraction_load_wrapper_combine_trial_strokes(animal, date, question_trial,\n",
    "                                                                       question_stroke,\n",
    "                                                list_time_windows,\n",
    "                                               combine_into_larger_areas = combine_into_larger_areas,\n",
    "                                               exclude_bad_areas=exclude_bad_areas,\n",
    "                                                SPIKES_VERSION=\"tdt\",\n",
    "                                                HACK_RENAME_SHAPES = HACK_RENAME_SHAPES,\n",
    "                                               do_fr_normalization=True,\n",
    "                                                    check_that_shapes_match=check_that_shapes_match,\n",
    "                                                check_that_locs_match=check_that_locs_match)\n",
    "else:\n",
    "    from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "    DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                              which_level=which_level,\n",
    "                                              combine_into_larger_areas = combine_into_larger_areas,\n",
    "                                              exclude_bad_areas = exclude_bad_areas,\n",
    "                                              SPIKES_VERSION = SPIKES_VERSION,\n",
    "                                              HACK_RENAME_SHAPES = HACK_RENAME_SHAPES,\n",
    "                                              do_fr_normalization=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58ea6f5073b3909f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### checking that trial and stroke level data are aligned in shape and loc sequences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4495aefc89b698e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import slice_by_row_label\n",
    "import numpy as np\n",
    "pa = DFallpa[DFallpa[\"event\"]==\"03_samp\"][\"pa\"].values[0]\n",
    "dflab_trial = pa.Xlabels[\"trials\"]\n",
    "\n",
    "\n",
    "for i, row in DFallpa.iterrows():\n",
    "    pathis = row[\"pa\"]\n",
    "    if \"06_on_STK_\" in row[\"event\"]:\n",
    "        dflab_stroke_this = pathis.Xlabels[\"trials\"]\n",
    "        \n",
    "        tmp = dflab_stroke_this[\"stroke_index\"].unique()\n",
    "        assert len(tmp)==1\n",
    "        si = tmp[0]\n",
    "        tcs = dflab_stroke_this[\"trialcode\"].tolist()\n",
    "    \n",
    "    \n",
    "        shapes_in_dflab_all = slice_by_row_label(dflab_trial, \"trialcode\", tcs, assert_exactly_one_each=True)[f\"seqc_{si}_shape\"]\n",
    "        shapes_in_dflab_this = dflab_stroke_this[\"shape_oriented\"]\n",
    "        shapes_in_dflab_this_2 = dflab_stroke_this[f\"seqc_{si}_shape\"]\n",
    "        if not np.all(shapes_in_dflab_all==shapes_in_dflab_this):\n",
    "            for sh1, sh2 in zip(shapes_in_dflab_all, shapes_in_dflab_this):\n",
    "                print(sh1, sh2)\n",
    "            assert False, \"probably a stroke was skipped before etraction to DS, therefore it skips an index...\"\n",
    "        if not np.all(shapes_in_dflab_all==shapes_in_dflab_this_2):\n",
    "            for sh1, sh2 in zip(shapes_in_dflab_all, shapes_in_dflab_this_2):\n",
    "                print(sh1, sh2)\n",
    "            assert False, \"probably a stroke was skipped before etraction to DS, therefore it skips an index...\"\n",
    "        \n",
    "        shapes_in_dflab_all = slice_by_row_label(dflab_trial, \"trialcode\", tcs, assert_exactly_one_each=True)[f\"seqc_{si}_loc\"]\n",
    "        shapes_in_dflab_this = dflab_stroke_this[\"gridloc\"]\n",
    "        shapes_in_dflab_this_2 = dflab_stroke_this[f\"seqc_{si}_loc\"]\n",
    "        assert np.all(shapes_in_dflab_all==shapes_in_dflab_this), \"probably a stroke was skipped before etraction to DS, therefore it skips an index...\"\n",
    "        assert np.all(shapes_in_dflab_all==shapes_in_dflab_this_2), \"probably a stroke was skipped before etraction to DS, therefore it skips an index...\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "313419c4fdc14f0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Preprocessing for \"novel prims\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a3689baf0bab97c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pa = DFallpa[\"pa\"].values[0]\n",
    "pa.Xlabels[\"trials\"][:5]\n",
    "pa.Xlabels[\"trials\"][\"shape_is_novel_all\"]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2366de64b0a76674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Preprocessing for char"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b1b72d90275f92c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 1. got shape labels for char\n",
    "pa = DFallpa[\"pa\"].values[0]\n",
    "pa.Xlabels[\"trials\"].columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb74e4ddd9a0be0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dflab = pa.Xlabels[\"trials\"]\n",
    "dflab[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959190bdc0e273bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace seqc context with char labels\n",
    "\n",
    "for i, row in dflab.iterrows():\n",
    "    row[\"charclust_shape_seq\"]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "265236b64df55c45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 2. prune cases of low score for char?\n",
    "# 3. Check if have single prims tasks? Can use this to train decoder for shape\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32997fee8e5492cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Also get location bins for char"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4260fa3c7ccac5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d746b1f557913998"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code example for benchmarking: decoding shapes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93edebd6e38ce91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This step takes in a representation of neural data and outputs a scalar score for how well you can decode \"shape\" from that data\n",
    "\n",
    "Here, this example is using the raw data (dimensionality = number of channels). The goal is to use methods to reduce the dimensionality of this data, each time running through this decoding benchmark, to compare the different methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "488ff1ca66a8098f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### First, pull out a specific PA. (just an example)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c025e2bfc54ebe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE: tjhis is just for demonstration. Eventually you will want to loop thru all PA, scoring them all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bb74aab4a961bd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10bc2b4059ed05b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make sure to normalize PA before running any modeling on it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "583a1e729ac08b30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_panorm = []\n",
    "for pa in DFallpa[\"pa\"].tolist():\n",
    "    from neuralmonkey.analyses.state_space_good import popanal_preprocess_scalar_normalization\n",
    "    PAnorm, PAscal, PAscalagg, fig, axes, groupdict = popanal_preprocess_scalar_normalization(pa, None, DO_AGG_TRIALS=False)\n",
    "    list_panorm.append(PAnorm)\n",
    "DFallpa[\"pa\"] = list_panorm\n",
    "\n",
    "# del DFallpa[\"pa_norm\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854db14b65f6c377"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "SAVEDIR_ANALYSIS = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/DECODE\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cab933058cb251a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Figure out how long is seuqence\n",
    "n_strokes_max = -1\n",
    "for i in range(2):\n",
    "    n_ignore = sum(PAnorm.Xlabels[\"trials\"][f\"seqc_{i}_shape\"]==\"IGNORE\")\n",
    "    n_total = len(PAnorm.Xlabels[\"trials\"][f\"seqc_{i}_shape\"])\n",
    "    print(n_ignore, n_total)\n",
    "    if n_ignore<n_total:\n",
    "        n_strokes_max=i+1\n",
    "assert n_strokes_max>0\n",
    "print(n_strokes_max)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a2fa80f4fc611b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PREPROCESS - factorize all relevant labels FIRST here.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8634a855e2b24c5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import preprocess_factorize_class_labels_ints\n",
    "MAP_LABELS_TO_INT = preprocess_factorize_class_labels_ints(DFallpa)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87e0da3ce5099d79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Novel prims, give fake \"task_kind\" as novel or not"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1f6d6530d4ec9ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This takes advantage of code already written for splitting analyses by task-kind\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c59d5b3c36922ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfd56bfb82a068ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8dce0ad0e1df944b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "list_panorm = []\n",
    "for pa in DFallpa[\"pa\"].tolist():\n",
    "    if \"task_kind_orig\" not in pa.Xlabels[\"trials\"].columns:\n",
    "        pa.Xlabels[\"trials\"][\"task_kind_orig\"] = pa.Xlabels[\"trials\"][\"task_kind\"]\n",
    "        pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"task_kind_orig\", \"shape_is_novel_all\"], \"task_kind\")\n",
    "    else:\n",
    "        pa.Xlabels[\"trials\"] = append_col_with_grp_index(pa.Xlabels[\"trials\"], [\"task_kind_orig\", \"shape_is_novel_all\"], \"task_kind\", use_strings=True, strings_compact=True)\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11cc55e35315c8ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pa.Xlabels[\"trials\"][\"shape_is_novel_all\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b9e9fbd116857a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAP_LABELS_TO_INT[\"shape\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69a04eebd6a0178"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Default: Time-resolved decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36745f908f0c7fd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR_ANALYSIS = \"/tmp\"\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94465b7df045064f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pa.X.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a4bd130b3df5d5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of task kinds\n",
    "pa = DFallpa[\"pa\"].values[0]\n",
    "pa.Xlabels[\"trials\"][\"task_kind\"].value_counts()\n",
    "\n",
    "LIST_TASK_KIND = pa.Xlabels[\"trials\"][\"task_kind\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa62eee84c10ed1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LIST_TASK_KIND"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f773164260f96bc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_strokes_max"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7307790e766395e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "807f5f5fe69f3af1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/1_time_resolved\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "n_min_trials = 6\n",
    "\n",
    "from neuralmonkey.utils.frmat import bin_frmat_in_time\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_timeresolved_singlevar, decodewrapouterloop_categorical_timeresolved\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import pandas as pd\n",
    "RES = []\n",
    "\n",
    "# list_vars_decode = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "# list_vars_decode = [\"seqc_0_shape\"]\n",
    "list_vars_decode = [\"shape_this_event\"]\n",
    "# list_vars_decode = [\"seqc_2_shape\"]\n",
    "\n",
    "# list_vars_decode = [\"shape_is_novel_all\"]\n",
    "\n",
    "time_bin_size = 0.2\n",
    "slide = 0.2\n",
    "max_nsplits = 2\n",
    "\n",
    "DFRES = decodewrapouterloop_categorical_timeresolved(DFallpa, list_vars_decode, SAVEDIR, time_bin_size, slide, n_min_trials,\n",
    "                                                     max_nsplits=max_nsplits)\n",
    "# \n",
    "# for i, row in DFallpa.iterrows():\n",
    "#     br = row[\"bregion\"]\n",
    "#     tw = row[\"twind\"]\n",
    "#     ev = row[\"event\"]\n",
    "#     PA = row[\"pa\"]\n",
    "#     \n",
    "#     for task_kind in LIST_TASK_KIND:\n",
    "#         pa = PA.slice_by_labels(\"trials\", \"task_kind\", [task_kind])\n",
    "#     \n",
    "#         # 2. Extract X from pa\n",
    "#         X = pa.X # (nchans, ntrials, ntimes)\n",
    "#         times = pa.Times\n",
    "#         dflab = pa.Xlabels[\"trials\"]\n",
    "#     \n",
    "#         \n",
    "#         for var_decode in list_vars_decode:\n",
    "#             print(br, ev, var_decode)\n",
    "#             \n",
    "#             # Prune dflab\n",
    "#             from pythonlib.tools.pandastools import filter_by_min_n\n",
    "#             dftmp = filter_by_min_n(dflab, var_decode, n_min_trials)\n",
    "#             \n",
    "#             if len(dftmp)>0:\n",
    "#                 indskeep = dftmp[\"_index\"].tolist()\n",
    "#                 Xthis = X[:, indskeep, :]\n",
    "#                 dflab_this = dflab.iloc[indskeep]\n",
    "#                 \n",
    "#                 if len(dflab_this[var_decode].unique())==1:\n",
    "#                     print(\"SKIPPING, becuase only one label:\")\n",
    "#                     print(dflab_this[var_decode].unique())\n",
    "#                     continue\n",
    "#         \n",
    "#                 if len(Xthis)>0:\n",
    "#                     res = decodewrap_categorical_timeresolved_singlevar(Xthis, times, dflab_this, [var_decode],\n",
    "#                                                   time_bin_size=time_bin_size, slide=slide, max_nsplits=max_nsplits)\n",
    "#                     for r in res:\n",
    "#                         r[\"event\"]=ev\n",
    "#                         r[\"bregion\"]=br\n",
    "#                         r[\"twind\"]=tw\n",
    "#                         r[\"var_decode\"]=var_decode\n",
    "#                         r[\"task_kind\"] = task_kind\n",
    "#                     RES.extend(res)\n",
    "#                 \n",
    "# DFRES = pd.DataFrame(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a276f76dd803cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22be7dac35ffb9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6256fa201420c7ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Cross-condition decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98ec3d6e76bf4ebb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each time bin, decode shape genearlizing across location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4237c1dd8d95bce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decode_train_model, decode_categorical_cross_condition,decodewrap_categorical_timeresolved_cross_condition\n",
    "\n",
    "list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "list_tw = DFallpa[\"twind\"].unique().tolist()\n",
    "list_ev = DFallpa[\"event\"].unique().tolist()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f752951c3d6f1846"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = \"/tmp\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78e6a7b2d10704a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_timeresolved_cross_condition, decodewrapouterloop_categorical_timeresolved_cross_condition\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/2_cross_decode\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "# PARAMS\n",
    "subtract_mean_vars_conj = True # WHether to normalize by sutbracting mean within each level of othervar...\n",
    "\n",
    "# list_var_decode = [\"shape_this_event\", \"loc_this_event\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"loc_this_event\"],\n",
    "#     [\"shape_this_event\"]]\n",
    "\n",
    "list_var_decode = [\"shape_this_event\"]\n",
    "list_vars_conj = [\n",
    "    [\"loc_this_event\"]]\n",
    "\n",
    "list_var_decode = [\"loc_this_event\"]\n",
    "list_vars_conj = [\n",
    "    [\"shape_this_event\"]]\n",
    "\n",
    "DFRES = decodewrapouterloop_categorical_timeresolved_cross_condition(DFallpa, list_var_decode,\n",
    "                                                                 list_vars_conj,\n",
    "                                                 SAVEDIR, time_bin_size=0.1, slide=0.05)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea62377c46cf112b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In progress: sequence context"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23cfb3198e6c30f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_timeresolved_cross_condition\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/2_cross_decode\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "# PARAMS\n",
    "subtract_mean_vars_conj = False # WHether to normalize by sutbracting mean within each level of othervar...\n",
    "\n",
    "# list_var_decode = [\"shape_this_event\", \"loc_this_event\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"loc_this_event\"],\n",
    "#     [\"shape_this_event\"]]\n",
    "\n",
    "### SEQUENCE CONTEXT\n",
    "# list_var_decode = [\"stroke_index\", \"shape_this_event\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"shape_this_event\", \"loc_this_event\"],\n",
    "#     [\"stroke_index\", \"loc_this_event\"],\n",
    "#     ]\n",
    "# SEQ_CONTEXT_MODE = \"seq_ctxt\"\n",
    "\n",
    "### SEQUENCE PREDICTION\n",
    "# list_var_decode = [\"CTXT_shape_next\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"shape_this_event\", \"loc_this_event\", \"CTXT_loc_next\"],\n",
    "#     ]\n",
    "# SEQ_CONTEXT_MODE = \"seq_pred\"\n",
    "\n",
    "list_var_decode = [\"CTXT_loc_next\"]\n",
    "list_vars_conj = [\n",
    "    [\"shape_this_event\", \"loc_this_event\", \"CTXT_shape_next\", \"CTXT_loc_prev\"],\n",
    "    ]\n",
    "SEQ_CONTEXT_MODE = \"seq_pred\"\n",
    "\n",
    "# RUns\n",
    "time_bin_size = 0.1\n",
    "slide = 0.05\n",
    "RES = []\n",
    "for i, row in DFallpa.iterrows():\n",
    "    br = row[\"bregion\"]\n",
    "    tw = row[\"twind\"]\n",
    "    ev = row[\"event\"]\n",
    "    pa = row[\"pa\"]\n",
    "                \n",
    "    for var_decode, vars_conj_condition in zip(list_var_decode, list_vars_conj):\n",
    "        print(ev, br, tw, var_decode)\n",
    "\n",
    "        plot_counts_heatmap_savepath = f\"/tmp/{br}-{var_decode}.png\"\n",
    "        \n",
    "        if SEQ_CONTEXT_MODE is not None:\n",
    "            # Clean up, to have clean analyses related to sequence\n",
    "            from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "            from neuralmonkey.analyses.rsa import preprocess_prune_pa_enough_data, preprocess_rsa_prepare_popanal_wrapper\n",
    "            \n",
    "            q_params = rsagood_questions_dict(animal, date, SEQ_CONTEXT_MODE)[SEQ_CONTEXT_MODE]\n",
    "            # q_params[\"effect_vars\"] = [\"shape_this_event\", \"loc_this_event\", \"stroke_index_fromlast_tskstks\"]         \n",
    "            # q_params[\"effect_vars\"] = [\"shape_this_event\", \"loc_this_event\", \"stroke_index\"]         \n",
    "            q_params[\"effect_vars\"] = [var_decode] + vars_conj_condition         \n",
    "            \n",
    "            # q_params[\"exclude_first_stroke\"] = False\n",
    "            pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "\n",
    "        # 1. Extract the specific pa for this (br, tw)\n",
    "        res = decodewrap_categorical_timeresolved_cross_condition(pa, var_decode,\n",
    "                                                    vars_conj_condition,\n",
    "                                                    subtract_mean_vars_conj=subtract_mean_vars_conj,\n",
    "                                                    time_bin_size=time_bin_size, slide=slide,\n",
    "                                                    do_center=True, do_std=False,\n",
    "                                                    plot_counts_heatmap_savepath=plot_counts_heatmap_savepath)\n",
    "        \n",
    "        plt.close(\"all\")\n",
    "        if res is not None:\n",
    "            for r in res:\n",
    "                r[\"bregion\"]=br\n",
    "                r[\"twind\"]=tw\n",
    "                r[\"event\"]=ev\n",
    "            RES.extend(res)\n",
    "DFRES = pd.DataFrame(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "312d277ef20d532c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2b) Separate decoder for each level of other var (then take average over decoders). Useful to controlling for variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14744ec7fa463a16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_timeresolved_within_condition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46879615721504a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decode_train_model, decode_categorical_cross_condition,decodewrap_categorical_timeresolved_cross_condition\n",
    "\n",
    "list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "list_tw = DFallpa[\"twind\"].unique().tolist()\n",
    "list_ev = DFallpa[\"event\"].unique().tolist()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5692844afa5b51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "# # PARAMS\n",
    "# # list_var_decode = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "# # list_vars_conj = [\n",
    "# #     [\"seqc_0_loc\"],\n",
    "# #     [\"seqc_0_shape\"]]\n",
    "# \n",
    "# # list_var_decode = [\"seqc_1_shape\", \"seqc_1_loc\"]\n",
    "# # list_vars_conj = [\n",
    "# #     [\"seqc_0_shape\", \"seqc_0_loc\", \"seqc_1_loc\", \"seqc_2_loc\"],\n",
    "# #     [\"seqc_0_shape\", \"seqc_0_loc\", \"seqc_1_shape\", \"seqc_2_shape\"],\n",
    "# # ]\n",
    "# list_var_decode = [\"seqc_1_shape\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"seqc_0_shape\", \"seqc_0_loc\", \"seqc_1_loc\", \"seqc_2_loc\"],\n",
    "# ]\n",
    "\n",
    "### SEQUENCE CONTEXT\n",
    "# list_var_decode = [\"stroke_index\", \"shape_this_event\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"shape_this_event\", \"loc_this_event\"],\n",
    "#     [\"stroke_index\", \"loc_this_event\"],\n",
    "#     ]\n",
    "# SEQ_CONTEXT_MODE = \"seq_ctxt\"\n",
    "\n",
    "### SEQUENCE PREDICTION\n",
    "# list_var_decode = [\"CTXT_shape_next\"]\n",
    "# list_vars_conj = [\n",
    "#     [\"shape_this_event\", \"loc_this_event\", \"CTXT_loc_next\"],\n",
    "#     ]\n",
    "list_var_decode = [\"CTXT_loc_next\"]\n",
    "list_vars_conj = [\n",
    "    [\"shape_this_event\", \"loc_this_event\", \"CTXT_shape_next\", \"CTXT_loc_prev\"],\n",
    "    ]\n",
    "SEQ_CONTEXT_MODE = \"seq_pred\"\n",
    "\n",
    "# RUns\n",
    "max_nsplits = 10\n",
    "time_bin_size = 0.1\n",
    "slide = 0.05\n",
    "RES = []\n",
    "for i, row in DFallpa.iterrows():\n",
    "    br = row[\"bregion\"]\n",
    "    tw = row[\"twind\"]\n",
    "    ev = row[\"event\"]\n",
    "    pa = row[\"pa\"]\n",
    "                \n",
    "    for var_decode, vars_conj_condition in zip(list_var_decode, list_vars_conj):\n",
    "        print(ev, br, tw, var_decode)\n",
    "\n",
    "        plot_counts_heatmap_savepath = f\"/tmp/{br}-{var_decode}.png\"\n",
    "        \n",
    "        if SEQ_CONTEXT_MODE is not None:\n",
    "            # Clean up, to have clean analyses related to sequence\n",
    "            from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "            from neuralmonkey.analyses.rsa import preprocess_prune_pa_enough_data, preprocess_rsa_prepare_popanal_wrapper\n",
    "            \n",
    "            q_params = rsagood_questions_dict(animal, date, SEQ_CONTEXT_MODE)[SEQ_CONTEXT_MODE]\n",
    "            # q_params[\"effect_vars\"] = [\"shape_this_event\", \"loc_this_event\", \"stroke_index_fromlast_tskstks\"]         \n",
    "            # q_params[\"effect_vars\"] = [\"shape_this_event\", \"loc_this_event\", \"stroke_index\"]         \n",
    "            q_params[\"effect_vars\"] = [var_decode] + vars_conj_condition         \n",
    "            \n",
    "            # q_params[\"exclude_first_stroke\"] = False\n",
    "            pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n",
    "\n",
    "        # 1. Extract the specific pa for this (br, tw)\n",
    "        # res = decodewrap_categorical_timeresolved_cross_condition(pa, var_decode,\n",
    "        #                                             vars_conj_condition,\n",
    "        #                                             subtract_mean_vars_conj=subtract_mean_vars_conj,\n",
    "        #                                             time_bin_size=time_bin_size, slide=slide,\n",
    "        #                                             do_center=True, do_std=False,\n",
    "        #                                             plot_counts_heatmap_savepath=plot_counts_heatmap_savepath)\n",
    "        res = decodewrap_categorical_timeresolved_within_condition(pa, var_decode,\n",
    "                                            vars_conj_condition,\n",
    "                                            time_bin_size=time_bin_size, slide=slide,\n",
    "                                            do_center=True, do_std=False, max_nsplits=max_nsplits,\n",
    "                                                                   plot_counts_heatmap_savepath=plot_counts_heatmap_savepath)\n",
    "\n",
    "        plt.close(\"all\")\n",
    "        if res is not None:\n",
    "            for r in res:\n",
    "                r[\"bregion\"]=br\n",
    "                r[\"twind\"]=tw\n",
    "                r[\"event\"]=ev\n",
    "            RES.extend(res)\n",
    "DFRES = pd.DataFrame(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "659c92a799c90cf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Plot the results, comparing score across methods!!\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "fig = sns.relplot(data=DFRES, x=\"time\", y=\"score\", hue=\"bregion\", row=\"event\", col=\"var_decode\",  kind=\"line\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12f348ba74dc8d3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Train a single decoder on specific dataset, then test across all time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3691223d7c46942"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HACK - combine all bregions into single pa\n",
    "if False:\n",
    "    from neuralmonkey.classes.population import concatenate_popanals_flexible\n",
    "    list_wl = DFallpa[\"which_level\"].unique().tolist()\n",
    "    resthis = []\n",
    "    for wl in list_wl:\n",
    "        for ev in list_ev:\n",
    "            for tw in list_tw:\n",
    "                a = DFallpa[\"event\"] == ev\n",
    "                b = DFallpa[\"twind\"] == tw\n",
    "                c = DFallpa[\"which_level\"] == wl\n",
    "                dfthis = DFallpa[a & b]\n",
    "                pa = concatenate_popanals_flexible(dfthis[\"pa\"].tolist(), \"chans\")[0]\n",
    "                resthis.append({\n",
    "                    \"which_level\":wl,\n",
    "                    \"event\":ev,\n",
    "                    \"bregion\":\"ALL\",\n",
    "                    \"twind\":tw,\n",
    "                    \"pa\":pa\n",
    "                })\n",
    "    DFallpa_ALL = pd.DataFrame(resthis)\n",
    "    DFallpa_ALL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c16b1af399872b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trials_train = pa.Xlabels[\"trials\"][pa.Xlabels[\"trials\"][\"task_kind\"]==\"prims_single\"].index.tolist()\n",
    "trials_test = pa.Xlabels[\"trials\"][pa.Xlabels[\"trials\"][\"task_kind\"]==\"prims_on_grid\"].index.tolist()\n",
    "ntrials_expected_assert = len(pa.Xlabels[\"trials\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d93407c75be66d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HACK = False\n",
    "if HACK:\n",
    "    # Use all brain regions\n",
    "    DFallpaTHIS = DFallpa_ALL\n",
    "else:\n",
    "    DFallpaTHIS = DFallpa\n",
    "\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_single_decoder_across_time\n",
    "list_br = DFallpaTHIS[\"bregion\"].unique().tolist()\n",
    "list_tw = DFallpaTHIS[\"twind\"].unique().tolist()\n",
    "# list_var_decode = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "list_var_decode = [\"seqc_0_shape\"]\n",
    "# ev_train = \"03_samp\"\n",
    "# twind_train = [0.4, 0.6]\n",
    "ev_train = \"06_on_strokeidx_0\"\n",
    "twind_train = [0.05, 0.35]\n",
    "\n",
    "ev_test = \"03_samp\"\n",
    "# ev_test = \"06_on_strokeidx_0\"\n",
    "time_bin_size=0.05\n",
    "slide=0.025\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e57ef9aec8741621"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RES = []\n",
    "for br in list_br:\n",
    "    for tw in list_tw:\n",
    "        # 1. Extract the specific pa for this (br, tw)\n",
    "        \n",
    "        # Prep train and test PA\n",
    "        PA_train = extract_single_pa(DFallpaTHIS, br, tw, event=ev_train)\n",
    "        PA_train = PA_train.slice_by_dim_values_wrapper(\"trials\", trials_train)\n",
    "        PA_train = PA_train.slice_by_dim_values_wrapper(\"times\", twind_train)\n",
    "        PA_train = PA_train.agg_wrapper(\"times\")\n",
    "        x_train = PA_train.X.squeeze(axis=2).T # (ntrials, nchans)\n",
    "        \n",
    "        PA_test = extract_single_pa(DFallpaTHIS, br, tw, event=ev_test)\n",
    "        PA_test = PA_test.slice_by_dim_values_wrapper(\"trials\", trials_test)\n",
    "        if time_bin_size is not None:\n",
    "            PA_test = PA_test.agg_by_time_windows_binned(time_bin_size, slide)\n",
    "        X_test = PA_test.X # (chans, trials, times)\n",
    "        times_test = PA_test.Times\n",
    "        \n",
    "        for var_decode in list_var_decode:\n",
    "\n",
    "            # Train model\n",
    "            labels_train = PA_train.Xlabels[\"trials\"][var_decode].tolist()\n",
    "            \n",
    "            # Get test data\n",
    "            labels_test = PA_test.Xlabels[\"trials\"][var_decode].tolist()\n",
    "            \n",
    "            res = decodewrap_categorical_single_decoder_across_time(x_train, labels_train, X_test, labels_test,\n",
    "                                                                  times_test, do_std=False)\n",
    "        \n",
    "            for r in res:\n",
    "                r[\"var_decode\"]=var_decode\n",
    "                r[\"bregion\"]=br\n",
    "                r[\"twind\"]=tw   \n",
    "                r[\"event\"] = ev_test\n",
    "\n",
    "            RES.extend(res)\n",
    "\n",
    "DFRES = pd.DataFrame(RES)\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6abaf4b5797db13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract single trial results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3a21d2839843b42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# PARAMS\n",
    "var_decode = \"seqc_0_shape\"\n",
    "bregion = \"PMv\"\n",
    "twind = (-0.4, 0.6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f98b66754dbece70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "a = DFRES[\"var_decode\"]==var_decode\n",
    "b = DFRES[\"bregion\"]==bregion\n",
    "c = DFRES[\"twind\"]==twind\n",
    "dfthis = DFRES[a & b & c]\n",
    "\n",
    "times = dfthis[\"time\"]\n",
    "import numpy as np\n",
    "labels_predicted = np.stack(dfthis[\"labels_predicted\"]).T # (ntrials, ntimes)\n",
    "labels_test = np.stack(dfthis[\"labels_test\"]).T\n",
    "conf_scores = np.transpose(np.stack(dfthis[\"conf_scores\"]), [1,0,2]) # (ntrials, ntimes, nclasses)\n",
    "\n",
    "# score each time bin as correct or incorrect\n",
    "labels_correct = (labels_predicted == labels_test).astype(int)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8296d5b9f2e7ea8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each trial, what was its label\n",
    "assert np.all(np.diff(labels_test, axis=1))==0, \"otherwise cant do next step.\"\n",
    "labels_each_trial = labels_test[:,0]\n",
    "labels_orig = PA_test.Xlabels[\"trials\"][var_decode].tolist()\n",
    "assert len(labels_orig)==len(labels_each_trial)\n",
    "\n",
    "map_labint_to_trials = {}\n",
    "map_labint_to_laborig = {}\n",
    "labels_unique = np.unique(labels_each_trial)\n",
    "for lab in labels_unique:\n",
    "    inds_this_label = np.argwhere(labels_each_trial == lab).squeeze() # the indices which ahve this label as the CORRECT\n",
    "    map_labint_to_trials[lab] = inds_this_label\n",
    "\n",
    "    # Map it back to original label\n",
    "    lab_orig = PA_test.Xlabels[\"trials\"][var_decode][inds_this_label].unique()\n",
    "    assert len(lab_orig)==1\n",
    "    map_labint_to_laborig[lab] = lab_orig[0]\n",
    "\n",
    "map_trial_to_labint = {i:labint for i, labint in enumerate(labels_each_trial)}\n",
    "print(map_labint_to_laborig)    \n",
    "map_laborig_to_labint = {}\n",
    "for labint, laborig in map_labint_to_laborig.items():\n",
    "    assert laborig not in map_laborig_to_labint\n",
    "    map_laborig_to_labint[laborig] = labint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pythonlib.tools.plottools import makeColors\n",
    "pcols = makeColors(len(map_labint_to_laborig))\n",
    "map_trial_to_seq = {}\n",
    "map_trial_to_seq_ints = {}\n",
    "for i, seq in enumerate(PA_test.Xlabels[\"trials\"].loc[:, [\"seqc_0_shape\", \"seqc_1_shape\"]].values.tolist()):\n",
    "    map_trial_to_seq[i] = (tuple(seq))\n",
    "    map_trial_to_seq_ints[i] = [map_laborig_to_labint[s] if s in map_laborig_to_labint else s for s in seq]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3025b97e2c823fa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pick n random trials\n",
    "import random\n",
    "n=  9\n",
    "trials_all = list(range(len(map_trial_to_labint)))\n",
    "trials = random.sample(trials_all, n)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(trials)/ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*3), sharex=True, sharey=True)\n",
    "\n",
    "for tr, ax in zip(trials, axes.flatten()):\n",
    "    \n",
    "    # ax = axes.flatten()[0]\n",
    "    lab_pred = labels_predicted[tr,:]\n",
    "    # lab_corr = labels_correct[trial,:]\n",
    "    # lab_test = labels_test[trial,:]\n",
    "    # ax.plot(times, lab_pred, \"-ok\"),\n",
    "    # ax.plot(times, lab_test, \"-r\")\n",
    "    \n",
    "    # for each class, plot its time series\n",
    "    for labint in map_labint_to_laborig:\n",
    "        cs_this_lab = conf_scores[tr, :, labint].squeeze()\n",
    "        \n",
    "        ax.plot(times, cs_this_lab, label=labint, color=pcols[labint])\n",
    "        # ax.plot(times[lab_pred==labint], cs_this_lab[lab_pred==labint], \"-\", color=pcols[labint], linewidth=4)\n",
    "        ax.plot(times[lab_pred==labint], cs_this_lab[lab_pred==labint], \"s\", color=pcols[labint])\n",
    "    \n",
    "    ax.axvline(0, color=\"k\", alpha=0.5)\n",
    "    ax.set_title(f\"{var_decode}={map_trial_to_labint[tr]}\", color=pcols[map_trial_to_labint[tr]])\n",
    "    ax.set_xlabel(f\"seq: {map_trial_to_seq_ints[tr]}\")\n",
    "    ax.legend()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeac8f2e7321bfd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each class label, collect all trials for which that is the correct label, and overlap\n",
    "\n",
    "labels_unique = np.unique(labels_each_trial)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(labels_unique)/ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*4))\n",
    "\n",
    "for lab, ax in zip(labels_unique, axes.flatten()):\n",
    "    inds_this_label = np.argwhere(labels_each_trial == lab).squeeze() # the indices which ahve this label as the CORRECT \n",
    "    ax.plot(times, labels_predicted[inds_this_label, :].T, \"-ok\", alpha=0.05)\n",
    "    ax.set_title(lab)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8efa1735ce6376c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "ax = axes.flatten()[0]\n",
    "ax.plot(times, labels_correct.T, \"xk\", alpha=0.003);"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7a8e07c6d583ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Plot the results, comparing score across methods!!\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "fig = sns.relplot(data=DFRES, x=\"time\", y=\"score\", hue=\"bregion\", col=\"var_decode\",  kind=\"line\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ddae94bd60f728e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Plot the results, comparing score across methods!!\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "fig = sns.relplot(data=DFRES, x=\"time\", y=\"score\", hue=\"bregion\", col=\"var_decode\",  kind=\"line\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef8b1c64ea5cadd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 4) Cross-decoding across time bins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3e89d8c3064a967"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_cross_time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5483ffe2c1ea629f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR_ANALYSIS = \"/tmp\"\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af81acf8ae6cd5d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/3_cross_temporal\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3813e5625d5647dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### A variation, where instead of first concatting PAs, just compute decode separately (and across PA) and then concatenate to plot\n",
    "Advantage: dont have to have identical trials, which is needed if you want to concat breigons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3658025bc4117336"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "assert False, \"add methdo to subtract mean in other window...\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28003a0e2a0f59f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb762d8394e495c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa_TEST = DFallpa[:20]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8cd08d2eabd4017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_var_decode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277450d6e1171809"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from neuralmonkey.analyses.decode_good import preprocess_extract_X_and_labels\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_plot\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/3_cross_temporal\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "# Make sure all pa use the same variables to refer to shapes.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")\n",
    "# warnings.filterwarnings(\"error\")\n",
    "time_bin_size = 0.3\n",
    "slide=0.3\n",
    "\n",
    "list_vars_decode = [\"seqc_0_shape\"]\n",
    "list_ev = ['04_go_cue', '06_on_STK_1']\n",
    "\n",
    "assert len(DFallpa[\"twind\"].unique())==1, \"not big deal. just change code below to iter over all (ev, tw).\"\n",
    "\n",
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_cross_time\n",
    "DFRES = decodewrapouterloop_categorical_cross_time(DFallpa_TEST, list_var_decode, time_bin_size, slide)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18e78f93ad0a24c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_plot\n",
    "decodewrapouterloop_categorical_cross_time_plot(DFRES, SAVEDIR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb93cb1ae6dee162"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Variation, focusing on \"shape_this_event\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9266dbf14fa3a06d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_var_decode = [\"shape_this_event\"]\n",
    "time_bin_size = 0.2\n",
    "slide = 0.2\n",
    "DFRES = decodewrapouterloop_categorical_cross_time(DFallpa, list_var_decode, time_bin_size, slide, savedir_ndata=\"/tmp\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f256b68bd8e88050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFRES[:2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40e8fbfe3e9fb2f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for var in [\"event_train\", \"event_test\", \"task_kind_train\", \"task_kind_test\"]:\n",
    "    print(DFRES[var].unique())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e137d821cf73cebb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "inds = list(range(100000))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b04d9cab20007a67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.all(np.diff(inds)==1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b0e131f5af23123"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(list(sorted(set(inds)))==inds) and (inds[-1] == len(inds)-1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b568bc83ee56db9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(list(sorted(set(inds)))==inds) and (inds[-1] == len(inds)-1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4af0d8c987c2465"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ebc0a166e61379b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "all([i2-i1==1 for i1, i2 in zip(inds[:-1], inds[1:])])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6b9549a0b1e970b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### New plots, which compare decoding across-context vs. within-context, etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a08e6fd791aa04c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFRESTHIS = DFRES[DFRES[\"bregion\"].isin([\"M1_m\", \"PM_v\"])].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "802badf0ea8691fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_cross_time_plot_compare_contexts\n",
    "\n",
    "var_decode = \"shape_this_event\"\n",
    "SAVEDIR = \"/tmp\"\n",
    "decodewrapouterloop_categorical_cross_time_plot_compare_contexts(DFRESTHIS, var_decode,\n",
    "                                                                     SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63a3476c431b9115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### For a given decoder, plot its predictions during a given event [OBSOLETE< see #5 below)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69deb18aa11e2266"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bregion = \"PMv\"\n",
    "\n",
    "var_decode = \"seqc_0_shape\"\n",
    "event_train = \"04_go_cue\"\n",
    "tbin_train = 1\n",
    "event_test = \"06_on_STK_1\"\n",
    "tbin_test = 1\n",
    "\n",
    "# var_decode = \"seqc_0_shape\"\n",
    "# event_train = \"04_go_cue\"\n",
    "# tbin_train = 1\n",
    "# event_test = \"06_on_STK_0\"\n",
    "# tbin_test = 1\n",
    "# \n",
    "# var_decode = \"seqc_0_shape\"\n",
    "# event_train = \"06_on_STK_0\"\n",
    "# tbin_train = 2\n",
    "# event_test = \"04_go_cue\"\n",
    "# tbin_test = 1\n",
    "\n",
    "a = DFRES[\"bregion\"] == bregion\n",
    "b = DFRES[\"var_decode\"] == var_decode\n",
    "c = DFRES[\"event_train\"] == event_train\n",
    "d = DFRES[\"tbin_train\"] == tbin_train\n",
    "e = DFRES[\"event_test\"] == event_test\n",
    "f = DFRES[\"tbin_test\"] == tbin_test\n",
    "\n",
    "tmp = DFRES[a & b & c & d & e & f]\n",
    "assert len(tmp)==1\n",
    "\n",
    "# Labels (convert label ints to actual value)\n",
    "labels_pred = tmp[\"labels_predicted\"].values[0]\n",
    "map_int_to_lab = tmp[\"map_int_to_lab\"].values[0]\n",
    "labels_pred = [map_int_to_lab[i] for i in labels_pred]\n",
    "\n",
    "score = tmp[\"score\"].values[0]\n",
    "\n",
    "# For each trial, compare to its sequence context\n",
    "inds_keep_test = tmp[\"inds_keep_test\"].values[0]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "106c2e0e46abc6a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3daaef98ed34201b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pa_test = extract_single_pa(DFallpa, bregion, twind, \"trial\", event=event_test)\n",
    "dflab_test = pa_test.Xlabels[\"trials\"].iloc[inds_keep_test].copy()\n",
    "\n",
    "\n",
    "list_seq = dflab_test.loc[:, [\"seqc_0_shape\", \"seqc_1_shape\"]].values.tolist() # list of n-tuples (len of seq)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c1824d8f13b0e8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dflab_test[\"labels_pred\"] = labels_pred\n",
    "dflab_test[:5]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f1c808932d0a3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.displot(data=dflab_test, x=\"labels_pred\", row=\"seqc_0_shape\", col=\"seqc_1_shape\", height=5)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb004a290865470e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sumamraize:\n",
    "\n",
    "# frequency of prediction matching first shape\n",
    "# same, for second shape.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccde37f8286c734a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5) Cross-temporal decoder, but with decoder trained using one variable (e.g., seqc_0_shape) and testing prediction of another variable (e.g., seqc_1_shape).\n",
    "Useful if, for example, want to see if decoder trained during visual presentation can decode 2nd stroke DURING 1st stroke."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1487d1d771d3490"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(range(1,3))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a147e241b53ce2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/4_cross_temporal_diff_var_train_test\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d443eeb181b33520"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa[\"event\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ab9fc36dea9c6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_cross_var"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df788b7c1c64483d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A = sorted(set(DFRES[\"var_decode_train\"].tolist()))\n",
    "B = sorted(set(DFRES[\"var_decode_test\"].tolist()))\n",
    "list_var_decode_train_test = [(a, b) for a in A for b in B]\n",
    "list_var_decode_train_test\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43d0bb9d59044017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make sure all pa use the same variables to refer to shapes.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")\n",
    "# warnings.filterwarnings(\"error\")\n",
    "\n",
    "# list_var_decode_train_test = [\n",
    "#     [\"seqc_0_shape\", \"seqc_1_shape\"], # (variable to construct decoder, variable that you will try to predict).\n",
    "#     [\"seqc_0_shape\", \"seqc_2_shape\"], # (variable to construct decoder, variable that you will try to predict).\n",
    "#     [\"seqc_1_shape\", \"seqc_2_shape\"] # (variable to construct decoder, variable that you will try to predict).\n",
    "# ]\n",
    "list_var_decode_train_test = [\n",
    "    [\"seqc_0_shape\", \"seqc_1_shape\"], # (variable to construct decoder, variable that you will try to predict).\n",
    "]\n",
    "\n",
    "labels_ignore = [\"IGNORE\", (\"IGNORE\",)]\n",
    "# list_ev = ['04_go_cue', '06_on_STK_0', '06_on_STK_1', '06_on_STK_2']\n",
    "list_ev = ['04_go_cue', '06_on_STK_0']\n",
    "\n",
    "assert len(DFallpa[\"twind\"].unique())==1, \"not big deal. just change code below to iter over all (ev, tw).\"\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/4_cross_temporal_diff_var_train_test\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time\n",
    "\n",
    "# Make sure all pa use the same variables to refer to shapes.\n",
    "time_bin_size = 0.3\n",
    "slide=0.3\n",
    "\n",
    "\n",
    "DFRES = decodewrapouterloop_categorical_cross_time_cross_var(DFallpa,\n",
    "                                                     list_var_decode_train_test,\n",
    "                                                         time_bin_size, slide)\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_plot\n",
    "decodewrapouterloop_categorical_cross_time_plot(DFRES, SAVEDIR)\n",
    "\n",
    "# \n",
    "# assert len(DFallpa[\"twind\"].unique())==1, \"not big deal. just change code below to iter over all (ev, tw).\"\n",
    "# \n",
    "# RES = []\n",
    "# for br in list_br:\n",
    "#     for tw in list_tw:\n",
    "#         for ev_train in list_ev:\n",
    "#             for ev_test in list_ev:\n",
    "# \n",
    "#                 print(br, tw, ev_train, ev_test)\n",
    "# \n",
    "#                 # TRAIN\n",
    "#                 PA_train = extract_single_pa(DFallpa, br, tw, event=ev_train)\n",
    "#                 if time_bin_size is not None:\n",
    "#                     PA_train = PA_train.agg_by_time_windows_binned(time_bin_size, slide)\n",
    "# \n",
    "#                 # TEST\n",
    "#                 PA_test = extract_single_pa(DFallpa, br, tw, event=ev_test)\n",
    "#                 if time_bin_size is not None:\n",
    "#                     PA_test = PA_test.agg_by_time_windows_binned(time_bin_size, slide)\n",
    "# \n",
    "#                 for var_decode_train, var_decode_test in list_var_decode_train_test:\n",
    "# \n",
    "#                     X_train, labels_train, times_train = preprocess_extract_X_and_labels(PA_train, var_decode_train)\n",
    "#                     X_test, labels_test, times_test = preprocess_extract_X_and_labels(PA_test, var_decode_test)\n",
    "# \n",
    "#                     if len(set(labels_train))==1 or len(set(labels_test))==1:\n",
    "#                         print(\"SKIPPING, becuase only one label:\")\n",
    "#                         print(\"Train:\", set(labels_train))\n",
    "#                         print(\"Test:\", set(labels_test))\n",
    "#                         continue\n",
    "# \n",
    "#                     # Only do splits if these are same trials\n",
    "#                     do_train_test_kfold_splits = labels_train==labels_test\n",
    "# \n",
    "#                     res = decodewrap_categorical_cross_time(X_train, labels_train, times_train,\n",
    "#                                                       X_test, labels_test, times_test,\n",
    "#                                                       do_std=False, labels_ignore=labels_ignore,\n",
    "#                                                             do_train_test_kfold_splits=do_train_test_kfold_splits)\n",
    "# \n",
    "#                     for r in res:\n",
    "#                         r[\"var_decode_train\"]=var_decode_train\n",
    "#                         r[\"var_decode_test\"]=var_decode_test\n",
    "#                         r[\"bregion\"]=br\n",
    "#                         r[\"event_train\"]=ev_train\n",
    "#                         r[\"event_test\"]=ev_test\n",
    "# \n",
    "#                     RES.extend(res)\n",
    "# DFRES = pd.DataFrame(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572ae792311d1a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOAD a pre-saved results from decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ce7eb19a2692bed"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pythonlib.tools.pandastools import grouping_print_n_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "import os\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:03:34.220285421Z",
     "start_time": "2024-02-27T21:03:34.185052513Z"
    }
   },
   "id": "3824ff7b34b8b0fb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Pancho\n",
    "# TIME_TRAIN_REL_TEST = 0.075\n",
    "# TIME_WIND_TEST_REL_EVENT = [-0.3, 0.3]\n",
    "\n",
    "# TIME_TRAIN_REL_TEST = 0.175\n",
    "# TIME_WIND_TEST_REL_EVENT = [-0.3, 0.4]\n",
    "\n",
    "# TIME_TRAIN_REL_TEST = 0.225\n",
    "# TIME_WIND_TEST_REL_EVENT = [-0.6, 0.6]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:03:34.597723371Z",
     "start_time": "2024-02-27T21:03:34.521359845Z"
    }
   },
   "id": "615dd90ea3b9ec89"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gorilla1/analyses/recordings/main/DECODE/Pancho-230126/CHAR-combined_1-combine_areas_False\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_4') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_4') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_4') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_4') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '03_samp') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '06_on_STK_0') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '06_on_STK_0') :     29040\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_3') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_4') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_2', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_2', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_3', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     36300\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('same', 'same', 'character', 'character', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_3', '06_on_STK_3') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_4', '06_on_STK_4') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'prims_single', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     29040\n",
      "2374020\n",
      "480690\n",
      "201390\n",
      "201390\n",
      "176400\n",
      "510\n",
      "510\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2374020\n",
      "343350\n",
      "143850\n",
      "143850\n",
      "126000\n",
      "510\n",
      "510\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2374020\n",
      "657270\n",
      "275370\n",
      "275370\n",
      "241200\n",
      "510\n",
      "510\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2374020\n",
      "480690\n",
      "201390\n",
      "201390\n",
      "176400\n",
      "510\n",
      "510\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2374020\n",
      "824040\n",
      "345240\n",
      "345240\n",
      "302400\n",
      "510\n",
      "510\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2374020\n",
      "613125\n",
      "256875\n",
      "256875\n",
      "225000\n",
      "510\n",
      "510\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "/gorilla1/analyses/recordings/main/DECODE/Diego-231201/CHAR-combined_1-combine_areas_False\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_4') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_4') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_4') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_4') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '03_samp') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '06_on_STK_0') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '06_on_STK_0') :     29040\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_4') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_4') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_3', '06_on_STK_4') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_3') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_3', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_2', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_3', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_2', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_3', '06_on_STK_4') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     36300\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('same', 'same', 'character', 'character', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_3', '06_on_STK_3') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_4', '06_on_STK_4') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_3', '06_on_STK_3') :     58080\n",
      "('same', 'same', 'prims_single', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     29040\n",
      "2642640\n",
      "535080\n",
      "241080\n",
      "241080\n",
      "213150\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2642640\n",
      "382200\n",
      "172200\n",
      "172200\n",
      "152250\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2642640\n",
      "731640\n",
      "329640\n",
      "329640\n",
      "291450\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2642640\n",
      "535080\n",
      "241080\n",
      "241080\n",
      "213150\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2642640\n",
      "917280\n",
      "413280\n",
      "413280\n",
      "365400\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2642640\n",
      "682500\n",
      "307500\n",
      "307500\n",
      "271875\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "/gorilla1/analyses/recordings/main/DECODE/Diego-231204/CHAR-combined_1-combine_areas_False\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '03_samp') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '06_on_STK_0') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '06_on_STK_0') :     29040\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_3') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_3', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_2', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_3', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     36300\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('same', 'same', 'character', 'character', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_3', '06_on_STK_3') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_3', '06_on_STK_3') :     58080\n",
      "('same', 'same', 'prims_single', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     29040\n",
      "2461140\n",
      "498330\n",
      "213150\n",
      "213150\n",
      "213150\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2461140\n",
      "355950\n",
      "152250\n",
      "152250\n",
      "152250\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2461140\n",
      "681390\n",
      "291450\n",
      "291450\n",
      "291450\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2461140\n",
      "498330\n",
      "213150\n",
      "213150\n",
      "213150\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2461140\n",
      "854280\n",
      "365400\n",
      "365400\n",
      "365400\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "2461140\n",
      "635625\n",
      "271875\n",
      "271875\n",
      "271875\n",
      "720\n",
      "720\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "/gorilla1/analyses/recordings/main/DECODE/Diego-231219/CHAR-combined_1-combine_areas_False\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'character', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '03_samp') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '03_samp', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '04_go_cue') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_0') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'character', '04_go_cue', '06_on_STK_1') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '03_samp') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '03_samp', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '04_go_cue') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     58080\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     36300\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     14520\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_on_grid', 'prims_single', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'character', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '03_samp') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '03_samp', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '04_go_cue') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_0') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_1') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_2') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_on_grid', '04_go_cue', '06_on_STK_3') :     7260\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '03_samp') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '03_samp', '06_on_STK_0') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '04_go_cue') :     29040\n",
      "('IGNORE', 'IGNORE', 'prims_single', 'prims_single', '04_go_cue', '06_on_STK_0') :     29040\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     7260\n",
      "('diff', 'diff_tk_diff_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'character', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_0', '06_on_STK_0') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'character', '06_on_STK_1', '06_on_STK_1') :     14520\n",
      "('diff', 'diff_tk_same_ev', 'prims_on_grid', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'character', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'diff_tk_same_ev', 'prims_single', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     7260\n",
      "('diff', 'same_tk_diff_ev', 'character', 'character', '06_on_STK_0', '06_on_STK_1') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_1') :     36300\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_2') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_3') :     14520\n",
      "('diff', 'same_tk_diff_ev', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_3') :     14520\n",
      "('same', 'same', 'character', 'character', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'character', 'character', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_0', '06_on_STK_0') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_1', '06_on_STK_1') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_2', '06_on_STK_2') :     58080\n",
      "('same', 'same', 'prims_on_grid', 'prims_on_grid', '06_on_STK_3', '06_on_STK_3') :     58080\n",
      "('same', 'same', 'prims_single', 'prims_single', '06_on_STK_0', '06_on_STK_0') :     29040\n",
      "1967460\n",
      "398370\n",
      "142590\n",
      "142590\n",
      "142590\n",
      "330\n",
      "330\n",
      "30\n",
      "30\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 138\u001B[0m\n\u001B[1;32m    135\u001B[0m         plt\u001B[38;5;241m.\u001B[39mclose(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    137\u001B[0m \u001B[38;5;66;03m# just \"same\" vs \"diff\" (combining all diff).\u001B[39;00m\n\u001B[0;32m--> 138\u001B[0m _, fig \u001B[38;5;241m=\u001B[39m \u001B[43mplot_45scatter_means_flexible_grouping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDFRES_THIS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_manip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontext_tk_ev_simple\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_lev_manip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msame\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_lev_manip\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdiff\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_subplot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvar_decode\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore_adjusted\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_datapt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbregion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m savefig(fig, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msavedir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/scatter-same-vs-diff.pdf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    140\u001B[0m plt\u001B[38;5;241m.\u001B[39mclose(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/gorilla1/code/pythonlib/pythonlib/tools/pandastools.py:2858\u001B[0m, in \u001B[0;36mplot_45scatter_means_flexible_grouping\u001B[0;34m(dfthis, var_manip, x_lev_manip, y_lev_manip, var_subplot, var_value, var_datapt, plot_text, alpha, SIZE, shareaxes, plot_error_bars)\u001B[0m\n\u001B[1;32m   2856\u001B[0m list_manip \u001B[38;5;241m=\u001B[39m [x_lev_manip, y_lev_manip]\n\u001B[1;32m   2857\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m x_lev_manip \u001B[38;5;129;01min\u001B[39;00m dfthis[var_manip]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m-> 2858\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m y_lev_manip \u001B[38;5;129;01min\u001B[39;00m dfthis[var_manip]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m   2860\u001B[0m nmin \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2861\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(var_subplot, \u001B[38;5;28mlist\u001B[39m):\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "\n",
    "N_STROKES_MAX = 4\n",
    "for animal, date in [\n",
    "    (\"Pancho\", 230126),\n",
    "    (\"Diego\", 231201),\n",
    "    (\"Diego\", 231204),\n",
    "    (\"Diego\", 231219),\n",
    "]:\n",
    "    # animal = \"Diego\"\n",
    "    # date = 231219\n",
    "    question = \"CHAR\"\n",
    "    combined_trial_stroke = 1\n",
    "    combine_areas = False\n",
    "    which_decode = \"3_cross_temporal\"\n",
    "    \n",
    "    ############################## LOAD DATA\n",
    "    SAVEDIR_LOAD = f\"/gorilla1/analyses/recordings/main/DECODE/{animal}-{date}/{question}-combined_{combined_trial_stroke}-combine_areas_{combine_areas}\" \n",
    "    # SAVEDIR = f\"/gorilla1/analyses/recordings/main/DECODE/{animal}-{date}/{question}-combined_{combined_trial_stroke}-combine_areas_{combine_areas}/{which_decode}\"\n",
    "    path = f\"{SAVEDIR_LOAD}/{which_decode}/DFRES.pkl\"\n",
    "    print(SAVEDIR_LOAD)\n",
    "    \n",
    "    DFRES = pd.read_pickle(path)    \n",
    "    \n",
    "    ############################# PREP DATAFRAME\n",
    "    ##### Context (same, diff) plots, binning tbins --> scalar\n",
    "    # Give new column that classifies the \"context\" \n",
    "    def F(x):\n",
    "        # If is not strokes, then ignore it (throw out)\n",
    "        if \"06_on_\" not in x[\"event_train\"]:\n",
    "            return \"IGNORE\"\n",
    "            \n",
    "        # Classify context\n",
    "        tktrain, tktest, evtrain, evtest = x[\"task_kind_train\"], x[\"task_kind_test\"], x[\"event_train\"], x[\"event_test\"]\n",
    "        if tktrain==tktest and evtrain==evtest:\n",
    "            return \"same\"\n",
    "        elif (tktrain==tktest) and (not evtrain==evtest):\n",
    "            return \"same_tk_diff_ev\"\n",
    "        elif (not tktrain==tktest) and (evtrain==evtest):\n",
    "            return \"diff_tk_same_ev\"\n",
    "        else:\n",
    "            return \"diff_tk_diff_ev\"    \n",
    "    DFRES = applyFunctionToAllRows(DFRES, F, \"context_tk_ev\")\n",
    "    \n",
    "    # Also group all \"diff\" into a single \"diff\"\n",
    "    inds = DFRES[\"context_tk_ev\"].isin([\"same_tk_diff_ev\", \"diff_tk_same_ev\", \"diff_tk_diff_ev\"])\n",
    "    DFRES[\"context_tk_ev_simple\"] = DFRES[\"context_tk_ev\"]\n",
    "    DFRES.loc[inds, \"context_tk_ev_simple\"] = \"diff\"\n",
    "        \n",
    "    # Only call \"diff\" those cases that are diff task kind... (avoid confounds in correlations across stroke indices, for same task kind).\n",
    "    inds = DFRES[\"context_tk_ev\"].isin([\"diff_tk_same_ev\", \"diff_tk_diff_ev\"])\n",
    "    DFRES[\"context_tk_ev_simple_diff_tk\"] = DFRES[\"context_tk_ev\"]\n",
    "    DFRES.loc[inds, \"context_tk_ev_simple_diff_tk\"] = \"diff\"\n",
    "    \n",
    "    # Append column with the test \"event x \"task kind\". \n",
    "    DFRES = append_col_with_grp_index(DFRES, [\"task_kind_test\", \"event_test\"], \"tk_ev_test\", strings_compact=True)\n",
    "    DFRES = append_col_with_grp_index(DFRES, [\"task_kind_train\", \"event_train\"], \"tk_ev_train\", strings_compact=True)\n",
    "    \n",
    "    grouping_print_n_samples(DFRES, [\"context_tk_ev_simple\", \"context_tk_ev\", \"task_kind_train\", \"task_kind_test\", \"event_train\", \"event_test\"]);\n",
    "    \n",
    "    for TIME_TRAIN_REL_TEST in [0.125, 0.175, 0.225]:\n",
    "        for TIME_WIND_TEST_REL_EVENT in [\n",
    "                                    [-0.5, 0.5],\n",
    "                                    [-0.3, 0.4],\n",
    "                                ]:\n",
    "            #             TIME_TRAIN_REL_TEST = 0.175\n",
    "            # TIME_WIND_TEST_REL_EVENT = [-0.3, 0.4]\n",
    "    \n",
    "            ##### Convert to scalar by taking mean over time bins\n",
    "            # criterion for time bins\n",
    "            def F(x):\n",
    "                # relative time\n",
    "                a = np.abs(x[\"time_train\"] - x[\"time_test\"]) < TIME_TRAIN_REL_TEST\n",
    "                # absolute time\n",
    "                b = (x[\"time_test\"] >= TIME_WIND_TEST_REL_EVENT[0]) & (x[\"time_test\"] <= TIME_WIND_TEST_REL_EVENT[1])\n",
    "                return a & b\n",
    "            DFRES = applyFunctionToAllRows(DFRES, F, \"keep_tbin\")\n",
    "        \n",
    "            # Print results\n",
    "            if False:\n",
    "                grouping_print_n_samples(DFRES, [\"keep_tbin\", \"time_train\", \"time_test\"])\n",
    "            \n",
    "            ####################### PRUNE DATA TO JUST TIME BINS OF INTEREST\n",
    "            DFRES_THIS = DFRES.copy()\n",
    "            \n",
    "            print(len(DFRES))\n",
    "            # Prune to just those with tbins to keep\n",
    "            DFRES_THIS = DFRES_THIS[DFRES_THIS[\"keep_tbin\"]==True]\n",
    "            print(len(DFRES_THIS))\n",
    "            \n",
    "            # Remove those with CONTEXT ignore.\n",
    "            DFRES_THIS = DFRES_THIS[DFRES_THIS[\"context_tk_ev\"] != \"IGNORE\"]\n",
    "            print(len(DFRES_THIS))\n",
    "            \n",
    "            for INCLUDE_PIG in [True, False]:\n",
    "                for EXCLUDE_FIRST_STROKE in [True, False]:\n",
    "                    \n",
    "                    if INCLUDE_PIG==False:\n",
    "                        # Only include data that is single prims or character (since PIG is low N).\n",
    "                        DFRES_THIS = DFRES_THIS[(DFRES_THIS[\"task_kind_train\"].isin([\"prims_single\", \"character\"])) & (DFRES_THIS[\"task_kind_test\"].isin([\"prims_single\", \"character\"]))]\n",
    "                    \n",
    "                    # Remove cases that are the high N stroke nums, they are usualyl noise\n",
    "                    events_keep = [f\"06_on_STK_{i}\" for i in range(N_STROKES_MAX)]\n",
    "                    print(len(DFRES_THIS))\n",
    "                    DFRES_THIS = DFRES_THIS[(DFRES_THIS[\"event_train\"].isin(events_keep)) & (DFRES_THIS[\"event_test\"].isin(events_keep))]\n",
    "                    print(len(DFRES_THIS))\n",
    "                    \n",
    "                    if EXCLUDE_FIRST_STROKE:\n",
    "                        DFRES_THIS = DFRES_THIS[~(DFRES_THIS[\"event_train\"]==\"06_on_STK_0\") & ~(DFRES_THIS[\"event_test\"]==\"06_on_STK_0\")]\n",
    "                    \n",
    "                    #################################### Aggregate to average over time bins\n",
    "                    from pythonlib.tools.pandastools import aggregGeneral\n",
    "                    DFRES_THIS = aggregGeneral(DFRES_THIS, [\"var_decode\", \"bregion\", \"event_train\", \"event_test\", \"task_kind_train\", \"task_kind_test\", \n",
    "                                                            \"context_tk_ev\", \"context_tk_ev_simple\", \"context_tk_ev_simple_diff_tk\",\n",
    "                                                            \"keep_tbin\", \"tk_ev_test\", \"tk_ev_train\"], values=[\"score\", \"score_adjusted\"])\n",
    "                    \n",
    "                    DFRES_THIS = DFRES_THIS.reset_index(drop=True)\n",
    "                    \n",
    "                    #################################### PLOTS\n",
    "                    a = TIME_TRAIN_REL_TEST\n",
    "                    b = \"_\".join([str(x) for x in TIME_WIND_TEST_REL_EVENT])\n",
    "                    savedir = f\"{SAVEDIR_LOAD}/3_cross_temporal_split_by_contexts_scalar/inclPIG={INCLUDE_PIG}-excldFrstStk={EXCLUDE_FIRST_STROKE}-TRelTest={a}-TWindRelEv={b}\"\n",
    "            \n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "                    \n",
    "                    dfthis = DFRES[DFRES[\"bregion\"]==DFRES[\"bregion\"].values[0]] # pick out a subsample --> smaler plots.\n",
    "                    fig = sns.relplot(data=dfthis, x=\"time_train\", y=\"time_test\", col=\"keep_tbin\")\n",
    "                    savefig(fig, f\"{savedir}/time_bins.pdf\")\n",
    "                    \n",
    "                    for ctxt_diff in [\"same_tk_diff_ev\", \"diff_tk_same_ev\", \"diff_tk_diff_ev\"]:\n",
    "                        if ctxt_diff in DFRES_THIS[\"context_tk_ev\"].tolist():\n",
    "                            _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev\", x_lev_manip=\"same\", y_lev_manip=ctxt_diff, var_subplot=\"var_decode\", var_value=\"score_adjusted\", var_datapt=\"bregion\")\n",
    "                            savefig(fig, f\"{savedir}/scatter-same-vs-{ctxt_diff}.pdf\")\n",
    "                            plt.close(\"all\")\n",
    "                    \n",
    "                    # just \"same\" vs \"diff\" (combining all diff).\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple\", x_lev_manip=\"same\", y_lev_manip=\"diff\", var_subplot=\"var_decode\", var_value=\"score_adjusted\", var_datapt=\"bregion\")\n",
    "                    savefig(fig, f\"{savedir}/scatter-same-vs-diff.pdf\")\n",
    "                    plt.close(\"all\")\n",
    "                    \n",
    "                    if \"diff\" in DFRES_THIS[\"context_tk_ev_simple_diff_tk\"].tolist():\n",
    "                        _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple_diff_tk\", x_lev_manip=\"same\", y_lev_manip=\"diff\", var_subplot=\"var_decode\", var_value=\"score_adjusted\", var_datapt=\"bregion\")\n",
    "                        savefig(fig, f\"{savedir}/scatter-same-vs-diff_task_kind.pdf\")\n",
    "                        plt.close(\"all\")\n",
    "                    \n",
    "                    # Separate subplot for each bregion.\n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple\", x_lev_manip=\"same\", y_lev_manip=\"diff\",    var_subplot=\"bregion\", var_value=\"score_adjusted\", var_datapt=\"tk_ev_test\", plot_error_bars = False, shareaxes=True)\n",
    "                    savefig(fig, f\"{savedir}/scatter_by_bregion-same-vs-diff.pdf\")\n",
    "                    \n",
    "                    _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple\", x_lev_manip=\"same\", y_lev_manip=\"diff\",    var_subplot=\"bregion\", var_value=\"score_adjusted\", var_datapt=\"tk_ev_test\", plot_error_bars = False, shareaxes=True)\n",
    "                    savefig(fig, f\"{savedir}/scatter_by_bregion-same-vs-diff-datpt=tk_ev_train.pdf\")\n",
    "                                        \n",
    "                    plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:42:05.508033776Z",
     "start_time": "2024-02-27T21:14:28.778842493Z"
    }
   },
   "id": "33c97d6655d46653"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [TESTING] State space plots (tsne)\n",
    "NOTE: previously have dPCA plots..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71caf94b369f4732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f94d98653399b13e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
