{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'\"\\nDevo code for all kinds of decoding analyses, inclduing:\\n[NOTE: all are time-resolved]\\n- default\\n- cross-generalization (at each time point)\\n- generalization over time\\n--- variation: single-trial covert.\\n- shared subspace (single decoder over time)\\n\\nInspired to do this mainly re: quyestion of evidence for single trial shape decoding during palnning, espeically for chars.\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"\"\n",
    "Devo code for all kinds of decoding analyses, inclduing:\n",
    "[NOTE: all are time-resolved]\n",
    "- default\n",
    "- cross-generalization (at each time point)\n",
    "- generalization over time\n",
    "--- variation: single-trial covert.\n",
    "- shared subspace (single decoder over time)\n",
    "\n",
    "Inspired to do this mainly re: quyestion of evidence for single trial shape decoding during palnning, espeically for chars.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T19:45:54.050210377Z",
     "start_time": "2024-03-08T19:45:53.914063445Z"
    }
   },
   "id": "9e3fd17c4f13ba43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a pre-computed Snippets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e64d1e318d5757d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "animal = \"Diego\"\n",
    "date = 230628\n",
    "# question = \"SP_shape_loc\"\n",
    "question = \"PIG_planning_shape_loc\"\n",
    "list_time_windows = [(-0.6, 0.6)]\n",
    "events_keep = [\"03_samp\", \"04_go_cue\", \"06_on_strokeidx_0\"]\n",
    "combine_into_larger_areas = True\n",
    "HACK_RENAME_SHAPES = False\n",
    "DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                events_keep = events_keep,\n",
    "                                HACK_RENAME_SHAPES=HACK_RENAME_SHAPES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b6e8425ba4d829d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a dataset (saved for Xuan)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f471c575413735b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load and plot a dataset of neural activity across population, in a PopAnal class object.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401a9851a443292c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T19:45:56.247009686Z",
     "start_time": "2024-03-08T19:45:56.098113308Z"
    }
   },
   "id": "6d72addf5c1494c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# this is the path to the dataset\n",
    "# path = '/gorilla1/analyses/recordings/main/RSA/Diego-230615/agg_True-subtr_None-dist_euclidian_unbiased/SP_shape_loc/DFallpa.pkl'\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_samp_and_stroke.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_pig_planning.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_pig_concat_trial_and_stroke_which_levels.pkl\"\n",
    "# path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_char_concat_trial_and_stroke_Pancho_230126.pkl\"\n",
    "path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_char_trial_Pancho_230126.pkl\"\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "320be7d5ecbddf5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa = pd.read_pickle(path)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "335e4da034da5b2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### To save"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb640a16cc1bc17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save it\n",
    "import pickle\n",
    "path = \"/gorilla4/Dropbox/SCIENCE/FREIWALD_LAB/DATA/for_xuan/DFallpa_char_trial_Pancho_230126.pkl\"\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(DFallpa, f)\n",
    "print(\"Saved to:\", path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c796042c0d19d1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a DFallpa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122e1e23ec6b587"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230630*/**\n",
      "Found this many paths:\n",
      "1\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230630/Diego-230630-124955\n",
      "session:  0\n",
      "Beh Sessions that exist on this date:  {230630: [(1, 'primsingridfixed6fDiego')]}\n",
      "------------------------------\n",
      "Loading this neural session: 0\n",
      "Loading these beh expts: ['primsingridfixed6fDiego']\n",
      "Loading these beh sessions: [1]\n",
      "Using this beh_trial_map_list: [(1, 0)]\n",
      "Searching using this string:\n",
      "/mnt/Freiwald/ltian/recordings/*Diego*/*230630*/**\n",
      "Found this many paths:\n",
      "1\n",
      "---\n",
      "/mnt/Freiwald/ltian/recordings/Diego/230630/Diego-230630-124955\n",
      "{'filename_components_hyphened': ['Diego', '230630', '124955'], 'basedirs': ['/mnt/Freiwald/ltian/recordings/Diego', '/mnt/Freiwald/ltian/recordings/Diego/230630'], 'basedirs_filenames': ['230630', 'Diego-230630-124955'], 'filename_final_ext': 'Diego-230630-124955', 'filename_final_noext': 'Diego-230630-124955'}\n",
      "FOund this path for spikes:  /mnt/Freiwald/ltian/recordings/Diego/230630/Diego-230630-124955/spikes_tdt_quick-4\n",
      "== PATHS for this expt: \n",
      "raws  --  /mnt/Freiwald/ltian/recordings/Diego/230630/Diego-230630-124955\n",
      "tank  --  /mnt/Freiwald/ltian/recordings/Diego/230630/Diego-230630-124955/Diego-230630-124955\n",
      "spikes  --  /mnt/Freiwald/ltian/recordings/Diego/230630/Diego-230630-124955/spikes_tdt_quick-4\n",
      "final_dir_name  --  Diego-230630-124955\n",
      "time  --  124955\n",
      "pathbase_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955\n",
      "tank_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/data_tank.pkl\n",
      "spikes_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/data_spikes.pkl\n",
      "datall_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/data_datall.pkl\n",
      "events_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/events_photodiode.pkl\n",
      "mapper_st2dat_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/mapper_st2dat.pkl\n",
      "figs_local  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/figs\n",
      "metadata_units  --  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego\n",
      "cached_dir  --  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/cached\n",
      "Found! metada path :  /home/lucast4/code/neuralmonkey/neuralmonkey/metadat/units_Diego/230630.yaml\n",
      "updating self.SitesDirty with:  ('sites_garbage', 'sites_error_spikes', 'sites_low_spk_magn')\n",
      "[_sitesdirty_update] skipping! since did not find:  sites_error_spikes\n",
      "Printing whether spikes gotten (o) or not (-) because of spike peak to trough\n",
      "== Loading TDT tank\n",
      "** Loading tank data from local (previusly cached)\n",
      "== Done\n",
      "== Trying to load events data\n",
      "Loading this events (pd) locally to:  /gorilla1/neural_preprocess/recordings/Diego/230630/Diego-230630-124955/events_photodiode.pkl\n",
      "== Done\n",
      "** MINIMAL_LOADING, therefore loading previuosly cached data\n",
      "=== CLEANING UP self.Dat (_cleanup_reloading_saved_state) ===== \n",
      "0 _behclass_alignsim_compute\n",
      "200 _behclass_alignsim_compute\n",
      "400 _behclass_alignsim_compute\n",
      "600 _behclass_alignsim_compute\n",
      "Running D._behclass_tokens_extract_datsegs\n",
      "0 _behclass_tokens_extract_datsegs\n",
      "200 _behclass_tokens_extract_datsegs\n",
      "400 _behclass_tokens_extract_datsegs\n",
      "600 _behclass_tokens_extract_datsegs\n",
      "stored in self.Dat[BehClass]\n",
      "Removing these trials: \n",
      "[]\n",
      "self.Dat starting legnth:  648\n",
      "Modified self.Dat, keeping only the inputted inds\n",
      "self.Dat final legnth:  648\n",
      "- starting/ending len (grouping params):\n",
      "648\n",
      "648\n",
      "- starting/ending len (getting sequence):\n",
      "648\n",
      "648\n",
      "--- Removing nans\n",
      "start len: 648\n",
      "- num names for each col\n",
      "not removing nans, since columns=[]\n",
      "ADded new column: supervision_online\n",
      "Reassigned train/test, using key: probe\n",
      "and values:\n",
      "Train =  [0]\n",
      "Test =  [1]\n",
      " \n",
      "New distribution of train/test:\n",
      "train    648\n",
      "Name: monkey_train_or_test, dtype: int64\n",
      "Appended column: los_info\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_SEQUENCE_ALPHA]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "Appended self.Dat[superv_VISUALFB_METH]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_new\n",
      "[taskgroup_reassign_by_mapper], reassigned values in column: taskgroup\n",
      "GROUPING epoch\n",
      "GROUPING_LEVELS ['230630']\n",
      "FEATURE_NAMES ['hdoffline', 'num_strokes_beh', 'num_strokes_task', 'circ', 'dist']\n",
      "SCORE_COL_NAMES []\n",
      "appended col to self.Dat:\n",
      "date_epoch\n",
      "Appended self.Dat[superv_SEQUENCE_SUP]\n",
      "Appended self.Dat[superv_COLOR_ON]\n",
      "Appended self.Dat[superv_COLOR_METHOD]\n",
      "Appended self.Dat[superv_COLOR_ITEMS_FADE_TO_DEFAULT_BINSTR]\n",
      "Appended self.Dat[superv_GUIDEDYN_ON]\n",
      "appended col to self.Dat:\n",
      "supervision_stage_concise\n",
      "Append column to self.Dat:  supervision_stage_semantic\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "... Generated these...\n",
      "self.BehTrialMapList [(1, 0)]\n",
      "self.BehTrialMapListGood {0: (0, 1), 1: (0, 2), 2: (0, 3), 3: (0, 4), 4: (0, 5), 5: (0, 6), 6: (0, 7), 7: (0, 8), 8: (0, 9), 9: (0, 10), 10: (0, 11), 11: (0, 12), 12: (0, 13), 13: (0, 14), 14: (0, 15), 15: (0, 16), 16: (0, 17), 17: (0, 18), 18: (0, 19), 19: (0, 20), 20: (0, 21), 21: (0, 22), 22: (0, 23), 23: (0, 24), 24: (0, 25), 25: (0, 26), 26: (0, 27), 27: (0, 28), 28: (0, 29), 29: (0, 30), 30: (0, 31), 31: (0, 32), 32: (0, 33), 33: (0, 34), 34: (0, 35), 35: (0, 36), 36: (0, 37), 37: (0, 38), 38: (0, 39), 39: (0, 40), 40: (0, 41), 41: (0, 42), 42: (0, 43), 43: (0, 44), 44: (0, 45), 45: (0, 46), 46: (0, 47), 47: (0, 48), 48: (0, 49), 49: (0, 50), 50: (0, 51), 51: (0, 52), 52: (0, 53), 53: (0, 54), 54: (0, 55), 55: (0, 56), 56: (0, 57), 57: (0, 58), 58: (0, 59), 59: (0, 60), 60: (0, 61), 61: (0, 62), 62: (0, 63), 63: (0, 64), 64: (0, 65), 65: (0, 66), 66: (0, 67), 67: (0, 68), 68: (0, 69), 69: (0, 70), 70: (0, 71), 71: (0, 72), 72: (0, 73), 73: (0, 74), 74: (0, 75), 75: (0, 76), 76: (0, 77), 77: (0, 78), 78: (0, 79), 79: (0, 80), 80: (0, 81), 81: (0, 82), 82: (0, 83), 83: (0, 84), 84: (0, 85), 85: (0, 86), 86: (0, 87), 87: (0, 88), 88: (0, 89), 89: (0, 90), 90: (0, 91), 91: (0, 92), 92: (0, 93), 93: (0, 94), 94: (0, 95), 95: (0, 96), 96: (0, 97), 97: (0, 98), 98: (0, 99), 99: (0, 100), 100: (0, 101), 101: (0, 102), 102: (0, 103), 103: (0, 104), 104: (0, 105), 105: (0, 106), 106: (0, 107), 107: (0, 108), 108: (0, 109), 109: (0, 110), 110: (0, 111), 111: (0, 112), 112: (0, 113), 113: (0, 114), 114: (0, 115), 115: (0, 116), 116: (0, 117), 117: (0, 118), 118: (0, 119), 119: (0, 120), 120: (0, 121), 121: (0, 122), 122: (0, 123), 123: (0, 124), 124: (0, 125), 125: (0, 126), 126: (0, 127), 127: (0, 128), 128: (0, 129), 129: (0, 130), 130: (0, 131), 131: (0, 132), 132: (0, 133), 133: (0, 134), 134: (0, 135), 135: (0, 136), 136: (0, 137), 137: (0, 138), 138: (0, 139), 139: (0, 140), 140: (0, 141), 141: (0, 142), 142: (0, 143), 143: (0, 144), 144: (0, 145), 145: (0, 146), 146: (0, 147), 147: (0, 148), 148: (0, 149), 149: (0, 150), 150: (0, 151), 151: (0, 152), 152: (0, 153), 153: (0, 154), 154: (0, 155), 155: (0, 156), 156: (0, 157), 157: (0, 158), 158: (0, 159), 159: (0, 160), 160: (0, 161), 161: (0, 162), 162: (0, 163), 163: (0, 164), 164: (0, 165), 165: (0, 166), 166: (0, 167), 167: (0, 168), 168: (0, 169), 169: (0, 170), 170: (0, 171), 171: (0, 172), 172: (0, 173), 173: (0, 174), 174: (0, 175), 175: (0, 176), 176: (0, 177), 177: (0, 178), 178: (0, 179), 179: (0, 180), 180: (0, 181), 181: (0, 182), 182: (0, 183), 183: (0, 184), 184: (0, 185), 185: (0, 186), 186: (0, 187), 187: (0, 188), 188: (0, 189), 189: (0, 190), 190: (0, 191), 191: (0, 192), 192: (0, 193), 193: (0, 194), 194: (0, 195), 195: (0, 196), 196: (0, 197), 197: (0, 198), 198: (0, 199), 199: (0, 200), 200: (0, 201), 201: (0, 202), 202: (0, 203), 203: (0, 204), 204: (0, 205), 205: (0, 206), 206: (0, 207), 207: (0, 208), 208: (0, 209), 209: (0, 210), 210: (0, 211), 211: (0, 212), 212: (0, 213), 213: (0, 214), 214: (0, 215), 215: (0, 216), 216: (0, 217), 217: (0, 218), 218: (0, 219), 219: (0, 220), 220: (0, 221), 221: (0, 222), 222: (0, 223), 223: (0, 224), 224: (0, 225), 225: (0, 226), 226: (0, 227), 227: (0, 228), 228: (0, 229), 229: (0, 230), 230: (0, 231), 231: (0, 232), 232: (0, 233), 233: (0, 234), 234: (0, 235), 235: (0, 236), 236: (0, 237), 237: (0, 238), 238: (0, 239), 239: (0, 240), 240: (0, 241), 241: (0, 242), 242: (0, 243), 243: (0, 244), 244: (0, 245), 245: (0, 246), 246: (0, 247), 247: (0, 248), 248: (0, 249), 249: (0, 250), 250: (0, 251), 251: (0, 252), 252: (0, 253), 253: (0, 254), 254: (0, 255), 255: (0, 256), 256: (0, 257), 257: (0, 258), 258: (0, 259), 259: (0, 260), 260: (0, 261), 261: (0, 262), 262: (0, 263), 263: (0, 264), 264: (0, 265), 265: (0, 266), 266: (0, 267), 267: (0, 268), 268: (0, 269), 269: (0, 270), 270: (0, 271), 271: (0, 272), 272: (0, 273), 273: (0, 274), 274: (0, 275), 275: (0, 276), 276: (0, 277), 277: (0, 278), 278: (0, 279), 279: (0, 280), 280: (0, 281), 281: (0, 282), 282: (0, 283), 283: (0, 284), 284: (0, 285), 285: (0, 286), 286: (0, 287), 287: (0, 288), 288: (0, 289), 289: (0, 290), 290: (0, 291), 291: (0, 292), 292: (0, 293), 293: (0, 294), 294: (0, 295), 295: (0, 296), 296: (0, 297), 297: (0, 298), 298: (0, 299), 299: (0, 300), 300: (0, 301), 301: (0, 302), 302: (0, 303), 303: (0, 304), 304: (0, 305), 305: (0, 306), 306: (0, 307), 307: (0, 308), 308: (0, 309), 309: (0, 310), 310: (0, 311), 311: (0, 312), 312: (0, 313), 313: (0, 314), 314: (0, 315), 315: (0, 316), 316: (0, 317), 317: (0, 318), 318: (0, 319), 319: (0, 320), 320: (0, 321), 321: (0, 322), 322: (0, 323), 323: (0, 324), 324: (0, 325), 325: (0, 326), 326: (0, 327), 327: (0, 328), 328: (0, 329), 329: (0, 330), 330: (0, 331), 331: (0, 332), 332: (0, 333), 333: (0, 334), 334: (0, 335), 335: (0, 336), 336: (0, 337), 337: (0, 338), 338: (0, 339), 339: (0, 340), 340: (0, 341), 341: (0, 342), 342: (0, 343), 343: (0, 344), 344: (0, 345), 345: (0, 346), 346: (0, 347), 347: (0, 348), 348: (0, 349), 349: (0, 350), 350: (0, 351), 351: (0, 352), 352: (0, 353), 353: (0, 354), 354: (0, 355), 355: (0, 356), 356: (0, 357), 357: (0, 358), 358: (0, 359), 359: (0, 360), 360: (0, 361), 361: (0, 362), 362: (0, 363), 363: (0, 364), 364: (0, 365), 365: (0, 366), 366: (0, 367), 367: (0, 368), 368: (0, 369), 369: (0, 370), 370: (0, 371), 371: (0, 372), 372: (0, 373), 373: (0, 374), 374: (0, 375), 375: (0, 376), 376: (0, 377), 377: (0, 378), 378: (0, 379), 379: (0, 380), 380: (0, 381), 381: (0, 382), 382: (0, 383), 383: (0, 384), 384: (0, 385), 385: (0, 386), 386: (0, 387), 387: (0, 388), 388: (0, 389), 389: (0, 390), 390: (0, 391), 391: (0, 392), 392: (0, 393), 393: (0, 394), 394: (0, 395), 395: (0, 396), 396: (0, 397), 397: (0, 398), 398: (0, 399), 399: (0, 400), 400: (0, 401), 401: (0, 402), 402: (0, 403), 403: (0, 404), 404: (0, 405), 405: (0, 406), 406: (0, 407), 407: (0, 408), 408: (0, 409), 409: (0, 410), 410: (0, 411), 411: (0, 412), 412: (0, 413), 413: (0, 414), 414: (0, 415), 415: (0, 416), 416: (0, 417), 417: (0, 418), 418: (0, 419), 419: (0, 420), 420: (0, 421), 421: (0, 422), 422: (0, 423), 423: (0, 424), 424: (0, 425), 425: (0, 426), 426: (0, 427), 427: (0, 428), 428: (0, 429), 429: (0, 430), 430: (0, 431), 431: (0, 432), 432: (0, 433), 433: (0, 434), 434: (0, 435), 435: (0, 436), 436: (0, 437), 437: (0, 438), 438: (0, 439), 439: (0, 440), 440: (0, 441), 441: (0, 442), 442: (0, 443), 443: (0, 444), 444: (0, 445), 445: (0, 446), 446: (0, 447), 447: (0, 448), 448: (0, 449), 449: (0, 450), 450: (0, 451), 451: (0, 452), 452: (0, 453), 453: (0, 454), 454: (0, 455), 455: (0, 456), 456: (0, 457), 457: (0, 458), 458: (0, 459), 459: (0, 460), 460: (0, 461), 461: (0, 462), 462: (0, 463), 463: (0, 464), 464: (0, 465), 465: (0, 466), 466: (0, 467), 467: (0, 468), 468: (0, 469), 469: (0, 470), 470: (0, 471), 471: (0, 472), 472: (0, 473), 473: (0, 474), 474: (0, 475), 475: (0, 476), 476: (0, 477), 477: (0, 478), 478: (0, 479), 479: (0, 480), 480: (0, 481), 481: (0, 482), 482: (0, 483), 483: (0, 484), 484: (0, 485), 485: (0, 486), 486: (0, 487), 487: (0, 488), 488: (0, 489), 489: (0, 490), 490: (0, 491), 491: (0, 492), 492: (0, 493), 493: (0, 494), 494: (0, 495), 495: (0, 496), 496: (0, 497), 497: (0, 498), 498: (0, 499), 499: (0, 500), 500: (0, 501), 501: (0, 502), 502: (0, 503), 503: (0, 504), 504: (0, 505), 505: (0, 506), 506: (0, 507), 507: (0, 508), 508: (0, 509), 509: (0, 510), 510: (0, 511), 511: (0, 512), 512: (0, 513), 513: (0, 514), 514: (0, 515), 515: (0, 516), 516: (0, 517), 517: (0, 518), 518: (0, 519), 519: (0, 520), 520: (0, 521), 521: (0, 522), 522: (0, 523), 523: (0, 524), 524: (0, 525), 525: (0, 526), 526: (0, 527), 527: (0, 528), 528: (0, 529), 529: (0, 530), 530: (0, 531), 531: (0, 532), 532: (0, 533), 533: (0, 534), 534: (0, 535), 535: (0, 536), 536: (0, 537), 537: (0, 538), 538: (0, 539), 539: (0, 540), 540: (0, 541), 541: (0, 542), 542: (0, 543), 543: (0, 544), 544: (0, 545), 545: (0, 546), 546: (0, 547), 547: (0, 548), 548: (0, 549), 549: (0, 550), 550: (0, 551), 551: (0, 552), 552: (0, 553), 553: (0, 554), 554: (0, 555), 555: (0, 556), 556: (0, 557), 557: (0, 558), 558: (0, 559), 559: (0, 560), 560: (0, 561), 561: (0, 562), 562: (0, 563), 563: (0, 564), 564: (0, 565), 565: (0, 566), 566: (0, 567), 567: (0, 568), 568: (0, 569), 569: (0, 570), 570: (0, 571), 571: (0, 572), 572: (0, 573), 573: (0, 574), 574: (0, 575), 575: (0, 576), 576: (0, 577), 577: (0, 578), 578: (0, 579), 579: (0, 580), 580: (0, 581), 581: (0, 582), 582: (0, 583), 583: (0, 584), 584: (0, 585), 585: (0, 586), 586: (0, 587), 587: (0, 588), 588: (0, 589), 589: (0, 590), 590: (0, 591), 591: (0, 592), 592: (0, 593), 593: (0, 594), 594: (0, 595), 595: (0, 596), 596: (0, 597), 597: (0, 598), 598: (0, 599), 599: (0, 600), 600: (0, 601), 601: (0, 602), 602: (0, 603), 603: (0, 604), 604: (0, 605), 605: (0, 606), 606: (0, 607), 607: (0, 608), 608: (0, 609), 609: (0, 610), 610: (0, 611), 611: (0, 612), 612: (0, 613), 613: (0, 614), 614: (0, 615), 615: (0, 616), 616: (0, 617), 617: (0, 618), 618: (0, 619), 619: (0, 620), 620: (0, 621), 621: (0, 622), 622: (0, 623), 623: (0, 624), 624: (0, 625), 625: (0, 626), 626: (0, 627), 627: (0, 628), 628: (0, 629), 629: (0, 630), 630: (0, 631), 631: (0, 632), 632: (0, 633), 633: (0, 634), 634: (0, 635), 635: (0, 636), 636: (0, 637), 637: (0, 638), 638: (0, 639), 639: (0, 640), 640: (0, 641), 641: (0, 642), 642: (0, 643), 643: (0, 644), 644: (0, 645), 645: (0, 646), 646: (0, 647), 647: (0, 648), 648: (0, 649), 649: (0, 650), 650: (0, 651), 651: (0, 652), 652: (0, 653), 653: (0, 654), 654: (0, 655), 655: (0, 656), 656: (0, 657), 657: (0, 658), 658: (0, 659), 659: (0, 660), 660: (0, 661), 661: (0, 662), 662: (0, 663), 663: (0, 664), 664: (0, 665), 665: (0, 666), 666: (0, 667), 667: (0, 668), 668: (0, 669), 669: (0, 670), 670: (0, 671), 671: (0, 672), 672: (0, 673), 673: (0, 674), 674: (0, 675), 675: (0, 676), 676: (0, 677), 677: (0, 678), 678: (0, 679), 679: (0, 680), 680: (0, 681), 681: (0, 682), 682: (0, 683), 683: (0, 684), 684: (0, 685), 685: (0, 686), 686: (0, 687), 687: (0, 688), 688: (0, 689), 689: (0, 690), 690: (0, 691), 691: (0, 692), 692: (0, 693), 693: (0, 694), 694: (0, 695), 695: (0, 696), 696: (0, 697), 697: (0, 698), 698: (0, 699), 699: (0, 700), 700: (0, 701), 701: (0, 702), 702: (0, 703), 703: (0, 704), 704: (0, 705), 705: (0, 706), 706: (0, 707), 707: (0, 708), 708: (0, 709), 709: (0, 710), 710: (0, 711), 711: (0, 712), 712: (0, 713), 713: (0, 714), 714: (0, 715), 715: (0, 716), 716: (0, 717), 717: (0, 718), 718: (0, 719), 719: (0, 720), 720: (0, 721), 721: (0, 722), 722: (0, 723), 723: (0, 724), 724: (0, 725), 725: (0, 726), 726: (0, 727), 727: (0, 728), 728: (0, 729), 729: (0, 730), 730: (0, 731), 731: (0, 732), 732: (0, 733), 733: (0, 734), 734: (0, 735), 735: (0, 736), 736: (0, 737), 737: (0, 738), 738: (0, 739), 739: (0, 740), 740: (0, 741), 741: (0, 742), 742: (0, 743), 743: (0, 744), 744: (0, 745), 745: (0, 746), 746: (0, 747), 747: (0, 748), 748: (0, 749), 749: (0, 750), 750: (0, 751), 751: (0, 752), 752: (0, 753), 753: (0, 754), 754: (0, 755), 755: (0, 756), 756: (0, 757), 757: (0, 758), 758: (0, 759), 759: (0, 760), 760: (0, 761), 761: (0, 762), 762: (0, 763), 763: (0, 764), 764: (0, 765), 765: (0, 766), 766: (0, 767), 767: (0, 768), 768: (0, 769), 769: (0, 770), 770: (0, 771), 771: (0, 772), 772: (0, 773), 773: (0, 774), 774: (0, 775), 775: (0, 776), 776: (0, 777), 777: (0, 778), 778: (0, 779), 779: (0, 780), 780: (0, 781), 781: (0, 782), 782: (0, 783), 783: (0, 784), 784: (0, 785), 785: (0, 786), 786: (0, 787), 787: (0, 788), 788: (0, 789), 789: (0, 790), 790: (0, 791), 791: (0, 792), 792: (0, 793), 793: (0, 794), 794: (0, 795), 795: (0, 796), 796: (0, 797), 797: (0, 798), 798: (0, 799), 799: (0, 800), 800: (0, 801), 801: (0, 802), 802: (0, 803), 803: (0, 804), 804: (0, 805), 805: (0, 806), 806: (0, 807), 807: (0, 808), 808: (0, 809), 809: (0, 810), 810: (0, 811), 811: (0, 812), 812: (0, 813), 813: (0, 814), 814: (0, 815), 815: (0, 816), 816: (0, 817), 817: (0, 818), 818: (0, 819), 819: (0, 820), 820: (0, 821), 821: (0, 822), 822: (0, 823), 823: (0, 824), 824: (0, 825), 825: (0, 826), 826: (0, 827), 827: (0, 828), 828: (0, 829), 829: (0, 830), 830: (0, 831), 831: (0, 832), 832: (0, 833), 833: (0, 834), 834: (0, 835), 835: (0, 836), 836: (0, 837), 837: (0, 838), 838: (0, 839), 839: (0, 840), 840: (0, 841), 841: (0, 842), 842: (0, 843), 843: (0, 844), 844: (0, 845), 845: (0, 846), 846: (0, 847), 847: (0, 848), 848: (0, 849), 849: (0, 850), 850: (0, 851), 851: (0, 852), 852: (0, 853), 853: (0, 854), 854: (0, 855), 855: (0, 856), 856: (0, 857), 857: (0, 858), 858: (0, 859), 859: (0, 860), 860: (0, 861), 861: (0, 862), 862: (0, 863), 863: (0, 864), 864: (0, 865), 865: (0, 866), 866: (0, 867), 867: (0, 868), 868: (0, 869), 869: (0, 870), 870: (0, 871), 871: (0, 872), 872: (0, 873), 873: (0, 874), 874: (0, 875), 875: (0, 876), 876: (0, 877), 877: (0, 878), 878: (0, 879), 879: (0, 880), 880: (0, 881), 881: (0, 882), 882: (0, 883), 883: (0, 884), 884: (0, 885), 885: (0, 886), 886: (0, 887), 887: (0, 888), 888: (0, 889), 889: (0, 890), 890: (0, 891), 891: (0, 892), 892: (0, 893), 893: (0, 894), 894: (0, 895), 895: (0, 896), 896: (0, 897), 897: (0, 898), 898: (0, 899), 899: (0, 900), 900: (0, 901), 901: (0, 902), 902: (0, 903), 903: (0, 904), 904: (0, 905), 905: (0, 906), 906: (0, 907), 907: (0, 908), 908: (0, 909), 909: (0, 910), 910: (0, 911), 911: (0, 912), 912: (0, 913), 913: (0, 914), 914: (0, 915), 915: (0, 916), 916: (0, 917), 917: (0, 918), 918: (0, 919), 919: (0, 920), 920: (0, 921), 921: (0, 922), 922: (0, 923), 923: (0, 924), 924: (0, 925), 925: (0, 926), 926: (0, 927), 927: (0, 928), 928: (0, 929), 929: (0, 930), 930: (0, 931), 931: (0, 932), 932: (0, 933), 933: (0, 934), 934: (0, 935), 935: (0, 936), 936: (0, 937), 937: (0, 938), 938: (0, 939), 939: (0, 940), 940: (0, 941), 941: (0, 942), 942: (0, 943), 943: (0, 944), 944: (0, 945), 945: (0, 946), 946: (0, 947), 947: (0, 948), 948: (0, 949), 949: (0, 950), 950: (0, 951), 951: (0, 952), 952: (0, 953), 953: (0, 954), 954: (0, 955), 955: (0, 956), 956: (0, 957), 957: (0, 958), 958: (0, 959), 959: (0, 960), 960: (0, 961), 961: (0, 962), 962: (0, 963), 963: (0, 964), 964: (0, 965), 965: (0, 966), 966: (0, 967), 967: (0, 968), 968: (0, 969), 969: (0, 970), 970: (0, 971), 971: (0, 972), 972: (0, 973), 973: (0, 974), 974: (0, 975), 975: (0, 976), 976: (0, 977), 977: (0, 978), 978: (0, 979), 979: (0, 980), 980: (0, 981), 981: (0, 982), 982: (0, 983), 983: (0, 984), 984: (0, 985), 985: (0, 986), 986: (0, 987), 987: (0, 988), 988: (0, 989), 989: (0, 990), 990: (0, 991), 991: (0, 992), 992: (0, 993), 993: (0, 994), 994: (0, 995), 995: (0, 996), 996: (0, 997), 997: (0, 998), 998: (0, 999), 999: (0, 1000), 1000: (0, 1001), 1001: (0, 1002), 1002: (0, 1003), 1003: (0, 1004), 1004: (0, 1005), 1005: (0, 1006), 1006: (0, 1007), 1007: (0, 1008), 1008: (0, 1009), 1009: (0, 1010), 1010: (0, 1011), 1011: (0, 1012), 1012: (0, 1013), 1013: (0, 1014), 1014: (0, 1015), 1015: (0, 1016), 1016: (0, 1017), 1017: (0, 1018), 1018: (0, 1019), 1019: (0, 1020), 1020: (0, 1021), 1021: (0, 1022), 1022: (0, 1023), 1023: (0, 1024), 1024: (0, 1025), 1025: (0, 1026), 1026: (0, 1027), 1027: (0, 1028), 1028: (0, 1029), 1029: (0, 1030), 1030: (0, 1031), 1031: (0, 1032), 1032: (0, 1033), 1033: (0, 1034), 1034: (0, 1035), 1035: (0, 1036), 1036: (0, 1037), 1037: (0, 1038), 1038: (0, 1039), 1039: (0, 1040), 1040: (0, 1041), 1041: (0, 1042), 1042: (0, 1043), 1043: (0, 1044), 1044: (0, 1045), 1045: (0, 1046), 1046: (0, 1047), 1047: (0, 1048), 1048: (0, 1049), 1049: (0, 1050), 1050: (0, 1051), 1051: (0, 1052), 1052: (0, 1053), 1053: (0, 1054), 1054: (0, 1055), 1055: (0, 1056), 1056: (0, 1057), 1057: (0, 1058), 1058: (0, 1059), 1059: (0, 1060), 1060: (0, 1061), 1061: (0, 1062), 1062: (0, 1063), 1063: (0, 1064), 1064: (0, 1065), 1065: (0, 1066), 1066: (0, 1067), 1067: (0, 1068), 1068: (0, 1069), 1069: (0, 1070), 1070: (0, 1071), 1071: (0, 1072), 1072: (0, 1073), 1073: (0, 1074), 1074: (0, 1075), 1075: (0, 1076), 1076: (0, 1077), 1077: (0, 1078), 1078: (0, 1079), 1079: (0, 1080), 1080: (0, 1081), 1081: (0, 1082), 1082: (0, 1083), 1083: (0, 1084), 1084: (0, 1085), 1085: (0, 1086), 1086: (0, 1087), 1087: (0, 1088), 1088: (0, 1089), 1089: (0, 1090), 1090: (0, 1091), 1091: (0, 1092), 1092: (0, 1093), 1093: (0, 1094), 1094: (0, 1095), 1095: (0, 1096), 1096: (0, 1097), 1097: (0, 1098), 1098: (0, 1099), 1099: (0, 1100), 1100: (0, 1101), 1101: (0, 1102), 1102: (0, 1103), 1103: (0, 1104), 1104: (0, 1105), 1105: (0, 1106), 1106: (0, 1107), 1107: (0, 1108), 1108: (0, 1109), 1109: (0, 1110), 1110: (0, 1111), 1111: (0, 1112), 1112: (0, 1113), 1113: (0, 1114), 1114: (0, 1115), 1115: (0, 1116), 1116: (0, 1117), 1117: (0, 1118), 1118: (0, 1119), 1119: (0, 1120), 1120: (0, 1121), 1121: (0, 1122), 1122: (0, 1123), 1123: (0, 1124), 1124: (0, 1125), 1125: (0, 1126), 1126: (0, 1127), 1127: (0, 1128), 1128: (0, 1129), 1129: (0, 1130), 1130: (0, 1131), 1131: (0, 1132), 1132: (0, 1133), 1133: (0, 1134), 1134: (0, 1135), 1135: (0, 1136), 1136: (0, 1137), 1137: (0, 1138), 1138: (0, 1139), 1139: (0, 1140), 1140: (0, 1141), 1141: (0, 1142), 1142: (0, 1143), 1143: (0, 1144), 1144: (0, 1145), 1145: (0, 1146), 1146: (0, 1147), 1147: (0, 1148), 1148: (0, 1149), 1149: (0, 1150), 1150: (0, 1151), 1151: (0, 1152), 1152: (0, 1153), 1153: (0, 1154), 1154: (0, 1155)}\n",
      "Generated self._MapperTrialcode2TrialToTrial!\n",
      "Extracted into self.Dat[epoch_orig]\n",
      "Extracted successfully for session:  0\n",
      "Generated index mappers!\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/DfScalar.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/fr_sm_times.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/DS.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/Params.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/ParamsGlobals.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/Sites.pkl\n",
      "Loading:  /gorilla1/analyses/recordings/main/anova/bytrial/Diego-230630-sess_0/Trials.pkl\n",
      "This many vals across loaded session\n",
      "0 : 2862080\n",
      "Assigning to SP.Params this item:\n",
      "{'which_level': 'trial', '_list_events': ['fixcue', 'fix_touch', 'rulecue2', 'samp', 'go_cue', 'first_raise', 'on_strokeidx_0', 'off_stroke_last', 'doneb', 'post', 'reward_all'], 'list_events_uniqnames': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_features_extraction': [], 'list_features_get_conjunction': [], 'list_pre_dur': [-0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6], 'list_post_dur': [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6], 'map_var_to_othervars': None, 'strokes_only_keep_single': None, 'tasks_only_keep_these': None, 'prune_feature_levels_min_n_trials': 1, 'fr_which_version': 'sqrt', 'SPIKES_VERSION': 'tdt', 'map_var_to_levels': None}\n",
      "Assigning to SP.ParamsGlobals this item:\n",
      "{'n_min_trials_per_level': 5, 'lenient_allow_data_if_has_n_levels': 2, 'PRE_DUR_CALC': -0.6, 'POST_DUR_CALC': 0.6, 'list_events': ['00_fixcue', '01_fix_touch', '02_rulecue2', '03_samp', '04_go_cue', '05_first_raise', '06_on_strokeidx_0', '07_off_stroke_last', '08_doneb', '09_post', '10_reward_all'], 'list_pre_dur': [-0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6, -0.6], 'list_post_dur': [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]}\n",
      "Keeping this many sites that pass fr thresh:\n",
      "416 / 416\n",
      "Using threshold:  1.5\n",
      "Updated self.Sites\n",
      "Question: PIG_BASE_trial\n",
      "These questions params:\n",
      "effect_vars  --  ['seqc_0_shape']\n",
      "exclude_last_stroke  --  False\n",
      "exclude_first_stroke  --  False\n",
      "keep_only_first_stroke  --  False\n",
      "min_taskstrokes  --  1\n",
      "max_taskstrokes  --  20\n",
      "THRESH_clust_sim_max  --  None\n",
      "distmat_animal  --  None\n",
      "distmat_date  --  None\n",
      "distmat_distance_ver  --  None\n",
      "list_which_level  --  ['trial']\n",
      "events_keep  --  None\n",
      "plot_pairwise_distmats_variables  --  None\n",
      "plot_pairwise_distmats_twinds  --  None\n",
      "slice_agg_slices  --  None\n",
      "slice_agg_vars_to_split  --  None\n",
      "list_subtract_mean_each_level_of_var  --  [None]\n",
      "list_vars_test_invariance_over_dict  --  [None]\n",
      "dict_vars_levels_prune  --  None\n",
      "ANALY_VER  --  seqcontext\n",
      "list_time_windows  --  [(-0.5, -0.3), (-0.3, -0.1), (-0.1, 0.1), (0.1, 0.3), (0.3, 0.5)]\n",
      "Done!, new len of dataset 648\n",
      "tests passed\n",
      "- Assigning to D.TokensVersion this value: taskclass\n",
      "These attributes are dicts, which will concat: []\n",
      "Dataset preprocess, these params:\n",
      "{'DO_CHARSEQ_VER': None, 'EXTRACT_EPOCHSETS': False, 'EXTRACT_EPOCHSETS_trial_label': None, 'EXTRACT_EPOCHSETS_n_max_epochs': None, 'EXTRACT_EPOCHSETS_merge_sets': None, 'taskgroup_reassign_simple_neural': False, 'preprocess_steps_append': ['one_to_one_beh_task_strokes_allow_unfinished', 'beh_strokes_at_least_one'], 'remove_aborts': False, 'list_superv_keep': None, 'list_superv_keep_full': None, 'DO_SCORE_SEQUENCE_VER': None, 'list_epoch_merge': [], 'epoch_merge_key': None, 'DO_EXTRACT_EPOCHKIND': False, 'datasetstrokes_extract_to_prune_trial': None, 'datasetstrokes_extract_to_prune_stroke_and_get_features': 'clean_one_to_one', 'substrokes_features_do_extraction': False, 'charclust_dataset_extract_shapes': False}\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gorilla1/code/pythonlib/pythonlib/drawmodel/features.py:183: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return [1-p/t for p,t in zip(displace,distance)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "This many strokes extracted:  1615\n",
      "DONE!\n",
      "Appended epoch to self.Dat\n",
      "Appended character to self.Dat\n",
      "Basis set of strokes: ['Lcentered-4-1-0', 'Lcentered-4-2-0', 'Lcentered-4-3-0', 'Lcentered-4-4-0', 'V-2-2-0', 'V-2-3-0', 'V-2-4-0', 'arcdeep-4-2-0', 'arcdeep-4-3-0', 'arcdeep-4-4-0', 'circle-6-1-0', 'line-8-1-0', 'line-8-2-0', 'line-8-3-0', 'line-8-4-0', 'squiggle3-3-1-0', 'squiggle3-3-1-1', 'squiggle3-3-2-0', 'squiggle3-3-2-1', 'usquare-1-2-0', 'usquare-1-3-0', 'usquare-1-4-0', 'zigzagSq-1-1-0', 'zigzagSq-1-1-1', 'zigzagSq-1-2-0', 'zigzagSq-1-2-1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gorilla1/code/pythonlib/pythonlib/tools/vectools.py:67: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return vector / np.linalg.norm(vector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndims for feature loc_on = 2\n",
      "New colname in tokens: loc_on_binned\n",
      "ndims for feature angle = 1\n",
      "New colname in tokens: angle_binned\n",
      "ndims for feature center = 2\n",
      "New colname in tokens: center_binned\n",
      "ndims for feature center = 2\n",
      "New colname in tokens: center_binned\n",
      "ndims for feature center = 2\n",
      "New colname in tokens: center_binned\n",
      "Appended columns gridsize!\n",
      "Num nan/total, for angle_overall\n",
      "195 / 648\n",
      "Num nan/total, for num_strokes_beh\n",
      "0 / 648\n",
      "Num nan/total, for num_strokes_task\n",
      "0 / 648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gorilla1/code/pythonlib/pythonlib/drawmodel/features.py:183: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return [1-p/t for p,t in zip(displace,distance)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nan/total, for circ\n",
      "7 / 648\n",
      "Num nan/total, for dist\n",
      "0 / 648\n",
      "Added these features:\n",
      "['FEAT_angle_overall', 'FEAT_num_strokes_beh', 'FEAT_num_strokes_task', 'FEAT_circ', 'FEAT_dist']\n",
      "Starting length of D.Dat: 648\n",
      "--BEFORE REMOVE; existing supervision_stage_concise:\n",
      "off|0||1111|0    648\n",
      "Name: supervision_stage_concise, dtype: int64\n",
      "############ TAKING ONLY NO SUPERVISION TRIALS\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['no_supervision']\n",
      "-- Len of D, before applying this param: no_supervision, ... 648\n",
      "after: 648\n",
      "Dataset final len: 648\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['one_to_one_beh_task_strokes_allow_unfinished', 'beh_strokes_at_least_one']\n",
      "-- Len of D, before applying this param: one_to_one_beh_task_strokes_allow_unfinished, ... 648\n",
      "after: 638\n",
      "-- Len of D, before applying this param: beh_strokes_at_least_one, ... 638\n",
      "after: 638\n",
      "*** RUNNING D.preprocessGood using these params:\n",
      "['beh_strokes_at_least_one', 'one_to_one_beh_task_strokes_allow_unfinished', 'no_supervision']\n",
      "-- Len of D, before applying this param: beh_strokes_at_least_one, ... 638\n",
      "after: 638\n",
      "-- Len of D, before applying this param: one_to_one_beh_task_strokes_allow_unfinished, ... 638\n",
      "after: 638\n",
      "-- Len of D, before applying this param: no_supervision, ... 638\n",
      "after: 638\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "This many strokes extracted:  1575\n",
      "DONE!\n",
      "Appended epoch to self.Dat\n",
      "Appended character to self.Dat\n",
      "Added column: dist_beh_task_strok\n",
      "clean_preprocess_data...\n",
      "len of DS.Dat = 1575, before running... stroke_too_short\n",
      "Doing...: stroke_too_short\n",
      "New len:  1575\n",
      "len of DS.Dat = 1575, before running... beh_task_dist_too_large\n",
      "Doing...: beh_task_dist_too_large\n",
      "New len:  1575\n",
      "len of DS.Dat = 1575, before running... stroke_too_quick\n",
      "Doing...: stroke_too_quick\n",
      "New len:  1575\n",
      "clean_preprocess_data...\n",
      "len of DS.Dat = 1575, before running... remove_if_multiple_behstrokes_per_taskstroke\n",
      "This many cases with >1 beh stroke needed to completed a task stroke:  0\n",
      "New len:  1575\n",
      "Starting len dfscalar:  2862080\n",
      "Ending len dfscalar:  2817984\n",
      "Using trial...\n",
      "Attempting to extract these features into Snippets:\n",
      "['seqc_0_shape', 'trialcode', 'aborted', 'event_time', 'task_kind', 'gridsize', 'FEAT_num_strokes_task', 'FEAT_num_strokes_beh', 'character', 'probe', 'supervision_stage_concise', 'epoch_orig', 'epoch', 'taskgroup', 'origin', 'donepos', 'shape_is_novel_all', 'shape_semantic_labels', 'shape_is_novel_list', 'taskconfig_shp', 'taskconfig_shploc', 'taskconfig_loc', 'Tkbeh_stkbeh', 'Tkbeh_stktask', 'Tktask', 'taskconfig_shp_SHSEM', 'taskconfig_shploc_SHSEM', 'seqc_0_shape', 'seqc_0_loc', 'seqc_0_shapesem', 'seqc_0_locon', 'seqc_0_locx', 'seqc_0_locy', 'seqc_0_center_binned', 'seqc_0_locon_binned', 'seqc_0_shapesemcat', 'seqc_0_angle', 'seqc_0_angle_binned', 'seqc_1_shape', 'seqc_1_loc', 'seqc_1_shapesem', 'seqc_1_locon', 'seqc_1_locx', 'seqc_1_locy', 'seqc_1_center_binned', 'seqc_1_locon_binned', 'seqc_1_shapesemcat', 'seqc_1_angle', 'seqc_1_angle_binned', 'seqc_2_shape', 'seqc_2_loc', 'seqc_2_shapesem', 'seqc_2_locon', 'seqc_2_locx', 'seqc_2_locy', 'seqc_2_center_binned', 'seqc_2_locon_binned', 'seqc_2_shapesemcat', 'seqc_2_angle', 'seqc_2_angle_binned', 'seqc_3_shape', 'seqc_3_loc', 'seqc_3_shapesem', 'seqc_3_locon', 'seqc_3_locx', 'seqc_3_locy', 'seqc_3_center_binned', 'seqc_3_locon_binned', 'seqc_3_shapesemcat', 'seqc_3_angle', 'seqc_3_angle_binned', 'seqc_4_shape', 'seqc_4_loc', 'seqc_4_shapesem', 'seqc_4_locon', 'seqc_4_locx', 'seqc_4_locy', 'seqc_4_center_binned', 'seqc_4_locon_binned', 'seqc_4_shapesemcat', 'seqc_4_angle', 'seqc_4_angle_binned', 'seqc_5_shape', 'seqc_5_loc', 'seqc_5_shapesem', 'seqc_5_locon', 'seqc_5_locx', 'seqc_5_locy', 'seqc_5_center_binned', 'seqc_5_locon_binned', 'seqc_5_shapesemcat', 'seqc_5_angle', 'seqc_5_angle_binned', 'seqc_nstrokes_beh', 'seqc_nstrokes_task']\n",
      "Appending...  seqc_0_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_shape\n",
      "Appending...  aborted\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "aborted\n",
      "Appending...  task_kind\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "task_kind\n",
      "Appending...  gridsize\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "gridsize\n",
      "Appending...  FEAT_num_strokes_task\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "FEAT_num_strokes_task\n",
      "Appending...  FEAT_num_strokes_beh\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "FEAT_num_strokes_beh\n",
      "Appending...  character\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "character\n",
      "Appending...  probe\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "probe\n",
      "Appending...  supervision_stage_concise\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "supervision_stage_concise\n",
      "Appending...  epoch_orig\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "epoch_orig\n",
      "Appending...  epoch\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "epoch\n",
      "Appending...  taskgroup\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskgroup\n",
      "Appending...  origin\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "origin\n",
      "Appending...  donepos\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "donepos\n",
      "Appending...  shape_is_novel_all\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "shape_is_novel_all\n",
      "Appending...  shape_semantic_labels\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "shape_semantic_labels\n",
      "Appending...  shape_is_novel_list\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "shape_is_novel_list\n",
      "Appending...  taskconfig_shp\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskconfig_shp\n",
      "Appending...  taskconfig_shploc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskconfig_shploc\n",
      "Appending...  taskconfig_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskconfig_loc\n",
      "Appending...  Tkbeh_stkbeh\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "Tkbeh_stkbeh\n",
      "Appending...  Tkbeh_stktask\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "Tkbeh_stktask\n",
      "Appending...  Tktask\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "Tktask\n",
      "Appending...  taskconfig_shp_SHSEM\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskconfig_shp_SHSEM\n",
      "Appending...  taskconfig_shploc_SHSEM\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "taskconfig_shploc_SHSEM\n",
      "Appending...  seqc_0_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_loc\n",
      "Appending...  seqc_0_shapesem\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_shapesem\n",
      "Appending...  seqc_0_locon\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_locon\n",
      "Appending...  seqc_0_locx\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_locx\n",
      "Appending...  seqc_0_locy\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_locy\n",
      "Appending...  seqc_0_center_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_center_binned\n",
      "Appending...  seqc_0_locon_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_locon_binned\n",
      "Appending...  seqc_0_shapesemcat\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_shapesemcat\n",
      "Appending...  seqc_0_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_angle\n",
      "Appending...  seqc_0_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_0_angle_binned\n",
      "Appending...  seqc_1_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_shape\n",
      "Appending...  seqc_1_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_loc\n",
      "Appending...  seqc_1_shapesem\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_shapesem\n",
      "Appending...  seqc_1_locon\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_locon\n",
      "Appending...  seqc_1_locx\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_locx\n",
      "Appending...  seqc_1_locy\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_locy\n",
      "Appending...  seqc_1_center_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_center_binned\n",
      "Appending...  seqc_1_locon_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_locon_binned\n",
      "Appending...  seqc_1_shapesemcat\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_shapesemcat\n",
      "Appending...  seqc_1_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_angle\n",
      "Appending...  seqc_1_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_1_angle_binned\n",
      "Appending...  seqc_2_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_shape\n",
      "Appending...  seqc_2_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_loc\n",
      "Appending...  seqc_2_shapesem\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_shapesem\n",
      "Appending...  seqc_2_locon\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_locon\n",
      "Appending...  seqc_2_locx\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_locx\n",
      "Appending...  seqc_2_locy\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_locy\n",
      "Appending...  seqc_2_center_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_center_binned\n",
      "Appending...  seqc_2_locon_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_locon_binned\n",
      "Appending...  seqc_2_shapesemcat\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_shapesemcat\n",
      "Appending...  seqc_2_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_angle\n",
      "Appending...  seqc_2_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_2_angle_binned\n",
      "Appending...  seqc_3_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_shape\n",
      "Appending...  seqc_3_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_loc\n",
      "Appending...  seqc_3_shapesem\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_shapesem\n",
      "Appending...  seqc_3_locon\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_locon\n",
      "Appending...  seqc_3_locx\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_locx\n",
      "Appending...  seqc_3_locy\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_locy\n",
      "Appending...  seqc_3_center_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_center_binned\n",
      "Appending...  seqc_3_locon_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_locon_binned\n",
      "Appending...  seqc_3_shapesemcat\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_shapesemcat\n",
      "Appending...  seqc_3_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_angle\n",
      "Appending...  seqc_3_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_3_angle_binned\n",
      "Appending...  seqc_4_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_shape\n",
      "Appending...  seqc_4_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_loc\n",
      "Appending...  seqc_4_shapesem\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_shapesem\n",
      "Appending...  seqc_4_locon\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_locon\n",
      "Appending...  seqc_4_locx\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_locx\n",
      "Appending...  seqc_4_locy\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_locy\n",
      "Appending...  seqc_4_center_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_center_binned\n",
      "Appending...  seqc_4_locon_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_locon_binned\n",
      "Appending...  seqc_4_shapesemcat\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_shapesemcat\n",
      "Appending...  seqc_4_angle\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_angle\n",
      "Appending...  seqc_4_angle_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_4_angle_binned\n",
      "Appending...  seqc_5_shape\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_shape\n",
      "Appending...  seqc_5_loc\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_loc\n",
      "Appending...  seqc_5_shapesem\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_shapesem\n",
      "Appending...  seqc_5_locon\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_locon\n",
      "Appending...  seqc_5_locx\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_locx\n",
      "Appending...  seqc_5_locy\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_locy\n",
      "Appending...  seqc_5_center_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_center_binned\n",
      "Appending...  seqc_5_locon_binned\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_locon_binned\n",
      "Appending...  seqc_5_shapesemcat\n",
      "Updating this column of self.DfScalar with Dataset beh:\n",
      "seqc_5_shapesemcat\n",
      "Failed to find this var: seqc_5_angle\n",
      "Len D: 638\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 75\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mneuralmonkey\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclasses\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopulation_mult\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dfallpa_extraction_load_wrapper\n\u001B[0;32m---> 75\u001B[0m     DFallpa \u001B[38;5;241m=\u001B[39m \u001B[43mdfallpa_extraction_load_wrapper\u001B[49m\u001B[43m(\u001B[49m\u001B[43manimal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlist_time_windows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mwhich_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhich_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevents_keep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevents_keep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mcombine_into_larger_areas\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcombine_into_larger_areas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mexclude_bad_areas\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mexclude_bad_areas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mSPIKES_VERSION\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mSPIKES_VERSION\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mHACK_RENAME_SHAPES\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mHACK_RENAME_SHAPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mdo_fr_normalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/gorilla1/code/neuralmonkey/neuralmonkey/classes/population_mult.py:643\u001B[0m, in \u001B[0;36mdfallpa_extraction_load_wrapper\u001B[0;34m(animal, date, question, list_time_windows, which_level, events_keep, combine_into_larger_areas, exclude_bad_areas, bin_by_time_dur, bin_by_time_slide, slice_agg_slices, slice_agg_vars_to_split, slice_agg_concat_dim, LOAD_FROM_RSA_ANALY, rsa_ver_dist, rsa_subtr, rsa_agg, rsa_invar, SPIKES_VERSION, HACK_RENAME_SHAPES, substrokes_plot_preprocess, strokes_split_into_multiple_pa, do_fr_normalization)\u001B[0m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;66;03m############### PARAMS\u001B[39;00m\n\u001B[1;32m    634\u001B[0m     \u001B[38;5;66;03m# animal = \"Diego\"\u001B[39;00m\n\u001B[1;32m    635\u001B[0m     \u001B[38;5;66;03m# date = 230615\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    640\u001B[0m \n\u001B[1;32m    641\u001B[0m     \u001B[38;5;66;03m## Load Snippets\u001B[39;00m\n\u001B[1;32m    642\u001B[0m     MS \u001B[38;5;241m=\u001B[39m load_mult_session_helper(date, animal, spikes_version\u001B[38;5;241m=\u001B[39mSPIKES_VERSION)\n\u001B[0;32m--> 643\u001B[0m     DFallpa \u001B[38;5;241m=\u001B[39m \u001B[43mdfallpa_extraction_load_wrapper_from_MS\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlist_time_windows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    644\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mwhich_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevents_keep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcombine_into_larger_areas\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mexclude_bad_areas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbin_by_time_dur\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbin_by_time_slide\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    646\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mslice_agg_slices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslice_agg_vars_to_split\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslice_agg_concat_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mHACK_RENAME_SHAPES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubstrokes_plot_preprocess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubstrokes_plot_preprocess\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mstrokes_split_into_multiple_pa\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrokes_split_into_multiple_pa\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m                                                      \u001B[49m\u001B[43mdo_fr_normalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_fr_normalization\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;66;03m# cleanup\u001B[39;00m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;66;03m# for pa in DFallpa.\u001B[39;00m\n\u001B[1;32m    654\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DFallpa\n",
      "File \u001B[0;32m/gorilla1/code/neuralmonkey/neuralmonkey/classes/population_mult.py:500\u001B[0m, in \u001B[0;36mdfallpa_extraction_load_wrapper_from_MS\u001B[0;34m(MS, question, list_time_windows, which_level, events_keep, combine_into_larger_areas, exclude_bad_areas, bin_by_time_dur, bin_by_time_slide, slice_agg_slices, slice_agg_vars_to_split, slice_agg_concat_dim, HACK_RENAME_SHAPES, substrokes_plot_preprocess, strokes_split_into_multiple_pa, do_fr_normalization)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m which_level \u001B[38;5;129;01min\u001B[39;00m q_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlist_which_level\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor else might run into error later.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# Clean up SP and extract features\u001B[39;00m\n\u001B[0;32m--> 500\u001B[0m D, list_features_extraction \u001B[38;5;241m=\u001B[39m \u001B[43mSP\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasetbeh_preprocess_clean_by_expt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43mANALY_VER\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mq_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mANALY_VER\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvars_extract_append\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mq_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meffect_vars\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubstrokes_plot_preprocess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubstrokes_plot_preprocess\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43mHACK_RENAME_SHAPES\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHACK_RENAME_SHAPES\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[38;5;66;03m# Keep only specific events - to make the following faster.\u001B[39;00m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events_keep \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/gorilla1/code/neuralmonkey/neuralmonkey/classes/snippets.py:6911\u001B[0m, in \u001B[0;36mSnippets.datasetbeh_preprocess_clean_by_expt\u001B[0;34m(self, ANALY_VER, vars_extract_append, substrokes_plot_preprocess, HACK_RENAME_SHAPES)\u001B[0m\n\u001B[1;32m   6908\u001B[0m     DS_for_feature_extraction\u001B[38;5;241m.\u001B[39mclean_preprocess_if_reloaded()\n\u001B[1;32m   6910\u001B[0m \u001B[38;5;66;03m# Perform extraction\u001B[39;00m\n\u001B[0;32m-> 6911\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdatasetbeh_append_column_helper(list_features_extraction, D, DS\u001B[38;5;241m=\u001B[39mDS_for_feature_extraction, stop_if_fail\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m==\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;66;03m# Extract all the vars here\u001B[39;00m\n\u001B[1;32m   6913\u001B[0m \u001B[38;5;66;03m# Sanity check that no Nones... Had this issue at one point.\u001B[39;00m\n\u001B[1;32m   6914\u001B[0m \u001B[38;5;66;03m# if \"gridloc\" in self.DfScalar.columns:\u001B[39;00m\n\u001B[1;32m   6915\u001B[0m \u001B[38;5;66;03m#     if sum(self.DfScalar[\"gridloc\"].isna())>0:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   6921\u001B[0m \u001B[38;5;66;03m#         # print(DS_for_feature_extraction.Dat[DS_for_feature_extraction.Dat[\"gridloc\"].isna()])\u001B[39;00m\n\u001B[1;32m   6922\u001B[0m \u001B[38;5;66;03m#         # assert False, \"Fix this above... see what is done in DatStrokes.clean_preprocess_if_reloaded.\"\u001B[39;00m\n\u001B[1;32m   6923\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpythonlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpandastools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m replace_values_with_this\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Char, \n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "# do_combine = True\n",
    "\n",
    "# Single prim, novels\n",
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "animal = \"Diego\"\n",
    "date = 230630\n",
    "do_combine = False\n",
    "\n",
    "if do_combine:\n",
    "    # COMBINE trial and stroke\n",
    "    dir_suffix = \"test\"\n",
    "    question = None\n",
    "    # q_params = None\n",
    "    which_level = None\n",
    "    q_params = {\n",
    "        \"effect_vars\": [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "    }\n",
    "    \n",
    "    combine_trial_and_stroke = True\n",
    "    \n",
    "    # PIG\n",
    "    # question_trial = \"PIG_BASE_trial\"\n",
    "    # question_stroke = \"PIG_BASE_stroke\"\n",
    "    # check_that_locs_match = True\n",
    "    \n",
    "    # CHAR\n",
    "    question_trial = \"CHAR_BASE_trial\"\n",
    "    question_stroke = \"CHAR_BASE_stroke\"\n",
    "    check_that_locs_match = True\n",
    "    check_that_shapes_match = True\n",
    "else:\n",
    "    # DONT COMBINE, use questions.\n",
    "    # question = \"CHAR_BASE_stroke\"\n",
    "    # question = \"CHAR_BASE_trial\"\n",
    "    # question = \"SP_shape_loc\"\n",
    "    # question = \"PIG_BASE_stroke\"\n",
    "    question = \"PIG_BASE_trial\"\n",
    "    combine_trial_and_stroke = False\n",
    "    # which_level = \"stroke\" # Doesnt matter\n",
    "    which_level = \"trial\" # Doesnt matter\n",
    "    dir_suffix = question\n",
    "\n",
    "    # Load q_params\n",
    "    from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "    q_params = rsagood_questions_dict(animal, date, question)[question]\n",
    "\n",
    "############### PARAMS\n",
    "exclude_bad_areas = True\n",
    "SPIKES_VERSION = \"tdt\" # since Snippets not yet extracted for ks\n",
    "combine_into_larger_areas = False\n",
    "HACK_RENAME_SHAPES = False\n",
    "list_time_windows = [(-0.6, 0.6)]\n",
    "events_keep = None\n",
    "\n",
    "########################################## RUN\n",
    "\n",
    "if combine_trial_and_stroke:\n",
    "    from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper_combine_trial_strokes\n",
    "    DFallpa = dfallpa_extraction_load_wrapper_combine_trial_strokes(animal, date, question_trial,\n",
    "                                                                       question_stroke,\n",
    "                                                list_time_windows, events_keep=events_keep,\n",
    "                                               combine_into_larger_areas = combine_into_larger_areas,\n",
    "                                               exclude_bad_areas=exclude_bad_areas,\n",
    "                                                SPIKES_VERSION=\"tdt\",\n",
    "                                                HACK_RENAME_SHAPES = HACK_RENAME_SHAPES,\n",
    "                                               do_fr_normalization=True,\n",
    "                                                    check_that_shapes_match=check_that_shapes_match,\n",
    "                                                check_that_locs_match=check_that_locs_match)\n",
    "else:\n",
    "    from neuralmonkey.classes.population_mult import dfallpa_extraction_load_wrapper\n",
    "    DFallpa = dfallpa_extraction_load_wrapper(animal, date, question, list_time_windows,\n",
    "                                              which_level=which_level, events_keep=events_keep,\n",
    "                                              combine_into_larger_areas = combine_into_larger_areas,\n",
    "                                              exclude_bad_areas = exclude_bad_areas,\n",
    "                                              SPIKES_VERSION = SPIKES_VERSION,\n",
    "                                              HACK_RENAME_SHAPES = HACK_RENAME_SHAPES,\n",
    "                                              do_fr_normalization=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T19:58:16.840380421Z",
     "start_time": "2024-03-08T19:46:26.962284044Z"
    }
   },
   "id": "58ea6f5073b3909f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code example for benchmarking: decoding shapes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93edebd6e38ce91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This step takes in a representation of neural data and outputs a scalar score for how well you can decode \"shape\" from that data\n",
    "\n",
    "Here, this example is using the raw data (dimensionality = number of channels). The goal is to use methods to reduce the dimensionality of this data, each time running through this decoding benchmark, to compare the different methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "488ff1ca66a8098f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### First, pull out a specific PA. (just an example)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c025e2bfc54ebe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE: tjhis is just for demonstration. Eventually you will want to loop thru all PA, scoring them all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bb74aab4a961bd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import extract_single_pa\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10bc2b4059ed05b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make sure to normalize PA before running any modeling on it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "583a1e729ac08b30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_panorm = []\n",
    "for pa in DFallpa[\"pa\"].tolist():\n",
    "    from neuralmonkey.analyses.state_space_good import popanal_preprocess_scalar_normalization\n",
    "    PAnorm, PAscal, PAscalagg, fig, axes, groupdict = popanal_preprocess_scalar_normalization(pa, None, DO_AGG_TRIALS=False)\n",
    "    list_panorm.append(PAnorm)\n",
    "DFallpa[\"pa\"] = list_panorm\n",
    "\n",
    "# del DFallpa[\"pa_norm\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854db14b65f6c377"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pythonlib.globals import PATH_ANALYSIS_OUTCOMES\n",
    "import os\n",
    "SAVEDIR_ANALYSIS = f\"{PATH_ANALYSIS_OUTCOMES}/recordings/main/DECODE\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cab933058cb251a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Figure out how long is seuqence\n",
    "n_strokes_max = -1\n",
    "for i in range(2):\n",
    "    n_ignore = sum(PAnorm.Xlabels[\"trials\"][f\"seqc_{i}_shape\"]==\"IGNORE\")\n",
    "    n_total = len(PAnorm.Xlabels[\"trials\"][f\"seqc_{i}_shape\"])\n",
    "    print(n_ignore, n_total)\n",
    "    if n_ignore<n_total:\n",
    "        n_strokes_max=i+1\n",
    "assert n_strokes_max>0\n",
    "print(n_strokes_max)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a2fa80f4fc611b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keep specific events"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9228636d0e758329"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa[\"event\"].unique()\n",
    "events_keep = [\"03_samp\", \"04_go_cue\"]\n",
    "DFallpa = DFallpa[DFallpa[\"event\"].isin(events_keep)].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f56c2188b45159"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PREPROCESS - factorize all relevant labels FIRST here.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8634a855e2b24c5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_preprocess_vars_conjunctions_extract\n",
    "dfallpa_preprocess_vars_conjunctions_extract(DFallpa, which_level=which_level)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "441abc9cc89c86b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dflab = pa.Xlabels[\"trials\"]\n",
    "sorted([col for col in dflab.columns if \"seqc_\" in col])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99c1d21c5e23270e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract all "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdabd18c2d872166"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import preprocess_factorize_class_labels_ints\n",
    "MAP_LABELS_TO_INT = preprocess_factorize_class_labels_ints(DFallpa)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87e0da3ce5099d79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sandbox -- distribution of variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0c53a7cd8b4115"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Context\n",
    "\n",
    "var = \"CTXT_shapeloc_next\"\n",
    "vars_others = (\"CTXT_shapeloc_prev\", \"shape\", \"gridloc\", \"stroke_index_semantic\", \"task_kind\") # important to have SIS, to separate (shapeloc) from END.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20356c2de41fb50c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Default: Time-resolved decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36745f908f0c7fd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR_ANALYSIS = \"/tmp\"\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94465b7df045064f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of task kinds\n",
    "pa = DFallpa[\"pa\"].values[0]\n",
    "pa.Xlabels[\"trials\"][\"task_kind\"].value_counts()\n",
    "\n",
    "LIST_TASK_KIND = pa.Xlabels[\"trials\"][\"task_kind\"].unique().tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa62eee84c10ed1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/1_time_resolved\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "n_min_trials = 6\n",
    "\n",
    "from neuralmonkey.utils.frmat import bin_frmat_in_time\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_timeresolved_singlevar, decodewrapouterloop_categorical_timeresolved\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import pandas as pd\n",
    "RES = []\n",
    "\n",
    "# list_vars_decode = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "# list_vars_decode = [\"seqc_0_shape\"]\n",
    "list_vars_decode = [\"shape_this_event\"]\n",
    "# list_vars_decode = [\"seqc_2_shape\"]\n",
    "\n",
    "# list_vars_decode = [\"shape_is_novel_all\"]\n",
    "\n",
    "time_bin_size = 0.2\n",
    "slide = 0.2\n",
    "max_nsplits = 2\n",
    "\n",
    "DFRES = decodewrapouterloop_categorical_timeresolved(DFallpa, list_vars_decode, SAVEDIR, time_bin_size, slide, n_min_trials,\n",
    "                                                     max_nsplits=max_nsplits)\n",
    "# \n",
    "# for i, row in DFallpa.iterrows():\n",
    "#     br = row[\"bregion\"]\n",
    "#     tw = row[\"twind\"]\n",
    "#     ev = row[\"event\"]\n",
    "#     PA = row[\"pa\"]\n",
    "#     \n",
    "#     for task_kind in LIST_TASK_KIND:\n",
    "#         pa = PA.slice_by_labels(\"trials\", \"task_kind\", [task_kind])\n",
    "#     \n",
    "#         # 2. Extract X from pa\n",
    "#         X = pa.X # (nchans, ntrials, ntimes)\n",
    "#         times = pa.Times\n",
    "#         dflab = pa.Xlabels[\"trials\"]\n",
    "#     \n",
    "#         \n",
    "#         for var_decode in list_vars_decode:\n",
    "#             print(br, ev, var_decode)\n",
    "#             \n",
    "#             # Prune dflab\n",
    "#             from pythonlib.tools.pandastools import filter_by_min_n\n",
    "#             dftmp = filter_by_min_n(dflab, var_decode, n_min_trials)\n",
    "#             \n",
    "#             if len(dftmp)>0:\n",
    "#                 indskeep = dftmp[\"_index\"].tolist()\n",
    "#                 Xthis = X[:, indskeep, :]\n",
    "#                 dflab_this = dflab.iloc[indskeep]\n",
    "#                 \n",
    "#                 if len(dflab_this[var_decode].unique())==1:\n",
    "#                     print(\"SKIPPING, becuase only one label:\")\n",
    "#                     print(dflab_this[var_decode].unique())\n",
    "#                     continue\n",
    "#         \n",
    "#                 if len(Xthis)>0:\n",
    "#                     res = decodewrap_categorical_timeresolved_singlevar(Xthis, times, dflab_this, [var_decode],\n",
    "#                                                   time_bin_size=time_bin_size, slide=slide, max_nsplits=max_nsplits)\n",
    "#                     for r in res:\n",
    "#                         r[\"event\"]=ev\n",
    "#                         r[\"bregion\"]=br\n",
    "#                         r[\"twind\"]=tw\n",
    "#                         r[\"var_decode\"]=var_decode\n",
    "#                         r[\"task_kind\"] = task_kind\n",
    "#                     RES.extend(res)\n",
    "#                 \n",
    "# DFRES = pd.DataFrame(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a276f76dd803cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Cross-condition decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98ec3d6e76bf4ebb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each time bin, decode shape genearlizing across location"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4237c1dd8d95bce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decode_train_model, decode_categorical_cross_condition,decodewrap_categorical_timeresolved_cross_condition\n",
    "\n",
    "list_br = DFallpa[\"bregion\"].unique().tolist()\n",
    "list_tw = DFallpa[\"twind\"].unique().tolist()\n",
    "list_ev = DFallpa[\"event\"].unique().tolist()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f752951c3d6f1846"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAP_LABELS_TO_INT[\"shape\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3991acb980a7948e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = \"/tmp\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78e6a7b2d10704a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAP_LABELS_TO_INT[\"loc\"][\"map_int_to_class\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8872524005a2f929"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_timeresolved_cross_condition, decodewrapouterloop_categorical_timeresolved_cross_condition\n",
    "\n",
    "filtdict = None\n",
    "separate_by_task_kind = True\n",
    "\n",
    "# PARAMS\n",
    "# subtract_mean_vars_conj = True # WHether to normalize by sutbracting mean within each level of othervar...\n",
    "\n",
    "\n",
    "# Context\n",
    "list_var_decode = [\n",
    "    \"taskconfig_shp_SHSEM\",\n",
    "    \"taskconfig_shp_SHSEM\",\n",
    "    \"taskconfig_shp_SHSEM\",\n",
    "    \"taskconfig_shploc_SHSEM\",\n",
    "    \"taskconfig_shploc_SHSEM\",\n",
    "    \"taskconfig_shploc_SHSEM\",\n",
    "]\n",
    "list_vars_conj = [\n",
    "    [\"task_kind\"], # minimal control\n",
    "    [\"seqc_0_shape\", \"seqc_0_center_binned\", \"task_kind\"], # control for first action.\n",
    "    [\"character\", \"task_kind\"], # control for image.\n",
    "    [\"task_kind\"], # minimal control\n",
    "    [\"seqc_0_shape\", \"seqc_0_center_binned\", \"task_kind\"], # control for first action.\n",
    "    [\"character\", \"task_kind\"], # control for image.\n",
    "    ]\n",
    "\n",
    "time_bin_size = 0.2\n",
    "slide = 0.2\n",
    "subtract_mean_vars_conj = False\n",
    "DFRES = decodewrapouterloop_categorical_timeresolved_cross_condition(DFallpa, list_var_decode,\n",
    "                                                     list_vars_conj,\n",
    "                                                     SAVEDIR, time_bin_size=time_bin_size, slide=slide,\n",
    "                                                     subtract_mean_vars_conj=subtract_mean_vars_conj,\n",
    "                                                                     filtdict=filtdict,\n",
    "                                                                     separate_by_task_kind=separate_by_task_kind)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea62377c46cf112b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2b) Separate decoder for each level of other var (then take average over decoders). Useful to controlling for variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14744ec7fa463a16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.classes.population_mult import dfallpa_preprocess_vars_conjunctions_extract\n",
    "dfallpa_preprocess_vars_conjunctions_extract(DFallpa, which_level)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e6958e15184de87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preprocess for sequence context\n",
    "SEQ_CONTEXT_MODE = \"seq_pred\"\n",
    "            from neuralmonkey.analyses.rsa import rsagood_questions_dict, rsagood_questions_params\n",
    "            from neuralmonkey.analyses.rsa import preprocess_prune_pa_enough_data, preprocess_rsa_prepare_popanal_wrapper\n",
    "            \n",
    "            q_params = rsagood_questions_dict(animal, date, SEQ_CONTEXT_MODE)[SEQ_CONTEXT_MODE]\n",
    "            # q_params[\"effect_vars\"] = [\"shape_this_event\", \"loc_this_event\", \"stroke_index_fromlast_tskstks\"]         \n",
    "            # q_params[\"effect_vars\"] = [\"shape_this_event\", \"loc_this_event\", \"stroke_index\"]         \n",
    "            q_params[\"effect_vars\"] = [var_decode] + vars_conj_condition         \n",
    "            \n",
    "            # q_params[\"exclude_first_stroke\"] = False\n",
    "            pa, res_check_tasksets, res_check_effectvars = preprocess_rsa_prepare_popanal_wrapper(pa, **q_params)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8105d779dd26c63c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_timeresolved_within_condition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46879615721504a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = \"/tmp\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5692844afa5b51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Variable --> shape config\n",
    "pa = DFallpa[\"pa\"].values[0]\n",
    "dflab = pa.Xlabels[\"trials\"]\n",
    "dflab[:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59fc914abdb214aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "\n",
    "separate_by_task_kind = True\n",
    "\n",
    "# Context\n",
    "# Context\n",
    "list_var_decode = [\n",
    "    \"seqc_0_shape\",\n",
    "    \"seqc_0_shape\",\n",
    "    \"seqc_0_shapesemcat\",\n",
    "    \"seqc_0_locon_binned\",\n",
    "]\n",
    "list_vars_conj = [\n",
    "    [\"seqc_0_center_binned\", \"gridsize\", \"task_kind\"],\n",
    "    [\"taskconfig_shp_SHSEM\", \"seqc_0_center_binned\", \"task_kind\"], # control for parse\n",
    "    [\"seqc_0_center_binned\", \"gridsize\", \"task_kind\"],\n",
    "    [\"seqc_0_shape\", \"gridsize\", \"task_kind\"],\n",
    "    ]\n",
    "# filtdict = {\n",
    "#     \"stroke_index\":[0,3,4,5,6,7,8],\n",
    "# }\n",
    "# TRy with and without this.\n",
    "filtdict = None\n",
    "\n",
    "\n",
    "# RUns\n",
    "max_nsplits = 2\n",
    "time_bin_size = 0.2\n",
    "slide = 0.2\n",
    "\n",
    "# filtdict = {\n",
    "#     \"stroke_index\":[0,1, 23,4,5,6,7,8],\n",
    "# }\n",
    "# filtdict = None\n",
    "\n",
    "# PARAMS\n",
    "DFRES = decodewrapouterloop_categorical_timeresolved_within_condition(DFallpa, list_var_decode,\n",
    "                                                     list_vars_conj,\n",
    "                                                    SAVEDIR, time_bin_size=time_bin_size, slide=slide, filtdict=filtdict,\n",
    "                                                                      separate_by_task_kind=separate_by_task_kind)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "659c92a799c90cf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pa = DFallpa[\"pa\"].values[0]\n",
    "dflab = pa.Xlabels[\"trials\"]\n",
    "from pythonlib.tools.pandastools import extract_with_levels_of_conjunction_vars\n",
    "prune_min_n_trials = 5\n",
    "prune_min_n_levs = 2\n",
    "plot_counts_heatmap_savepath = \"/tmp/tmp.png\"\n",
    "balance_no_missed_conjunctions = False\n",
    "extract_with_levels_of_conjunction_vars(dflab, list_var_decode[0], list_vars_conj[0],\n",
    "                                                                 n_min_across_all_levs_var=prune_min_n_trials,\n",
    "                                                                 lenient_allow_data_if_has_n_levels=prune_min_n_levs,\n",
    "                                                                 prune_levels_with_low_n=True,\n",
    "                                                                 ignore_values_called_ignore=True,\n",
    "                                                                 plot_counts_heatmap_savepath=plot_counts_heatmap_savepath,\n",
    "                                                                 balance_no_missed_conjunctions=balance_no_missed_conjunctions)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11427ae70eb5cc2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Train a single decoder on specific dataset, then test across all time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3691223d7c46942"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HACK - combine all bregions into single pa\n",
    "if False:\n",
    "    from neuralmonkey.classes.population import concatenate_popanals_flexible\n",
    "    list_wl = DFallpa[\"which_level\"].unique().tolist()\n",
    "    resthis = []\n",
    "    for wl in list_wl:\n",
    "        for ev in list_ev:\n",
    "            for tw in list_tw:\n",
    "                a = DFallpa[\"event\"] == ev\n",
    "                b = DFallpa[\"twind\"] == tw\n",
    "                c = DFallpa[\"which_level\"] == wl\n",
    "                dfthis = DFallpa[a & b]\n",
    "                pa = concatenate_popanals_flexible(dfthis[\"pa\"].tolist(), \"chans\")[0]\n",
    "                resthis.append({\n",
    "                    \"which_level\":wl,\n",
    "                    \"event\":ev,\n",
    "                    \"bregion\":\"ALL\",\n",
    "                    \"twind\":tw,\n",
    "                    \"pa\":pa\n",
    "                })\n",
    "    DFallpa_ALL = pd.DataFrame(resthis)\n",
    "    DFallpa_ALL"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c16b1af399872b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trials_train = pa.Xlabels[\"trials\"][pa.Xlabels[\"trials\"][\"task_kind\"]==\"prims_single\"].index.tolist()\n",
    "trials_test = pa.Xlabels[\"trials\"][pa.Xlabels[\"trials\"][\"task_kind\"]==\"prims_on_grid\"].index.tolist()\n",
    "ntrials_expected_assert = len(pa.Xlabels[\"trials\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d93407c75be66d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HACK = False\n",
    "if HACK:\n",
    "    # Use all brain regions\n",
    "    DFallpaTHIS = DFallpa_ALL\n",
    "else:\n",
    "    DFallpaTHIS = DFallpa\n",
    "\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_single_decoder_across_time\n",
    "list_br = DFallpaTHIS[\"bregion\"].unique().tolist()\n",
    "list_tw = DFallpaTHIS[\"twind\"].unique().tolist()\n",
    "# list_var_decode = [\"seqc_0_shape\", \"seqc_0_loc\"]\n",
    "list_var_decode = [\"seqc_0_shape\"]\n",
    "# ev_train = \"03_samp\"\n",
    "# twind_train = [0.4, 0.6]\n",
    "ev_train = \"06_on_strokeidx_0\"\n",
    "twind_train = [0.05, 0.35]\n",
    "\n",
    "ev_test = \"03_samp\"\n",
    "# ev_test = \"06_on_strokeidx_0\"\n",
    "time_bin_size=0.05\n",
    "slide=0.025\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e57ef9aec8741621"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RES = []\n",
    "for br in list_br:\n",
    "    for tw in list_tw:\n",
    "        # 1. Extract the specific pa for this (br, tw)\n",
    "        \n",
    "        # Prep train and test PA\n",
    "        PA_train = extract_single_pa(DFallpaTHIS, br, tw, event=ev_train)\n",
    "        PA_train = PA_train.slice_by_dim_values_wrapper(\"trials\", trials_train)\n",
    "        PA_train = PA_train.slice_by_dim_values_wrapper(\"times\", twind_train)\n",
    "        PA_train = PA_train.agg_wrapper(\"times\")\n",
    "        x_train = PA_train.X.squeeze(axis=2).T # (ntrials, nchans)\n",
    "        \n",
    "        PA_test = extract_single_pa(DFallpaTHIS, br, tw, event=ev_test)\n",
    "        PA_test = PA_test.slice_by_dim_values_wrapper(\"trials\", trials_test)\n",
    "        if time_bin_size is not None:\n",
    "            PA_test = PA_test.agg_by_time_windows_binned(time_bin_size, slide)\n",
    "        X_test = PA_test.X # (chans, trials, times)\n",
    "        times_test = PA_test.Times\n",
    "        \n",
    "        for var_decode in list_var_decode:\n",
    "\n",
    "            # Train model\n",
    "            labels_train = PA_train.Xlabels[\"trials\"][var_decode].tolist()\n",
    "            \n",
    "            # Get test data\n",
    "            labels_test = PA_test.Xlabels[\"trials\"][var_decode].tolist()\n",
    "            \n",
    "            res = decodewrap_categorical_single_decoder_across_time(x_train, labels_train, X_test, labels_test,\n",
    "                                                                  times_test, do_std=False)\n",
    "        \n",
    "            for r in res:\n",
    "                r[\"var_decode\"]=var_decode\n",
    "                r[\"bregion\"]=br\n",
    "                r[\"twind\"]=tw   \n",
    "                r[\"event\"] = ev_test\n",
    "\n",
    "            RES.extend(res)\n",
    "\n",
    "DFRES = pd.DataFrame(RES)\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6abaf4b5797db13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract single trial results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3a21d2839843b42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# PARAMS\n",
    "var_decode = \"seqc_0_shape\"\n",
    "bregion = \"PMv\"\n",
    "twind = (-0.4, 0.6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f98b66754dbece70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "a = DFRES[\"var_decode\"]==var_decode\n",
    "b = DFRES[\"bregion\"]==bregion\n",
    "c = DFRES[\"twind\"]==twind\n",
    "dfthis = DFRES[a & b & c]\n",
    "\n",
    "times = dfthis[\"time\"]\n",
    "import numpy as np\n",
    "labels_predicted = np.stack(dfthis[\"labels_predicted\"]).T # (ntrials, ntimes)\n",
    "labels_test = np.stack(dfthis[\"labels_test\"]).T\n",
    "conf_scores = np.transpose(np.stack(dfthis[\"conf_scores\"]), [1,0,2]) # (ntrials, ntimes, nclasses)\n",
    "\n",
    "# score each time bin as correct or incorrect\n",
    "labels_correct = (labels_predicted == labels_test).astype(int)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8296d5b9f2e7ea8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each trial, what was its label\n",
    "assert np.all(np.diff(labels_test, axis=1))==0, \"otherwise cant do next step.\"\n",
    "labels_each_trial = labels_test[:,0]\n",
    "labels_orig = PA_test.Xlabels[\"trials\"][var_decode].tolist()\n",
    "assert len(labels_orig)==len(labels_each_trial)\n",
    "\n",
    "map_labint_to_trials = {}\n",
    "map_labint_to_laborig = {}\n",
    "labels_unique = np.unique(labels_each_trial)\n",
    "for lab in labels_unique:\n",
    "    inds_this_label = np.argwhere(labels_each_trial == lab).squeeze() # the indices which ahve this label as the CORRECT\n",
    "    map_labint_to_trials[lab] = inds_this_label\n",
    "\n",
    "    # Map it back to original label\n",
    "    lab_orig = PA_test.Xlabels[\"trials\"][var_decode][inds_this_label].unique()\n",
    "    assert len(lab_orig)==1\n",
    "    map_labint_to_laborig[lab] = lab_orig[0]\n",
    "\n",
    "map_trial_to_labint = {i:labint for i, labint in enumerate(labels_each_trial)}\n",
    "print(map_labint_to_laborig)    \n",
    "map_laborig_to_labint = {}\n",
    "for labint, laborig in map_labint_to_laborig.items():\n",
    "    assert laborig not in map_laborig_to_labint\n",
    "    map_laborig_to_labint[laborig] = labint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pythonlib.tools.plottools import makeColors\n",
    "pcols = makeColors(len(map_labint_to_laborig))\n",
    "map_trial_to_seq = {}\n",
    "map_trial_to_seq_ints = {}\n",
    "for i, seq in enumerate(PA_test.Xlabels[\"trials\"].loc[:, [\"seqc_0_shape\", \"seqc_1_shape\"]].values.tolist()):\n",
    "    map_trial_to_seq[i] = (tuple(seq))\n",
    "    map_trial_to_seq_ints[i] = [map_laborig_to_labint[s] if s in map_laborig_to_labint else s for s in seq]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3025b97e2c823fa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pick n random trials\n",
    "import random\n",
    "n=  9\n",
    "trials_all = list(range(len(map_trial_to_labint)))\n",
    "trials = random.sample(trials_all, n)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(trials)/ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*3), sharex=True, sharey=True)\n",
    "\n",
    "for tr, ax in zip(trials, axes.flatten()):\n",
    "    \n",
    "    # ax = axes.flatten()[0]\n",
    "    lab_pred = labels_predicted[tr,:]\n",
    "    # lab_corr = labels_correct[trial,:]\n",
    "    # lab_test = labels_test[trial,:]\n",
    "    # ax.plot(times, lab_pred, \"-ok\"),\n",
    "    # ax.plot(times, lab_test, \"-r\")\n",
    "    \n",
    "    # for each class, plot its time series\n",
    "    for labint in map_labint_to_laborig:\n",
    "        cs_this_lab = conf_scores[tr, :, labint].squeeze()\n",
    "        \n",
    "        ax.plot(times, cs_this_lab, label=labint, color=pcols[labint])\n",
    "        # ax.plot(times[lab_pred==labint], cs_this_lab[lab_pred==labint], \"-\", color=pcols[labint], linewidth=4)\n",
    "        ax.plot(times[lab_pred==labint], cs_this_lab[lab_pred==labint], \"s\", color=pcols[labint])\n",
    "    \n",
    "    ax.axvline(0, color=\"k\", alpha=0.5)\n",
    "    ax.set_title(f\"{var_decode}={map_trial_to_labint[tr]}\", color=pcols[map_trial_to_labint[tr]])\n",
    "    ax.set_xlabel(f\"seq: {map_trial_to_seq_ints[tr]}\")\n",
    "    ax.legend()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeac8f2e7321bfd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For each class label, collect all trials for which that is the correct label, and overlap\n",
    "\n",
    "labels_unique = np.unique(labels_each_trial)\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(labels_unique)/ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*4))\n",
    "\n",
    "for lab, ax in zip(labels_unique, axes.flatten()):\n",
    "    inds_this_label = np.argwhere(labels_each_trial == lab).squeeze() # the indices which ahve this label as the CORRECT \n",
    "    ax.plot(times, labels_predicted[inds_this_label, :].T, \"-ok\", alpha=0.05)\n",
    "    ax.set_title(lab)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8efa1735ce6376c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "ax = axes.flatten()[0]\n",
    "ax.plot(times, labels_correct.T, \"xk\", alpha=0.003);"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7a8e07c6d583ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Plot the results, comparing score across methods!!\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "fig = sns.relplot(data=DFRES, x=\"time\", y=\"score\", hue=\"bregion\", col=\"var_decode\",  kind=\"line\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ddae94bd60f728e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Plot the results, comparing score across methods!!\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "fig = sns.relplot(data=DFRES, x=\"time\", y=\"score\", hue=\"bregion\", col=\"var_decode\",  kind=\"line\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef8b1c64ea5cadd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 4) Cross-decoding across time bins"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3e89d8c3064a967"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_cross_time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5483ffe2c1ea629f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR_ANALYSIS = \"/tmp\"\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af81acf8ae6cd5d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/3_cross_temporal\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3813e5625d5647dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### A variation, where instead of first concatting PAs, just compute decode separately (and across PA) and then concatenate to plot\n",
    "Advantage: dont have to have identical trials, which is needed if you want to concat breigons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3658025bc4117336"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "assert False, \"add methdo to subtract mean in other window...\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28003a0e2a0f59f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb762d8394e495c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa_TEST = DFallpa[:20]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8cd08d2eabd4017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_var_decode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277450d6e1171809"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from neuralmonkey.analyses.decode_good import preprocess_extract_X_and_labels\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_plot\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/3_cross_temporal\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "# Make sure all pa use the same variables to refer to shapes.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")\n",
    "# warnings.filterwarnings(\"error\")\n",
    "time_bin_size = 0.3\n",
    "slide=0.3\n",
    "\n",
    "list_vars_decode = [\"seqc_0_shape\"]\n",
    "list_ev = ['04_go_cue', '06_on_STK_1']\n",
    "\n",
    "assert len(DFallpa[\"twind\"].unique())==1, \"not big deal. just change code below to iter over all (ev, tw).\"\n",
    "\n",
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_cross_time\n",
    "DFRES = decodewrapouterloop_categorical_cross_time(DFallpa_TEST, list_var_decode, time_bin_size, slide)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18e78f93ad0a24c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_plot\n",
    "decodewrapouterloop_categorical_cross_time_plot(DFRES, SAVEDIR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb93cb1ae6dee162"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Variation, focusing on \"shape_this_event\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9266dbf14fa3a06d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_var_decode = [\"shape_this_event\"]\n",
    "time_bin_size = 0.2\n",
    "slide = 0.2\n",
    "DFRES = decodewrapouterloop_categorical_cross_time(DFallpa, list_var_decode, time_bin_size, slide, savedir_ndata=\"/tmp\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f256b68bd8e88050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFRES[:2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40e8fbfe3e9fb2f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for var in [\"event_train\", \"event_test\", \"task_kind_train\", \"task_kind_test\"]:\n",
    "    print(DFRES[var].unique())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e137d821cf73cebb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "inds = list(range(100000))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b04d9cab20007a67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.all(np.diff(inds)==1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b0e131f5af23123"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(list(sorted(set(inds)))==inds) and (inds[-1] == len(inds)-1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b568bc83ee56db9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(list(sorted(set(inds)))==inds) and (inds[-1] == len(inds)-1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4af0d8c987c2465"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ebc0a166e61379b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "all([i2-i1==1 for i1, i2 in zip(inds[:-1], inds[1:])])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6b9549a0b1e970b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### New plots, which compare decoding across-context vs. within-context, etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a08e6fd791aa04c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFRESTHIS = DFRES[DFRES[\"bregion\"].isin([\"M1_m\", \"PM_v\"])].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "802badf0ea8691fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrapouterloop_categorical_cross_time_plot_compare_contexts\n",
    "\n",
    "var_decode = \"shape_this_event\"\n",
    "SAVEDIR = \"/tmp\"\n",
    "decodewrapouterloop_categorical_cross_time_plot_compare_contexts(DFRESTHIS, var_decode,\n",
    "                                                                     SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63a3476c431b9115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### For a given decoder, plot its predictions during a given event [OBSOLETE< see #5 below)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69deb18aa11e2266"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bregion = \"PMv\"\n",
    "\n",
    "var_decode = \"seqc_0_shape\"\n",
    "event_train = \"04_go_cue\"\n",
    "tbin_train = 1\n",
    "event_test = \"06_on_STK_1\"\n",
    "tbin_test = 1\n",
    "\n",
    "# var_decode = \"seqc_0_shape\"\n",
    "# event_train = \"04_go_cue\"\n",
    "# tbin_train = 1\n",
    "# event_test = \"06_on_STK_0\"\n",
    "# tbin_test = 1\n",
    "# \n",
    "# var_decode = \"seqc_0_shape\"\n",
    "# event_train = \"06_on_STK_0\"\n",
    "# tbin_train = 2\n",
    "# event_test = \"04_go_cue\"\n",
    "# tbin_test = 1\n",
    "\n",
    "a = DFRES[\"bregion\"] == bregion\n",
    "b = DFRES[\"var_decode\"] == var_decode\n",
    "c = DFRES[\"event_train\"] == event_train\n",
    "d = DFRES[\"tbin_train\"] == tbin_train\n",
    "e = DFRES[\"event_test\"] == event_test\n",
    "f = DFRES[\"tbin_test\"] == tbin_test\n",
    "\n",
    "tmp = DFRES[a & b & c & d & e & f]\n",
    "assert len(tmp)==1\n",
    "\n",
    "# Labels (convert label ints to actual value)\n",
    "labels_pred = tmp[\"labels_predicted\"].values[0]\n",
    "map_int_to_lab = tmp[\"map_int_to_lab\"].values[0]\n",
    "labels_pred = [map_int_to_lab[i] for i in labels_pred]\n",
    "\n",
    "score = tmp[\"score\"].values[0]\n",
    "\n",
    "# For each trial, compare to its sequence context\n",
    "inds_keep_test = tmp[\"inds_keep_test\"].values[0]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "106c2e0e46abc6a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3daaef98ed34201b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pa_test = extract_single_pa(DFallpa, bregion, twind, \"trial\", event=event_test)\n",
    "dflab_test = pa_test.Xlabels[\"trials\"].iloc[inds_keep_test].copy()\n",
    "\n",
    "\n",
    "list_seq = dflab_test.loc[:, [\"seqc_0_shape\", \"seqc_1_shape\"]].values.tolist() # list of n-tuples (len of seq)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c1824d8f13b0e8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dflab_test[\"labels_pred\"] = labels_pred\n",
    "dflab_test[:5]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f1c808932d0a3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.displot(data=dflab_test, x=\"labels_pred\", row=\"seqc_0_shape\", col=\"seqc_1_shape\", height=5)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb004a290865470e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sumamraize:\n",
    "\n",
    "# frequency of prediction matching first shape\n",
    "# same, for second shape.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccde37f8286c734a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5) Cross-temporal decoder, but with decoder trained using one variable (e.g., seqc_0_shape) and testing prediction of another variable (e.g., seqc_1_shape).\n",
    "Useful if, for example, want to see if decoder trained during visual presentation can decode 2nd stroke DURING 1st stroke."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1487d1d771d3490"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(range(1,3))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a147e241b53ce2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/4_cross_temporal_diff_var_train_test\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d443eeb181b33520"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DFallpa[\"event\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ab9fc36dea9c6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_cross_var"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df788b7c1c64483d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A = sorted(set(DFRES[\"var_decode_train\"].tolist()))\n",
    "B = sorted(set(DFRES[\"var_decode_test\"].tolist()))\n",
    "list_var_decode_train_test = [(a, b) for a in A for b in B]\n",
    "list_var_decode_train_test\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43d0bb9d59044017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make sure all pa use the same variables to refer to shapes.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")\n",
    "# warnings.filterwarnings(\"error\")\n",
    "\n",
    "# list_var_decode_train_test = [\n",
    "#     [\"seqc_0_shape\", \"seqc_1_shape\"], # (variable to construct decoder, variable that you will try to predict).\n",
    "#     [\"seqc_0_shape\", \"seqc_2_shape\"], # (variable to construct decoder, variable that you will try to predict).\n",
    "#     [\"seqc_1_shape\", \"seqc_2_shape\"] # (variable to construct decoder, variable that you will try to predict).\n",
    "# ]\n",
    "list_var_decode_train_test = [\n",
    "    [\"seqc_0_shape\", \"seqc_1_shape\"], # (variable to construct decoder, variable that you will try to predict).\n",
    "]\n",
    "\n",
    "labels_ignore = [\"IGNORE\", (\"IGNORE\",)]\n",
    "# list_ev = ['04_go_cue', '06_on_STK_0', '06_on_STK_1', '06_on_STK_2']\n",
    "list_ev = ['04_go_cue', '06_on_STK_0']\n",
    "\n",
    "assert len(DFallpa[\"twind\"].unique())==1, \"not big deal. just change code below to iter over all (ev, tw).\"\n",
    "\n",
    "SAVEDIR = f\"{SAVEDIR_ANALYSIS}/4_cross_temporal_diff_var_train_test\"\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "print(SAVEDIR)\n",
    "\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time\n",
    "\n",
    "# Make sure all pa use the same variables to refer to shapes.\n",
    "time_bin_size = 0.3\n",
    "slide=0.3\n",
    "\n",
    "\n",
    "DFRES = decodewrapouterloop_categorical_cross_time_cross_var(DFallpa,\n",
    "                                                     list_var_decode_train_test,\n",
    "                                                         time_bin_size, slide)\n",
    "from neuralmonkey.analyses.decode_good import decodewrap_categorical_cross_time, decodewrapouterloop_categorical_cross_time_plot\n",
    "decodewrapouterloop_categorical_cross_time_plot(DFRES, SAVEDIR)\n",
    "\n",
    "# \n",
    "# assert len(DFallpa[\"twind\"].unique())==1, \"not big deal. just change code below to iter over all (ev, tw).\"\n",
    "# \n",
    "# RES = []\n",
    "# for br in list_br:\n",
    "#     for tw in list_tw:\n",
    "#         for ev_train in list_ev:\n",
    "#             for ev_test in list_ev:\n",
    "# \n",
    "#                 print(br, tw, ev_train, ev_test)\n",
    "# \n",
    "#                 # TRAIN\n",
    "#                 PA_train = extract_single_pa(DFallpa, br, tw, event=ev_train)\n",
    "#                 if time_bin_size is not None:\n",
    "#                     PA_train = PA_train.agg_by_time_windows_binned(time_bin_size, slide)\n",
    "# \n",
    "#                 # TEST\n",
    "#                 PA_test = extract_single_pa(DFallpa, br, tw, event=ev_test)\n",
    "#                 if time_bin_size is not None:\n",
    "#                     PA_test = PA_test.agg_by_time_windows_binned(time_bin_size, slide)\n",
    "# \n",
    "#                 for var_decode_train, var_decode_test in list_var_decode_train_test:\n",
    "# \n",
    "#                     X_train, labels_train, times_train = preprocess_extract_X_and_labels(PA_train, var_decode_train)\n",
    "#                     X_test, labels_test, times_test = preprocess_extract_X_and_labels(PA_test, var_decode_test)\n",
    "# \n",
    "#                     if len(set(labels_train))==1 or len(set(labels_test))==1:\n",
    "#                         print(\"SKIPPING, becuase only one label:\")\n",
    "#                         print(\"Train:\", set(labels_train))\n",
    "#                         print(\"Test:\", set(labels_test))\n",
    "#                         continue\n",
    "# \n",
    "#                     # Only do splits if these are same trials\n",
    "#                     do_train_test_kfold_splits = labels_train==labels_test\n",
    "# \n",
    "#                     res = decodewrap_categorical_cross_time(X_train, labels_train, times_train,\n",
    "#                                                       X_test, labels_test, times_test,\n",
    "#                                                       do_std=False, labels_ignore=labels_ignore,\n",
    "#                                                             do_train_test_kfold_splits=do_train_test_kfold_splits)\n",
    "# \n",
    "#                     for r in res:\n",
    "#                         r[\"var_decode_train\"]=var_decode_train\n",
    "#                         r[\"var_decode_test\"]=var_decode_test\n",
    "#                         r[\"bregion\"]=br\n",
    "#                         r[\"event_train\"]=ev_train\n",
    "#                         r[\"event_test\"]=ev_test\n",
    "# \n",
    "#                     RES.extend(res)\n",
    "# DFRES = pd.DataFrame(RES)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572ae792311d1a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOAD a pre-saved results from decoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ce7eb19a2692bed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pythonlib.tools.pandastools import grouping_print_n_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pythonlib.tools.pandastools import applyFunctionToAllRows\n",
    "from pythonlib.tools.pandastools import append_col_with_grp_index\n",
    "import os\n",
    "from pythonlib.tools.pandastools import plot_45scatter_means_flexible_grouping\n",
    "from pythonlib.tools.plottools import savefig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3824ff7b34b8b0fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pancho\n",
    "# TIME_TRAIN_REL_TEST = 0.075\n",
    "# TIME_WIND_TEST_REL_EVENT = [-0.3, 0.3]\n",
    "\n",
    "# TIME_TRAIN_REL_TEST = 0.175\n",
    "# TIME_WIND_TEST_REL_EVENT = [-0.3, 0.4]\n",
    "\n",
    "# TIME_TRAIN_REL_TEST = 0.225\n",
    "# TIME_WIND_TEST_REL_EVENT = [-0.6, 0.6]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "615dd90ea3b9ec89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# animal = \"Pancho\"\n",
    "# date = 230126\n",
    "\n",
    "N_STROKES_MAX = 4\n",
    "for animal, date, question in [\n",
    "    (\"Pancho\", 230623, \"PIG\"),\n",
    "    (\"Diego\", 230628, \"PIG\"),\n",
    "    (\"Pancho\", 230626, \"PIG\"),\n",
    "    (\"Diego\", 230630, \"PIG\"),\n",
    "    (\"Pancho\", 230126, \"CHAR\"),\n",
    "    (\"Diego\", 231201, \"CHAR\"),\n",
    "    (\"Diego\", 231204, \"CHAR\"),\n",
    "    (\"Diego\", 231219, \"CHAR\"),\n",
    "]:\n",
    "    # animal = \"Diego\"\n",
    "    # date = 231219\n",
    "    # question = \"CHAR\"\n",
    "    combined_trial_stroke = 1\n",
    "    combine_areas = False\n",
    "    which_decode = \"3_cross_temporal\"\n",
    "    \n",
    "    ############################## LOAD DATA\n",
    "    SAVEDIR_LOAD = f\"/gorilla1/analyses/recordings/main/DECODE/{animal}-{date}/{question}-combined_{combined_trial_stroke}-combine_areas_{combine_areas}\" \n",
    "    # SAVEDIR = f\"/gorilla1/analyses/recordings/main/DECODE/{animal}-{date}/{question}-combined_{combined_trial_stroke}-combine_areas_{combine_areas}/{which_decode}\"\n",
    "    path = f\"{SAVEDIR_LOAD}/{which_decode}/DFRES.pkl\"\n",
    "    print(SAVEDIR_LOAD)\n",
    "    \n",
    "    DFRES = pd.read_pickle(path)    \n",
    "    \n",
    "    ############################# PREP DATAFRAME\n",
    "    ##### Context (same, diff) plots, binning tbins --> scalar\n",
    "    # Give new column that classifies the \"context\" \n",
    "    def F(x):\n",
    "        # If is not strokes, then ignore it (throw out)\n",
    "        if \"06_on_\" not in x[\"event_train\"]:\n",
    "            return \"IGNORE\"\n",
    "            \n",
    "        # Classify context\n",
    "        tktrain, tktest, evtrain, evtest = x[\"task_kind_train\"], x[\"task_kind_test\"], x[\"event_train\"], x[\"event_test\"]\n",
    "        if tktrain==tktest and evtrain==evtest:\n",
    "            return \"same\"\n",
    "        elif (tktrain==tktest) and (not evtrain==evtest):\n",
    "            return \"same_tk_diff_ev\"\n",
    "        elif (not tktrain==tktest) and (evtrain==evtest):\n",
    "            return \"diff_tk_same_ev\"\n",
    "        else:\n",
    "            return \"diff_tk_diff_ev\"    \n",
    "    DFRES = applyFunctionToAllRows(DFRES, F, \"context_tk_ev\")\n",
    "    \n",
    "    # Also group all \"diff\" into a single \"diff\"\n",
    "    inds = DFRES[\"context_tk_ev\"].isin([\"same_tk_diff_ev\", \"diff_tk_same_ev\", \"diff_tk_diff_ev\"])\n",
    "    DFRES[\"context_tk_ev_simple\"] = DFRES[\"context_tk_ev\"]\n",
    "    DFRES.loc[inds, \"context_tk_ev_simple\"] = \"diff\"\n",
    "        \n",
    "    # Only call \"diff\" those cases that are diff task kind... (avoid confounds in correlations across stroke indices, for same task kind).\n",
    "    inds = DFRES[\"context_tk_ev\"].isin([\"diff_tk_same_ev\", \"diff_tk_diff_ev\"])\n",
    "    DFRES[\"context_tk_ev_simple_diff_tk\"] = DFRES[\"context_tk_ev\"]\n",
    "    DFRES.loc[inds, \"context_tk_ev_simple_diff_tk\"] = \"diff\"\n",
    "    \n",
    "    # Append column with the test \"event x \"task kind\". \n",
    "    DFRES = append_col_with_grp_index(DFRES, [\"task_kind_test\", \"event_test\"], \"tk_ev_test\", strings_compact=True)\n",
    "    DFRES = append_col_with_grp_index(DFRES, [\"task_kind_train\", \"event_train\"], \"tk_ev_train\", strings_compact=True)\n",
    "    \n",
    "    grouping_print_n_samples(DFRES, [\"context_tk_ev_simple\", \"context_tk_ev\", \"task_kind_train\", \"task_kind_test\", \"event_train\", \"event_test\"]);\n",
    "    \n",
    "    # for TIME_TRAIN_REL_TEST in [0.125, 0.175, 0.225]:\n",
    "    for TIME_TRAIN_REL_TEST in [0.175]:\n",
    "        for TIME_WIND_TEST_REL_EVENT in [\n",
    "                                    # [-0.5, 0.5],\n",
    "                                    # [-0.3, 0.4],\n",
    "                                    [-0.4, -0.025],\n",
    "                                    [0.025, 0.4],\n",
    "                                ]:\n",
    "            #             TIME_TRAIN_REL_TEST = 0.175\n",
    "            # TIME_WIND_TEST_REL_EVENT = [-0.3, 0.4]\n",
    "    \n",
    "            ##### Convert to scalar by taking mean over time bins\n",
    "            # criterion for time bins\n",
    "            def F(x):\n",
    "                # relative time\n",
    "                a = np.abs(x[\"time_train\"] - x[\"time_test\"]) < TIME_TRAIN_REL_TEST\n",
    "                # absolute time\n",
    "                b = (x[\"time_test\"] >= TIME_WIND_TEST_REL_EVENT[0]) & (x[\"time_test\"] <= TIME_WIND_TEST_REL_EVENT[1])\n",
    "                return a & b\n",
    "            DFRES = applyFunctionToAllRows(DFRES, F, \"keep_tbin\")\n",
    "        \n",
    "            # Print results\n",
    "            if False:\n",
    "                grouping_print_n_samples(DFRES, [\"keep_tbin\", \"time_train\", \"time_test\"])\n",
    "            \n",
    "            for INCLUDE_PIG in [True, False]:\n",
    "                for EXCLUDE_FIRST_STROKE in [True, False]:\n",
    "                    \n",
    "                    ####################### PRUNE DATA TO JUST TIME BINS OF INTEREST\n",
    "                    DFRES_THIS = DFRES.copy()\n",
    "                    \n",
    "                    print(len(DFRES))\n",
    "                    # Prune to just those with tbins to keep\n",
    "                    DFRES_THIS = DFRES_THIS[DFRES_THIS[\"keep_tbin\"]==True]\n",
    "                    print(len(DFRES_THIS))\n",
    "                    \n",
    "                    # Remove those with CONTEXT ignore.\n",
    "                    DFRES_THIS = DFRES_THIS[DFRES_THIS[\"context_tk_ev\"] != \"IGNORE\"]\n",
    "                    print(len(DFRES_THIS))\n",
    "                    \n",
    "                    if INCLUDE_PIG==False:\n",
    "                        # Only include data that is single prims or character (since PIG is low N).\n",
    "                        DFRES_THIS = DFRES_THIS[(DFRES_THIS[\"task_kind_train\"].isin([\"prims_single\", \"character\"])) & (DFRES_THIS[\"task_kind_test\"].isin([\"prims_single\", \"character\"]))]\n",
    "                    \n",
    "                    # Remove cases that are the high N stroke nums, they are usualyl noise\n",
    "                    events_keep = [f\"06_on_STK_{i}\" for i in range(N_STROKES_MAX)]\n",
    "                    print(len(DFRES_THIS))\n",
    "                    DFRES_THIS = DFRES_THIS[(DFRES_THIS[\"event_train\"].isin(events_keep)) & (DFRES_THIS[\"event_test\"].isin(events_keep))]\n",
    "                    print(len(DFRES_THIS))\n",
    "                    \n",
    "                    if EXCLUDE_FIRST_STROKE:\n",
    "                        DFRES_THIS = DFRES_THIS[~(DFRES_THIS[\"event_train\"]==\"06_on_STK_0\") & ~(DFRES_THIS[\"event_test\"]==\"06_on_STK_0\")]\n",
    "                    \n",
    "                    #################################### Aggregate to average over time bins\n",
    "                    from pythonlib.tools.pandastools import aggregGeneral\n",
    "                    DFRES_THIS = aggregGeneral(DFRES_THIS, [\"var_decode\", \"bregion\", \"event_train\", \"event_test\", \"task_kind_train\", \"task_kind_test\", \n",
    "                                                            \"context_tk_ev\", \"context_tk_ev_simple\", \"context_tk_ev_simple_diff_tk\",\n",
    "                                                            \"keep_tbin\", \"tk_ev_test\", \"tk_ev_train\"], values=[\"score\", \"score_adjusted\"])\n",
    "                    \n",
    "                    DFRES_THIS = DFRES_THIS.reset_index(drop=True)\n",
    "                    \n",
    "                    #################################### PLOTS\n",
    "                    a = TIME_TRAIN_REL_TEST\n",
    "                    b = \"_\".join([str(x) for x in TIME_WIND_TEST_REL_EVENT])\n",
    "                    savedir = f\"{SAVEDIR_LOAD}/3_cross_temporal_split_by_contexts_scalar/inclPIG={INCLUDE_PIG}-excldFrstStk={EXCLUDE_FIRST_STROKE}-TRelTest={a}-TWindRelEv={b}\"\n",
    "            \n",
    "                    os.makedirs(savedir, exist_ok=True)\n",
    "                    \n",
    "                    if \"diff\" in DFRES_THIS[\"context_tk_ev_simple\"].tolist():\n",
    "                        # Sometimes pruning leads to loss of all data...\n",
    "                        \n",
    "                        dfthis = DFRES[DFRES[\"bregion\"]==DFRES[\"bregion\"].values[0]] # pick out a subsample --> smaler plots.\n",
    "                        fig = sns.relplot(data=dfthis, x=\"time_train\", y=\"time_test\", col=\"keep_tbin\")\n",
    "                        savefig(fig, f\"{savedir}/time_bins.pdf\")\n",
    "                        \n",
    "                        for ctxt_diff in [\"same_tk_diff_ev\", \"diff_tk_same_ev\", \"diff_tk_diff_ev\"]:\n",
    "                            if ctxt_diff in DFRES_THIS[\"context_tk_ev\"].tolist():\n",
    "                                _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev\", x_lev_manip=\"same\", y_lev_manip=ctxt_diff, var_subplot=\"var_decode\", var_value=\"score_adjusted\", var_datapt=\"bregion\")\n",
    "                                savefig(fig, f\"{savedir}/scatter-same-vs-{ctxt_diff}.pdf\")\n",
    "                                plt.close(\"all\")\n",
    "                        \n",
    "                        # just \"same\" vs \"diff\" (combining all diff).\n",
    "                        _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple\", x_lev_manip=\"same\", y_lev_manip=\"diff\", var_subplot=\"var_decode\", var_value=\"score_adjusted\", var_datapt=\"bregion\")\n",
    "                        savefig(fig, f\"{savedir}/scatter-same-vs-diff.pdf\")\n",
    "                        plt.close(\"all\")\n",
    "                        \n",
    "                        if \"diff\" in DFRES_THIS[\"context_tk_ev_simple_diff_tk\"].tolist():\n",
    "                            _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple_diff_tk\", x_lev_manip=\"same\", y_lev_manip=\"diff\", var_subplot=\"var_decode\", var_value=\"score_adjusted\", var_datapt=\"bregion\")\n",
    "                            savefig(fig, f\"{savedir}/scatter-same-vs-diff_task_kind.pdf\")\n",
    "                            plt.close(\"all\")\n",
    "                        \n",
    "                        # Separate subplot for each bregion.\n",
    "                        for var_datapt in [\"tk_ev_train\", \"tk_ev_test\"]:\n",
    "                            _, fig = plot_45scatter_means_flexible_grouping(DFRES_THIS, var_manip=\"context_tk_ev_simple\", x_lev_manip=\"same\", y_lev_manip=\"diff\",    var_subplot=\"bregion\", var_value=\"score_adjusted\", var_datapt=var_datapt, plot_error_bars = False, shareaxes=True)\n",
    "                            savefig(fig, f\"{savedir}/scatter_by_bregion-same-vs-diff-datapt={var_datapt}.pdf\")\n",
    "                                                \n",
    "                            plt.close(\"all\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c97d6655d46653"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [TESTING] State space plots (tsne)\n",
    "NOTE: previously have dPCA plots..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71caf94b369f4732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f94d98653399b13e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
